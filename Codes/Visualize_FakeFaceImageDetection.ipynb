{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dnMNcrwOJ5Eh","executionInfo":{"status":"ok","timestamp":1679692484132,"user_tz":240,"elapsed":21366,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"ab0994c1-32d9-48f8-ddc3-cae2d3b9acea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","Folder exists\n"]}],"source":["from pathlib import Path\n","USE_COLAB: bool = True\n","dataset_base_path = Path(\"/content/drive/My Drive/ECE 792 - Advance Topics in Machine Learning/Datasets\")\n","if USE_COLAB:\n","  from google.colab import drive\n","  \n","  # Mount the drive to access google shared docs\n","  drive.mount('/content/drive/', force_remount=True)\n","\n","  if dataset_base_path.exists():\n","    print(\"Folder exists\")\n","  else:\n","    print(\"DOESN'T EXIST. Add desired folder as a shortcut in your 'My Drive'\")"]},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dataset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","from typing import Tuple, Optional, List\n","\n","import argparse\n","import os\n","from tqdm import tqdm\n","import time\n","import copy\n","import math\n","from zipfile import ZipFile\n","\n","from PIL import Image\n","from typing import Dict, List, Union\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# PyTorch's versions:\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)\n","print(\"NumPy Version: \",np.__version__)\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F3BAhDaXMYfd","executionInfo":{"status":"ok","timestamp":1679678546699,"user_tz":240,"elapsed":3276,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"35d34e6a-8ec8-4be7-dbec-96f7cc0d2d5c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch Version:  1.13.1+cu116\n","Torchvision Version:  0.14.1+cu116\n","NumPy Version:  1.22.4\n","Fri Mar 24 17:22:26 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["class CelebrityData(torch.utils.data.Dataset):\n","\n","  def __init__(\n","    self,\n","    base_path: Path,\n","    transform = None,\n","    seed = None,\n","    gans_to_skip: Optional[List[str]] = None,\n","    n_fake_imgs_to_extract: int = 40000,\n","    *,\n","    unzip_real_imgs: bool = True,\n","    ):\n","    '''\n","    Folder structure of base_path should be\n","    base_path -> RealFaces/FakeFaces\n","    RealFaces -> .zip\n","    FakeFaces -> GANType -> .zip (e.g., FakeFaces -> PGGAN -> .zip)\n","    '''\n","    super().__init__()\n","    self.rng = np.random.default_rng(seed)\n","    self.unzip_real_imgs = unzip_real_imgs\n","\n","    self.fake_images_path = Path(base_path) / \"FakeFaces\"\n","    self.fake_image_gan_names = os.listdir(str(self.fake_images_path))\n","    if gans_to_skip is not None:\n","      print(f\"Not unzipping '{gans_to_skip}'\")\n","      self.fake_image_gan_names = list(filter(lambda x: x not in gans_to_skip, self.fake_image_gan_names))\n","    self.fake_images: Dict[str, List[Union[str, Path]]] = {}\n","    for fake_image_gan_name in self.fake_image_gan_names:\n","      print(f\"Extracting imagery for '{fake_image_gan_name}'\")\n","      fake_image_gan_path = self.fake_images_path / fake_image_gan_name\n","      zip_file = sorted(fake_image_gan_path.glob(\"*.zip\"))\n","      if len(zip_file) == 1:\n","        zip_file = zip_file[0]\n","        with ZipFile(str(zip_file), 'r') as zipObj:\n","          zipObj.extractall()\n","        self.fake_images.update({fake_image_gan_name: zipObj.namelist()[:n_fake_imgs_to_extract]})\n","      else:\n","        fake_image_paths = sorted(fake_image_gan_path.glob(\"*.jpg\"))\n","        self.fake_images.update({fake_image_gan_name: fake_image_paths[:n_fake_imgs_to_extract]})\n","\n","    self.n_fake_gans = len(list(self.fake_images.keys()))\n","\n","    if unzip_real_imgs:\n","      self.real_images_path = Path(base_path) / \"RealFaces\"\n","      print(\"Extracting RealFaces imagery\")\n","      real_images_zip_files = sorted(self.real_images_path.glob(\"*.zip\"))\n","      if len(real_images_zip_files) != 1:\n","        raise RuntimeError(f\"Got more than or less than 1 zip file in '{self.real_images_path}'. Got '{len(real_images_zip_files)}'\")\n","      self.real_images_zip_file = real_images_zip_files[0]\n","      # Create a ZipFile Object and load sample.zip in it\n","      with ZipFile(str(self.real_images_zip_file), 'r') as zipObj:\n","        # Extract all the contents of zip file in current directory\n","        zipObj.extractall()\n","      self.real_images = zipObj.namelist()[1:self.len_of_fake_images+1]\n","    else:\n","      self.real_imgs = []\n","\n","    self.transform = transform\n","\n","    # according to Deep Fake Image Detection Based on Pairwise Learning, we need to make combinations for all\n","    # real images with all fake images\n","    fake_img_list = []\n","    for fake_imgs in self.fake_images.values():\n","      fake_img_list.extend(fake_imgs)\n","    self.fake_img_list = fake_img_list\n","\n","  def fake_image_rand_selection(self, index) -> str:\n","    rand_selection = self.rng.uniform(low=-0.499, high=len(self.fake_images) - 0.501)\n","    gan_selection = self.fake_image_gan_names[int(np.round(rand_selection))]\n","    \n","    return self.fake_images.get(gan_selection)[index // len(self.fake_images)]\n","\n","  @property\n","  def len_of_fake_images(self) -> int:\n","    total_len = 0\n","    for val in self.fake_images.values():\n","      total_len += len(val)\n","\n","    return total_len\n","\n","  def len_of_real_and_fake(self):\n","    return len(self.real_images) + self.len_of_fake_images"],"metadata":{"id":"feIzG8dArDEw","executionInfo":{"status":"ok","timestamp":1679678548567,"user_tz":240,"elapsed":14,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import re\n","def get_latest_model(base_path, suffix: str = \".pt\") -> Path:\n","  epoch_num = []\n","  all_files = sorted(Path(base_path).glob(\"*.pth\"))\n","  for file_ in all_files:\n","    idx_num = re.search(\"--\", str(file_)).span()\n","    idx_pt = re.search(suffix, str(file_)).span()\n","    model_num = str(file_)[idx_num[-1]:idx_pt[0]]\n","    try:\n","      epoch_num.append(int(model_num))\n","    except ValueError:\n","      idx_num = re.search(\"--\", str(model_num)).span()\n","      epoch_num.append(int(model_num[idx_num[-1]:]))\n","\n","  idx = epoch_num.index(np.max(epoch_num))\n","  return all_files[idx]"],"metadata":{"id":"sM86rgUEC9tf","executionInfo":{"status":"ok","timestamp":1679678563551,"user_tz":240,"elapsed":1331,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Two-step learning policy as employed by 'Deep Fake Image Detection Based on Pairwise Learning'. Therefore, we first train the CFFN network with the contrastive loss. After the CFFN network is learned to minimize the contrastive loss, we then train the classification network using the outputs from the CFFN network, as this better feature representation output will allow the classification network to better classify the images as fake or real. The classification network is trained using the binary cross-entropy loss of predicting whether the image is real or fake [p.6. section 2.4]"],"metadata":{"id":"QLtX8bv5Qw_6"}},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","\n","def circular_index(idx: int, upper_bound: int) -> int:\n","  if idx < upper_bound:\n","    return idx\n","  return idx - (upper_bound * (idx // upper_bound))\n","\n","class CelebrityDataClassificationNetwork(CelebrityData):\n","  def __init__(\n","      self,\n","      base_path: Path,\n","      transform = None,\n","      seed = None,\n","      gans_to_skip: Optional[List[str]] = None,\n","      unzip_real_imgs: bool = True,\n","  ):\n","    super().__init__(\n","        base_path=base_path,\n","        transform=transform,\n","        seed=seed,\n","        gans_to_skip=gans_to_skip,\n","        unzip_real_imgs=unzip_real_imgs,\n","    )\n","    self.to_tensor = transforms.ToTensor()\n","    self.enc = OneHotEncoder()\n","    self.enc.fit([[0], [1]])\n","\n","  def __getitem__(self, index):\n","    img_path, label, gan_selection = self.choose_real_or_fake_image(index)\n","\n","    img = Image.open(img_path).convert('RGB')\n","    if self.transform is not None:\n","      img = self.transform(img)\n","    else:\n","      img = self.to_tensor(img)\n","\n","    return img, label, gan_selection\n","\n","  def choose_real_or_fake_image(self, index) -> Tuple[str, np.ndarray, str]:\n","    if self.rng.standard_normal() > 0:\n","      img = self.real_images[circular_index(index // 2, len(self.real_images))]  # divide by 2 b/c we define __len__ as all real & fake images\n","      gan_selection = \"real\"\n","      label = [1]\n","    else:\n","      img, gan_selection = self.fake_image_rand_selection(index)\n","      label = [0]\n","\n","    label = np.squeeze(self.enc.transform(np.column_stack(label).reshape(-1, 1)).toarray())\n","    return img, label.astype(np.float32), gan_selection\n","\n","  def fake_image_rand_selection(self, index) -> str:\n","    rand_selection = self.rng.uniform(low=-0.499, high=self.n_fake_gans - 0.501)\n","    gan_selection = self.fake_image_gan_names[int(np.round(rand_selection))]\n","\n","    return self.fake_images[gan_selection][index // (self.n_fake_gans * 2)], gan_selection  # mult den by 2 b/c of definition of __len__ being all real & fake images\n","\n","  def names_of_gans(self) -> List[str]:\n","    return list(self.fake_images.keys())\n","\n","  def __len__(self):\n","    return len(self.real_images) + self.len_of_fake_images"],"metadata":{"id":"NGvuBcd_rj6p","executionInfo":{"status":"ok","timestamp":1679678605731,"user_tz":240,"elapsed":1083,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["**1. Common Fake Feature Network**\n","\n","Network structure includes a pairwise learning approach. \"A fake face image detector based on the novel CFFN, consisting of an improved DenseNet backbone network and Siamese network architecture...The cross-layer features are investigated by the proposed CFFN, which can be used to improve the performance.\"\n","\n","The fake and real images are paired together and the pairwise information is used to construct the contrastive loss to learn the discriminative common fake feature (CFF) by the CFFN. The paper states that 2 million pairwise samples are used for training.\n","\n","\"One way to learn both the CFFs and classifier is the join learning strategy incorporating the contrastive loss and cross-entropy loss into the total energy function. In another way, the CFFN is first trained by the proposed contrastive loss and follows by training the classifier based on cross-entropy loss. When the first strategy is applied, it is difficult to observe the impact of both contrastive and cross-entropy loss functions on the performance of the fake image detection tasks. Therefore, we adopt the second strategy to ensure the best performance of the proposed method.\""],"metadata":{"id":"RkqQitJIUMJf"}},{"cell_type":"code","source":["# We will be working with GPU:\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('Device : ' , device)\n","\n","# Number of GPUs available. \n","num_GPU = torch.cuda.device_count()\n","print('Number of GPU : ', num_GPU)\n","\n","model_output_path = Path(\"/content/drive/MyDrive/ECE 792 - Advance Topics in Machine Learning/Code/DeepFakeImageDetection/CFFN/model\")\n","\n","config = { 'batch_size'             : 88,\n","           'image_size'             : 64,\n","           'n_channel'              : 3,\n","           'n_epochs'               : 15,\n","           'lr'                     : 1e-3,\n","           'growth_rate'            : 24,\n","           'transition_layer_theta' : 0.5,\n","           'device'                 : device,\n","           'm_th'                   : 0.5,\n","           'n_combinations'         : 2e6,\n","           'seed'                   : 999,\n","           'model_output_path'      : model_output_path,\n","           'chkp_freq'              : 1,  # number of epochs to save model out\n","           'n_workers'              : 4,\n","          #  'gans_to_skip'           : [\"CDCGAN\", \"LSGAN\", \"WGAN-GP\"],\n","}\n","\n","celebrity_data = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QA-ybvdX_aVl","executionInfo":{"status":"ok","timestamp":1679678611945,"user_tz":240,"elapsed":85,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"eac19a4c-d7d3-4b63-c7d2-efc85242f892"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Device :  cuda\n","Number of GPU :  1\n"]}]},{"cell_type":"code","source":["if celebrity_data is None:\n","  celebrity_data = CelebrityDataClassificationNetwork(\n","    base_path=dataset_base_path,\n","    transform=transforms.Compose(\n","      [\n","        transforms.Resize(int(config[\"image_size\"] * 1.1)),\n","        transforms.CenterCrop(config[\"image_size\"]),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n","      ]\n","    ),\n","    seed=config[\"seed\"],\n","    gans_to_skip=config.get(\"gans_to_skip\"),\n","  )\n","\n","dataloader = torch.utils.data.DataLoader(\n","  dataset=celebrity_data,\n","  shuffle=True,\n","  batch_size=config[\"batch_size\"],\n","  num_workers=config[\"n_workers\"],\n","  drop_last=True,  # drop last batch that may not be the same size as the expected batch for the network\n","  pin_memory=True,\n",")"],"metadata":{"id":"8hF9OU_QqKni","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679678742138,"user_tz":240,"elapsed":127812,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"aad88329-2e7f-465c-ca47-6597443a212b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting imagery for 'WGAN-CP'\n","Extracting imagery for 'PGGAN'\n","Extracting imagery for 'DCGAN'\n","Extracting imagery for 'LSGAN'\n","Extracting imagery for 'CDCGAN'\n","Extracting imagery for 'WGAN-GP'\n","Extracting RealFaces imagery\n"]}]},{"cell_type":"code","source":["from typing import Callable, Tuple\n","from tqdm import tqdm\n","\n","class DenseBlock2(nn.Module):\n","  conv0_0_out = None\n","  conv0_1_out = None\n","  batch_norm0_out = None\n","  concat0_out = None\n","  activation0_out = None\n","  conv1_0_out = None\n","  conv1_1_out = None\n","  batch_norm1_out = None\n","  concat1_out = None\n","  activation1_out = None\n","  trans_layer_out = None\n","  trans_layer_out_indices = None\n","\n","  def __init__(\n","    self,\n","    in_channels: int,\n","    out_channels: int,\n","    growth_rate: int,\n","    transition_layer_theta: float,\n","    device: torch.device = None,\n","  ):\n","    super().__init__()\n","    if device is None:\n","      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.conv0_0 = nn.Conv2d(\n","      in_channels=in_channels,\n","      out_channels=in_channels * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv0_1 = nn.Conv2d(\n","      in_channels=in_channels * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm0 = nn.BatchNorm2d(in_channels + growth_rate, device=device)\n","\n","    self.conv1_0 = nn.Conv2d(\n","      in_channels=in_channels + growth_rate,\n","      out_channels=(in_channels + growth_rate) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv1_1 = nn.Conv2d(\n","      in_channels=(in_channels + growth_rate) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm1 = nn.BatchNorm2d(in_channels + (2 * growth_rate), device=device)\n","\n","    trans_kernel_size = int(1/transition_layer_theta)\n","    self.trans_layer = nn.MaxPool3d(kernel_size=(trans_kernel_size, 1, 1), return_indices=True)\n","    self.activation_func = nn.ReLU()\n","\n","  def reconstruction_entries(self, index: int, trans_layer_out: Optional[torch.Tensor] = None) -> dict:\n","    if not isinstance(index, int):\n","      if len(index) > 1:\n","        raise RuntimeError(f\"got multiple indices for DenseBlock4 reconstruction. Only one index at a time is supported currently.\")\n","    \n","    if trans_layer_out is None:\n","      trans_layer_out = self.trans_layer_out[index].unsqueeze(0)\n","    \n","    maxunpool = nn.MaxUnpool3d(kernel_size=self.trans_layer.kernel_size)\n","    maxpool_idx = self.trans_layer_out_indices[index].unsqueeze(0)\n","    trans_layer_unpool = maxunpool(trans_layer_out, maxpool_idx).squeeze()\n","    trans_layer_act = self.activation_func(trans_layer_unpool.unsqueeze(0))\n","\n","    weights = [\n","      self.conv1_1.weight, self.conv1_0.weight, self.conv0_1.weight, self.conv0_0.weight,\n","    ]\n","    deconv_shape_determinations = [\n","        (1, 3, 1), (0, 1, 1), (1, 3, 1), (0, 1, 1),\n","    ]\n","    deconv_shape_upsample_factors = [1, 1, 1, 1]\n","    reconstruction_inputs = [\n","        trans_layer_act[:, :self.conv1_1_out.shape[1]],\n","        self.conv1_0_out[index],\n","        self.activation0_out[index, :self.conv0_1_out.shape[1]],\n","        self.conv0_0_out[index],\n","    ]\n","    activation_layers = [\n","      self.activation_func,\n","      None,\n","      self.activation_func,\n","      None,\n","    ]\n","    indexing_for_unconcatenating = [\n","        None,\n","        self.conv0_1_out.shape[1],\n","        None,\n","        None,\n","    ]\n","\n","    return {\n","      \"weights\": weights,\n","      \"deconv_shape_determinations\": deconv_shape_determinations,\n","      \"deconv_shape_upsample_factors\": deconv_shape_upsample_factors,\n","      \"reconstruction_inputs\": reconstruction_inputs,\n","      \"activation_layers\": activation_layers,\n","      \"indexing_for_unconcatenating\": indexing_for_unconcatenating,\n","    }\n","\n","  def forward(self, x):\n","    self.conv0_0_out = self.conv0_0(x)\n","    self.conv0_1_out = self.conv0_1(self.conv0_0_out)\n","    self.concat0_out = torch.concat((self.conv0_1_out, x), dim=1)\n","    self.batch_norm0_out = self.batch_norm0(self.concat0_out)\n","    self.activation0_out = self.activation_func(self.batch_norm0_out)\n","\n","    self.conv1_0_out = self.conv1_0(self.activation0_out)\n","    self.conv1_1_out = self.conv1_1(self.conv1_0_out)\n","    self.concat1_out = torch.concat((self.conv1_1_out, self.activation0_out), dim=1)\n","    self.batch_norm1_out = self.batch_norm1(self.concat1_out)\n","    self.activation1_out = self.activation_func(self.batch_norm1_out)\n","\n","    self.trans_layer_out, self.trans_layer_out_indices = self.trans_layer(self.activation1_out)\n","\n","    return self.trans_layer_out\n","\n","class DenseBlock3(nn.Module):\n","  conv0_0_out = None\n","  conv0_1_out = None\n","  batch_norm0_out = None\n","  concat0_out = None\n","  activation0_out = None\n","  conv1_0_out = None\n","  conv1_1_out = None\n","  batch_norm1_out = None\n","  concat1_out = None\n","  activation1_out = None\n","  conv2_0_out = None\n","  conv2_1_out = None\n","  batch_norm2_out = None\n","  concat2_out = None\n","  activation2_out = None\n","  trans_layer_out = None\n","  trans_layer_out_indices = None\n","\n","  def __init__(\n","    self,\n","    in_channels: int,\n","    out_channels: int,\n","    growth_rate: int,\n","    transition_layer_theta: float,\n","    device: torch.device = None,\n","  ):\n","    super().__init__()\n","    if device is None:\n","      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.conv0_0 = nn.Conv2d(\n","      in_channels=in_channels,\n","      out_channels=in_channels * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv0_1 = nn.Conv2d(\n","      in_channels=in_channels * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm0 = nn.BatchNorm2d(in_channels + growth_rate, device=device)\n","\n","    self.conv1_0 = nn.Conv2d(\n","      in_channels=in_channels + growth_rate,\n","      out_channels=(in_channels + growth_rate) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv1_1 = nn.Conv2d(\n","      in_channels=(in_channels + growth_rate) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm1 = nn.BatchNorm2d(in_channels + (2 * growth_rate), device=device)\n","\n","    self.conv2_0 = nn.Conv2d(\n","      in_channels=in_channels + (2 * growth_rate),\n","      out_channels=(in_channels + (2 * growth_rate)) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv2_1 = nn.Conv2d(\n","      in_channels=(in_channels + (2 * growth_rate)) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm2 = nn.BatchNorm2d((in_channels + (3 * growth_rate)), device=device)\n","\n","    trans_kernel_size = int(1/transition_layer_theta)\n","    self.trans_layer = nn.MaxPool3d(kernel_size=(trans_kernel_size, 1, 1), return_indices=True)\n","    self.activation_func = nn.ReLU()\n","\n","  def reconstruction_entries(self, index: int, trans_layer_out: Optional[torch.Tensor] = None) -> dict:\n","    if not isinstance(index, int):\n","      if len(index) > 1:\n","        raise RuntimeError(f\"got multiple indices for DenseBlock4 reconstruction. Only one index at a time is supported currently.\")\n","    \n","    if trans_layer_out is None:\n","      trans_layer_out = self.trans_layer_out[index].unsqueeze(0)\n","\n","    maxunpool = nn.MaxUnpool3d(kernel_size=self.trans_layer.kernel_size)\n","    maxpool_idx = self.trans_layer_out_indices[index].unsqueeze(0)\n","    trans_layer_unpool = maxunpool(trans_layer_out, maxpool_idx).squeeze()\n","    trans_layer_act = self.activation_func(trans_layer_unpool.unsqueeze(0))\n","\n","    weights = [\n","      self.conv2_1.weight, self.conv2_0.weight, self.conv1_1.weight,\n","      self.conv1_0.weight, self.conv0_1.weight, self.conv0_0.weight,\n","    ]\n","    deconv_shape_determinations = [\n","        (1, 3, 1), (0, 1, 1), (1, 3, 1),\n","        (0, 1, 1), (1, 3, 1), (0, 1, 1),\n","    ]\n","    deconv_shape_upsample_factors = [1, 1, 1, 1, 1, 1]\n","    reconstruction_inputs = [\n","        trans_layer_act[:, :self.conv2_1_out.shape[1]],\n","        self.conv2_0_out[index],\n","        self.activation1_out[index, :self.conv1_1_out.shape[1]],\n","        self.conv1_0_out[index],\n","        self.activation0_out[index, :self.conv0_1_out.shape[1]],\n","        self.conv0_0_out[index],\n","    ]\n","    activation_layers = [\n","        self.activation_func,\n","        None,\n","        self.activation_func,\n","        None,\n","        self.activation_func,\n","        None,\n","    ]\n","    indexing_for_unconcatenating = [\n","        None,\n","        self.conv1_1_out.shape[1],\n","        None,\n","        self.conv0_1_out.shape[1],\n","        None,\n","        None,\n","    ]\n","\n","    return {\n","      \"weights\": weights,\n","      \"deconv_shape_determinations\": deconv_shape_determinations,\n","      \"deconv_shape_upsample_factors\": deconv_shape_upsample_factors,\n","      \"reconstruction_inputs\": reconstruction_inputs,\n","      \"activation_layers\": activation_layers,\n","      \"indexing_for_unconcatenating\": indexing_for_unconcatenating,\n","    }\n","\n","  def forward(self, x):\n","    self.conv0_0_out = self.conv0_0(x)\n","    self.conv0_1_out = self.conv0_1(self.conv0_0_out)\n","    self.concat0_out = torch.concat((self.conv0_1_out, x), dim=1)\n","    self.batch_norm0_out = self.batch_norm0(self.concat0_out)\n","    self.activation0_out = self.activation_func(self.batch_norm0_out)\n","\n","    self.conv1_0_out = self.conv1_0(self.activation0_out)\n","    self.conv1_1_out = self.conv1_1(self.conv1_0_out)\n","    self.concat1_out = torch.concat((self.conv1_1_out, self.activation0_out), dim=1)\n","    self.batch_norm1_out = self.batch_norm1(self.concat1_out)\n","    self.activation1_out = self.activation_func(self.batch_norm1_out)\n","\n","    self.conv2_0_out = self.conv2_0(self.activation1_out)\n","    self.conv2_1_out = self.conv2_1(self.conv2_0_out)\n","    self.concat2_out = torch.concat((self.conv2_1_out, self.activation1_out), dim=1)\n","    self.batch_norm2_out = self.batch_norm2(self.concat2_out)\n","    self.activation2_out = self.activation_func(self.batch_norm2_out)\n","\n","    self.trans_layer_out, self.trans_layer_out_indices = self.trans_layer(self.activation2_out)\n","\n","    return self.trans_layer_out\n","\n","class DenseBlock4(nn.Module):\n","  conv0_0_out = None\n","  conv0_1_out = None\n","  batch_norm0_out = None\n","  concat0_out = None\n","  activation0_out = None\n","  conv1_0_out = None\n","  conv1_1_out = None\n","  batch_norm1_out = None\n","  concat1_out = None\n","  activation1_out = None\n","  conv2_0_out = None\n","  conv2_1_out = None\n","  batch_norm2_out = None\n","  concat2_out = None\n","  activation2_out = None\n","  conv3_0_out = None\n","  conv3_1_out = None\n","  batch_norm3_out = None\n","  concat3_out = None\n","  activation3_out = None\n","  trans_layer_out = None\n","  trans_layer_out_indices = None\n","\n","  def __init__(\n","    self,\n","    in_channels: int,\n","    out_channels: int,\n","    growth_rate: int,\n","    transition_layer_theta: float,\n","    device: torch.device = None,\n","  ):\n","    super().__init__()\n","    if device is None:\n","      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.conv0_0 = nn.Conv2d(\n","      in_channels=in_channels,\n","      out_channels=in_channels * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv0_1 = nn.Conv2d(\n","      in_channels=in_channels * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm0 = nn.BatchNorm2d(in_channels + growth_rate, device=device)\n","\n","    self.conv1_0 = nn.Conv2d(\n","      in_channels=in_channels + growth_rate,\n","      out_channels=(in_channels + growth_rate) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv1_1 = nn.Conv2d(\n","      in_channels=(in_channels + growth_rate) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm1 = nn.BatchNorm2d(in_channels + (2 * growth_rate), device=device)\n","\n","    self.conv2_0 = nn.Conv2d(\n","      in_channels=in_channels + (2 * growth_rate),\n","      out_channels=(in_channels + (2 * growth_rate)) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv2_1 = nn.Conv2d(\n","      in_channels=(in_channels + (2 * growth_rate)) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm2 = nn.BatchNorm2d((in_channels + (3 * growth_rate)), device=device)\n","\n","    self.conv3_0 = nn.Conv2d(\n","      in_channels=in_channels + (3 * growth_rate),\n","      out_channels=(in_channels + (3 * growth_rate)) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv3_1 = nn.Conv2d(\n","      in_channels=(in_channels + (3 * growth_rate)) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm3 = nn.BatchNorm2d((in_channels + (4 * growth_rate)), device=device)\n","\n","    trans_kernel_size = int(1/transition_layer_theta)\n","    self.trans_layer = nn.MaxPool3d(kernel_size=(trans_kernel_size, 1, 1), return_indices=True)\n","    self.activation_func = nn.ReLU()\n","\n","  def reconstruction_entries(\n","    self,\n","    index: int,\n","    trans_layer_out: Optional[torch.Tensor] = None,\n","  ) -> dict:\n","    if not isinstance(index, int):\n","      if len(index) > 1:\n","        raise RuntimeError(f\"got multiple indices for DenseBlock4 reconstruction. Only one index at a time is supported currently.\")\n","    \n","    if trans_layer_out is None:\n","      trans_layer_out = self.trans_layer_out[index].unsqueeze(0)\n","\n","    maxunpool = nn.MaxUnpool3d(kernel_size=self.trans_layer.kernel_size)\n","    maxpool_idx = self.trans_layer_out_indices[index].unsqueeze(0)\n","    trans_layer_unpool = maxunpool(trans_layer_out, maxpool_idx).squeeze()\n","    trans_layer_act = self.activation_func(trans_layer_unpool.unsqueeze(0))\n","    \n","    weights = [\n","        self.conv3_1.weight, self.conv3_0.weight, self.conv2_1.weight,\n","        self.conv2_0.weight, self.conv1_1.weight, self.conv1_0.weight,\n","        self.conv0_1.weight, self.conv0_0.weight,\n","    ]\n","    # padding, kernel, stride\n","    deconv_shape_determinations = [\n","        (1, 3, 1), (0, 1, 1), (1, 3, 1), (0, 1, 1), (1, 3, 1), (0, 1, 1),\n","        (1, 3, 1), (0, 1, 1),\n","    ]\n","    # stride values last entry in deconv_shape_determinations\n","    deconv_shape_upsample_factors = [1, 1, 1, 1, 1, 1, 1, 1]\n","    reconstruction_inputs = [\n","        trans_layer_act[:, :self.conv3_1_out.shape[1]],\n","        self.conv3_0_out[index],\n","        self.activation2_out[index, :self.conv2_1_out.shape[1]],\n","        self.conv2_0_out[index],\n","        self.activation1_out[index, :self.conv1_1_out.shape[1]],\n","        self.conv1_0_out[index],\n","        self.activation0_out[index, :self.conv0_1_out.shape[1]],\n","        self.conv0_0_out[index],\n","    ]\n","    activation_layers = [\n","        self.activation_func,\n","        None,\n","        self.activation_func,\n","        None,\n","        self.activation_func,\n","        None,\n","        self.activation_func,\n","        None,\n","    ]\n","    indexing_for_unconcatenating = [\n","        None,\n","        self.conv2_1_out.shape[1],\n","        None,\n","        self.conv1_1_out.shape[1],\n","        None,\n","        self.conv0_1_out.shape[1],\n","        None,\n","        None,\n","    ]\n","\n","    return {\n","      \"weights\": weights,\n","      \"deconv_shape_determinations\": deconv_shape_determinations,\n","      \"deconv_shape_upsample_factors\": deconv_shape_upsample_factors,\n","      \"reconstruction_inputs\": reconstruction_inputs,\n","      \"activation_layers\": activation_layers,\n","      \"indexing_for_unconcatenating\": indexing_for_unconcatenating,\n","    }\n","\n","  def forward(self, x):\n","    self.conv0_0_out = self.conv0_0(x)\n","    self.conv0_1_out = self.conv0_1(self.conv0_0_out)\n","    self.concat0_out = torch.concat((self.conv0_1_out, x), dim=1)\n","    self.batch_norm0_out = self.batch_norm0(self.concat0_out)\n","    self.activation0_out = self.activation_func(self.batch_norm0_out)\n","\n","    self.conv1_0_out = self.conv1_0(self.activation0_out)\n","    self.conv1_1_out = self.conv1_1(self.conv1_0_out)\n","    self.concat1_out = torch.concat((self.conv1_1_out, self.activation0_out), dim=1)\n","    self.batch_norm1_out = self.batch_norm1(self.concat1_out)\n","    self.activation1_out = self.activation_func(self.batch_norm1_out)\n","\n","    self.conv2_0_out = self.conv2_0(self.activation1_out)\n","    self.conv2_1_out = self.conv2_1(self.conv2_0_out)\n","    self.concat2_out = torch.concat((self.conv2_1_out, self.activation1_out), dim=1)\n","    self.batch_norm2_out = self.batch_norm2(self.concat2_out)\n","    self.activation2_out = self.activation_func(self.batch_norm2_out)\n","\n","    self.conv3_0_out = self.conv3_0(self.activation2_out)\n","    self.conv3_1_out = self.conv3_1(self.conv3_0_out)\n","    self.concat3_out = torch.concat((self.conv3_1_out, self.activation2_out), dim=1)\n","    self.batch_norm3_out = self.batch_norm3(self.concat3_out)\n","    self.activation3_out = self.activation_func(self.batch_norm3_out)\n","\n","    self.trans_layer_out, self.trans_layer_out_indices = self.trans_layer(self.activation3_out)\n","\n","    return self.trans_layer_out\n","\n","# class DenseBlock(nn.Module):\n","#   def __init__(\n","#     self,\n","#     n_conv: int,\n","#     in_channels: int,\n","#     out_channels: int,\n","#     growth_rate: int,\n","#     transition_layer_theta: float,\n","#     device: torch.device = None,\n","#   ):\n","#     super().__init__()\n","#     self.modules = []\n","#     self.batch_norms = []\n","#     if device is None:\n","#       device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#     for idx in range(n_conv):\n","#       in_channels_with_growth = in_channels + (idx * growth_rate)\n","#       out_channels_with_growth = in_channels + ((idx + 1) * growth_rate)\n","#       self.modules.append(\n","#         [\n","#           nn.Conv2d(\n","#             in_channels=in_channels_with_growth,\n","#             out_channels=in_channels_with_growth * 2,\n","#             kernel_size=(1, 1),\n","#             padding=0,\n","#             stride=(1, 1),\n","#             device=device,\n","#           ),\n","#           nn.Conv2d(\n","#             in_channels=in_channels_with_growth * 2,\n","#             out_channels=growth_rate,\n","#             kernel_size=(3, 3),\n","#             padding=1,\n","#             stride=(1, 1),\n","#             device=device,\n","#           ),\n","#         ]\n","#       )\n","#       self.batch_norms.append(nn.BatchNorm2d(out_channels_with_growth, device=device))\n","#     trans_kernel_size = int(1 / transition_layer_theta)\n","#     self.trans_layer = nn.MaxPool3d(kernel_size=(trans_kernel_size, 1, 1))\n","#     self.activation_func = nn.ReLU()\n","\n","#   def forward(self, x):\n","#     layer_outputs = [x]\n","#     for d_block, batch_norm in zip(self.modules, self.batch_norms):\n","#       for module in d_block:\n","#         x = module(x)\n","#       x = torch.concat((x, layer_outputs[-1]), dim=1)\n","#       x = batch_norm(x)\n","#       x = self.activation_func(x)\n","#       layer_outputs.append(x)\n","\n","#     x = self.trans_layer(x)\n","#     return x\n","\n","\n","class CFFNEnergyFunction(nn.Module):\n","  loss = None\n","\n","  def __init__(self, batch_size: int = 88, m_th: float = 0.5, device=device):\n","    super().__init__()\n","    self.m_th = torch.empty(batch_size, device=device).fill_(m_th)\n","    self.zero_tensor = torch.empty(batch_size, device=device).fill_(0)\n","    self.energy_function = nn.MSELoss(reduction=\"none\")\n","\n","  def forward(self, img0, img1, pairs_indicator):\n","    E_w = torch.mean(self.energy_function(img0, img1), dim=1)\n","    real_pairs = (0.5 * torch.mul(pairs_indicator, torch.pow(E_w, 2)))\n","    fake_pairs = torch.mul(\n","        (1 - pairs_indicator),\n","        torch.max(self.zero_tensor, self.energy_function(self.m_th, E_w))\n","        )\n","    self.loss = torch.mean(torch.add(real_pairs, fake_pairs))\n","\n","    return self.loss\n","\n","  def item(self):\n","    return self.loss.item()\n","\n","\n","class CFFN(nn.Module):\n","  conv0_out = None\n","  batch_norm0_out = None\n","  activation0_out = None\n","  dense_conv1_out = None\n","  dense_conv2_out = None\n","  dense_conv3_out = None\n","  dense_conv4_out = None\n","  conv5_out = None\n","  batch_norm5_out = None\n","  activation5_out = None\n","\n","  def __init__(\n","    self,\n","    input_image_shape: Tuple[int, int],\n","    growth_rate: int = 24,\n","    transition_layer_theta: float = 0.5,\n","    learning_rate: float = 1e-3,\n","    m_th: float = 0.5,  # threshold for contrastive loss\n","    batch_size: int = 88,\n","    device: torch.device = None,\n","  ):\n","    super().__init__()\n","    self.conv0 = nn.Conv2d(in_channels=3, out_channels=48, kernel_size=(7, 7), stride=(4, 4))\n","    self.batch_norm0 = nn.BatchNorm2d(48)\n","    self.activation0 = nn.ReLU()\n","    if device is None:\n","      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    # self.dense_conv1 = DenseBlock(\n","    #   n_conv=2,\n","    #   in_channels=48,\n","    #   out_channels=48,\n","    #   growth_rate=growth_rate,\n","    #   transition_layer_theta=transition_layer_theta,\n","    #   device=device,\n","    # ).to(device)\n","    self.dense_conv1 = DenseBlock2(\n","      in_channels=48,\n","      out_channels=48,\n","      growth_rate=growth_rate,\n","      transition_layer_theta=transition_layer_theta,\n","      device=device,\n","    )\n","    self.dense_conv2 = DenseBlock3(\n","      in_channels=48,\n","      out_channels=60,\n","      growth_rate=24,\n","      transition_layer_theta=transition_layer_theta,\n","      device=device,\n","    ).to(device)\n","    self.dense_conv3 = DenseBlock4(\n","      in_channels=60,\n","      out_channels=78,\n","      growth_rate=24,\n","      transition_layer_theta=transition_layer_theta,\n","      device=device,\n","    ).to(device)\n","    self.dense_conv4 = DenseBlock2(\n","      in_channels=78,\n","      out_channels=126,\n","      growth_rate=24,\n","      transition_layer_theta=1,\n","      device=device,\n","    ).to(device)\n","    self.conv5 = nn.Conv2d(in_channels=126, out_channels=128, kernel_size=(3, 3))\n","    self.batch_norm5 = nn.BatchNorm2d(128)\n","    self.activation5 = nn.ReLU()\n","    # self.fully_connected: Callable = lambda in_conv_n_channels, conv_shape0, conv_shape1: nn.Sequential(\n","    #   nn.Flatten(),\n","    #   nn.Linear(in_conv_n_channels * conv_shape0 * conv_shape1, 128, device=device),\n","    #   nn.ReLU(),\n","    # )\n","    self.flatten = nn.Flatten()\n","    self.fully_connected3 = nn.Linear(78 * 15 * 15, 128, device=device)\n","    self.fully_connected4 = nn.Linear(126 * 15 * 15, 128, device=device)\n","    self.fully_connected5 = nn.Linear(128 * 13 * 13, 128, device=device)\n","    self.activation_func = nn.ReLU()\n","    self.loss = CFFNEnergyFunction(batch_size=batch_size, m_th=m_th, device=device).to(device)\n","    self.optimizer = torch.optim.Adam(\n","        self.parameters(), lr=learning_rate,\n","    )\n","\n","  def reconstruction(self, index: int, label: str, fig_output_path: Path, original_img: torch.Tensor):\n","    fig_output_path.mkdir(exist_ok=True, parents=True)\n","    original_img_min = torch.min(original_img)\n","    original_img_plt_range = torch.divide(torch.subtract(original_img, original_img_min), torch.max(torch.subtract(original_img, original_img_min)))\n","    original_img_fig_path = fig_output_path / \"original-img.png\"\n","    plt.imsave(str(original_img_fig_path), original_img_plt_range.permute(1, 2, 0).detach().cpu().numpy())\n","    \n","    print(\"Conv0\")\n","    act0 = self.activation0_out[index]\n","    act0 = F.relu(act0)\n","    weights = [self.conv0.weight]\n","    deconv_shape_determinations = [(0, weights[0].shape[-1], 4)]\n","    deconv_shape_upsample_factors = [4]\n","    activation_layers = [None]\n","    indexing_for_unconcatenating = [None]\n","    fig_output_path_ = fig_output_path / \"conv0\"\n","    fig_output_path_.mkdir(exist_ok=True, parents=True)\n","    deconvnet(\n","      act0,\n","      weights,\n","      fig_output_path_,\n","      [label],\n","      deconv_shape_determinations,\n","      deconv_shape_upsample_factors,\n","    )\n","    \n","    dense_conv1_reconstruction_entries = self.dense_conv1.reconstruction_entries(index)\n","    dense_conv1_weights = dense_conv1_reconstruction_entries.get(\"weights\")\n","    dense_conv1_deconv_shape_determinations = dense_conv1_reconstruction_entries.get(\"deconv_shape_determinations\")\n","    dense_conv1_deconv_shape_upsample_factors = dense_conv1_reconstruction_entries.get(\"deconv_shape_upsample_factors\")\n","    dense_conv1_reconstruction_inputs = dense_conv1_reconstruction_entries.get(\"reconstruction_inputs\")\n","    dense_conv1_activation_layers = dense_conv1_reconstruction_entries.get(\"activation_layers\")\n","    dense_conv1_indexing_for_unconcatenating = dense_conv1_reconstruction_entries.get(\"indexing_for_unconcatenating\")\n","    print(f\"DenseConv1\")\n","    for idx in tqdm(range(len(dense_conv1_weights)-1, -1, -1)):\n","      # print(f\"idx = {idx}\")\n","      fig_output_path_ = fig_output_path / \"dense-conv1\" / f\"layer-{len(dense_conv1_weights) - idx}\"\n","      fig_output_path_.mkdir(exist_ok=True, parents=True)\n","      weights_ = dense_conv1_weights[idx:].copy()\n","      weights_.extend(weights.copy())\n","      deconv_shape_determinations_ = dense_conv1_deconv_shape_determinations[idx:].copy()\n","      deconv_shape_determinations_.extend(deconv_shape_determinations.copy())\n","      deconv_shape_upsample_factors_ = dense_conv1_deconv_shape_upsample_factors[idx:].copy()\n","      deconv_shape_upsample_factors_.extend(deconv_shape_upsample_factors.copy())\n","      reconstruction_input = torch.clone(dense_conv1_reconstruction_inputs[idx])\n","      activation_layers_ = dense_conv1_activation_layers[idx:].copy()\n","      activation_layers_.extend(activation_layers.copy())\n","      indexing_for_unconcatenating_ = dense_conv1_indexing_for_unconcatenating[idx:].copy()\n","      indexing_for_unconcatenating_.extend(indexing_for_unconcatenating.copy())\n","      deconvnet(\n","        reconstruction_input,\n","        weights_,\n","        fig_output_path_,\n","        [label],\n","        deconv_shape_determinations_,\n","        deconv_shape_upsample_factors_,\n","        activation_layer=activation_layers_,\n","        indexing_for_unconcatenating=indexing_for_unconcatenating_,\n","      )\n","    \n","    dense_conv2_reconstruction_entries = self.dense_conv2.reconstruction_entries(index)\n","    dense_conv2_weights = dense_conv2_reconstruction_entries.get(\"weights\")\n","    dense_conv2_deconv_shape_determinations = dense_conv2_reconstruction_entries.get(\"deconv_shape_determinations\")\n","    dense_conv2_deconv_shape_upsample_factors = dense_conv2_reconstruction_entries.get(\"deconv_shape_upsample_factors\")\n","    dense_conv2_reconstruction_inputs = dense_conv2_reconstruction_entries.get(\"reconstruction_inputs\")\n","    dense_conv2_activation_layers = dense_conv2_reconstruction_entries.get(\"activation_layers\")\n","    dense_conv2_indexing_for_unconcatenating = dense_conv2_reconstruction_entries.get(\"indexing_for_unconcatenating\")\n","    dense_conv2_maxunpool = [nn.MaxUnpool3d(kernel_size=self.dense_conv1.trans_layer.kernel_size)]\n","    maxunpool_layers_for_dense_conv1 = [None] * (len(dense_conv1_weights) - 1)\n","    dense_conv2_maxunpool.extend(maxunpool_layers_for_dense_conv1)\n","    dense_conv2_maxunpool_indices = [self.dense_conv1.trans_layer_out_indices[index].unsqueeze(0)]\n","    maxunpool_indices = [None] * (len(dense_conv1_weights) - 1)\n","    dense_conv2_maxunpool_indices.extend(maxunpool_indices)\n","\n","    cum_weights = dense_conv1_weights.copy()\n","    cum_weights.extend(weights.copy())\n","    \n","    cum_deconv_shape_determinations = dense_conv1_deconv_shape_determinations.copy()\n","    cum_deconv_shape_determinations.extend(deconv_shape_determinations.copy())\n","    \n","    cum_deconv_shape_upsample_factors = dense_conv1_deconv_shape_upsample_factors.copy()\n","    cum_deconv_shape_upsample_factors.extend(deconv_shape_upsample_factors.copy())\n","\n","    cum_activation_layers = dense_conv1_activation_layers.copy()\n","    cum_activation_layers.extend(activation_layers.copy())\n","\n","    cum_indexing_for_unconcatenating = dense_conv1_indexing_for_unconcatenating.copy()\n","    # cum_indexing_for_unconcatenating[0] = self.dense_conv1.conv1_1_out.shape[1]\n","    cum_indexing_for_unconcatenating.extend(indexing_for_unconcatenating.copy())\n","    \n","    print(\"DenseConv2\")\n","    for idx in tqdm(range(len(dense_conv2_weights)-1, -1, -1)):\n","      # print(f\"idx = {idx}\")\n","      fig_output_path_ = fig_output_path / \"dense-conv2\" / f\"layer-{len(dense_conv2_weights) - idx}\"\n","      fig_output_path_.mkdir(exist_ok=True, parents=True)\n","      weights_ = dense_conv2_weights[idx:].copy()\n","      weights_.extend(cum_weights.copy())\n","      \n","      deconv_shape_determinations_ = dense_conv2_deconv_shape_determinations[idx:].copy()\n","      deconv_shape_determinations_.extend(cum_deconv_shape_determinations.copy())\n","      \n","      deconv_shape_upsample_factors_ = dense_conv2_deconv_shape_upsample_factors[idx:].copy()\n","      deconv_shape_upsample_factors_.extend(cum_deconv_shape_upsample_factors.copy())\n","      \n","      # print(f\"reconstruction_input.shape = {dense_conv2_reconstruction_inputs[idx].shape}\")\n","      reconstruction_input = torch.clone(dense_conv2_reconstruction_inputs[idx])\n","      \n","      activation_layers_ = dense_conv2_activation_layers[idx:].copy()\n","      activation_layers_[-1] = self.dense_conv1.activation_func\n","      activation_layers_.extend(cum_activation_layers.copy())\n","      \n","      indexing_for_unconcatenating_ = dense_conv2_indexing_for_unconcatenating[idx:].copy()\n","      indexing_for_unconcatenating_[-1] = self.dense_conv1.conv1_1_out.shape[1]\n","      indexing_for_unconcatenating_.extend(cum_indexing_for_unconcatenating.copy())\n","      \n","      maxunpool_layers_ = [None] * (len(dense_conv2_weights) - idx)\n","      maxunpool_layers_.extend(dense_conv2_maxunpool.copy())\n","      maxunpool_indices_ = [None] * (len(dense_conv2_weights) - idx)\n","      maxunpool_indices_.extend(dense_conv2_maxunpool_indices.copy())\n","      # for weight in weights_:\n","      #   print(f\"weight.shape: '{weight.shape}'\")\n","      # print(f\"indexing_for_unconcatenating_: {indexing_for_unconcatenating_}\")\n","      # print(f\"reconstruction_input.shape: '{reconstruction_input.shape}'\")\n","      # print(f\"activation_layers_ = {activation_layers_}\")\n","      deconvnet(\n","        reconstruction_input,\n","        weights_,\n","        fig_output_path_,\n","        [label],\n","        deconv_shape_determinations_,\n","        deconv_shape_upsample_factors_,\n","        activation_layer=activation_layers_,\n","        maxunpool=maxunpool_layers_,\n","        maxpool_idx=maxunpool_indices_,\n","        indexing_for_unconcatenating=indexing_for_unconcatenating_,\n","      )\n","    \n","    dense_conv3_reconstruction_entries = self.dense_conv3.reconstruction_entries(index)\n","    dense_conv3_weights = dense_conv3_reconstruction_entries.get(\"weights\")\n","    dense_conv3_deconv_shape_determinations = dense_conv3_reconstruction_entries.get(\"deconv_shape_determinations\")\n","    dense_conv3_deconv_shape_upsample_factors = dense_conv3_reconstruction_entries.get(\"deconv_shape_upsample_factors\")\n","    dense_conv3_reconstruction_inputs = dense_conv3_reconstruction_entries.get(\"reconstruction_inputs\")\n","    dense_conv3_activation_layers = dense_conv3_reconstruction_entries.get(\"activation_layers\")\n","    dense_conv3_indexing_for_unconcatenating = dense_conv3_reconstruction_entries.get(\"indexing_for_unconcatenating\")\n","\n","    dense_conv3_maxunpool = [nn.MaxUnpool3d(kernel_size=self.dense_conv2.trans_layer.kernel_size)]\n","    maxunpool_layers_for_dense_conv2 = [None] * (len(dense_conv2_weights) - 1)\n","    dense_conv3_maxunpool.extend(maxunpool_layers_for_dense_conv2)\n","    \n","    dense_conv3_maxunpool_indices = [self.dense_conv2.trans_layer_out_indices[index].unsqueeze(0)]\n","    maxunpool_indices = [None] * (len(dense_conv2_weights) - 1)\n","    dense_conv3_maxunpool_indices.extend(maxunpool_indices)\n","\n","    cum_weights_ = dense_conv2_weights.copy()\n","    cum_weights_.extend(cum_weights.copy())\n","    cum_weights = cum_weights_.copy()\n","\n","    cum_deconv_shape_determinations_ = dense_conv2_deconv_shape_determinations.copy()\n","    cum_deconv_shape_determinations_.extend(cum_deconv_shape_determinations.copy())\n","    cum_deconv_shape_determinations = cum_deconv_shape_determinations_.copy()\n","\n","    cum_deconv_shape_upsample_factors_ = dense_conv2_deconv_shape_upsample_factors.copy()\n","    cum_deconv_shape_upsample_factors_.extend(cum_deconv_shape_upsample_factors.copy())\n","    cum_deconv_shape_upsample_factors = cum_deconv_shape_upsample_factors_.copy()\n","\n","    cum_activation_layers_ = dense_conv2_activation_layers.copy()\n","    cum_activation_layers_[-1] = self.dense_conv1.activation_func\n","    cum_activation_layers_.extend(cum_activation_layers.copy())\n","    cum_activation_layers = cum_activation_layers_.copy()\n","\n","    cum_indexing_for_unconcatenating_ = dense_conv2_indexing_for_unconcatenating.copy()\n","    cum_indexing_for_unconcatenating_[-1] = self.dense_conv1.conv1_1_out.shape[1]\n","    cum_indexing_for_unconcatenating_.extend(cum_indexing_for_unconcatenating.copy())\n","    cum_indexing_for_unconcatenating = cum_indexing_for_unconcatenating_.copy()\n","\n","    cum_maxunpool_layers = [None] * (len(dense_conv2_weights) - 1)\n","    cum_maxunpool_layers.extend(dense_conv2_maxunpool.copy())\n","    \n","    cum_maxunpool_indices = [None] * (len(dense_conv2_weights) - 1)\n","    cum_maxunpool_indices.extend(dense_conv2_maxunpool_indices.copy())\n","    \n","    print(\"DenseConv3\")\n","    for idx in tqdm(range(len(dense_conv3_weights)-1, -1, -1)):\n","      # print(f\"idx = {idx}\")\n","      fig_output_path_ = fig_output_path / \"dense-conv3\" / f\"layer-{len(dense_conv3_weights) - idx}\"\n","      fig_output_path_.mkdir(exist_ok=True, parents=True)\n","      weights_ = dense_conv3_weights[idx:].copy()\n","      weights_.extend(cum_weights.copy())\n","\n","      deconv_shape_determinations_ = dense_conv3_deconv_shape_determinations[idx:].copy()\n","      deconv_shape_determinations_.extend(cum_deconv_shape_determinations.copy())\n","      \n","      deconv_shape_upsample_factors_ = dense_conv3_deconv_shape_upsample_factors[idx:].copy()\n","      deconv_shape_upsample_factors_.extend(cum_deconv_shape_upsample_factors.copy())\n","      \n","      # print(f\"reconstruction_input.shape = {dense_conv3_reconstruction_inputs[idx].shape}\")\n","      reconstruction_input = torch.clone(dense_conv3_reconstruction_inputs[idx])\n","      \n","      activation_layers_ = dense_conv3_activation_layers[idx:].copy()\n","      activation_layers_[-1] = self.dense_conv2.activation_func\n","      activation_layers_.extend(cum_activation_layers.copy())\n","      \n","      indexing_for_unconcatenating_ = dense_conv3_indexing_for_unconcatenating[idx:].copy()\n","      indexing_for_unconcatenating_[-1] = self.dense_conv2.conv2_1_out.shape[1]\n","      indexing_for_unconcatenating_.extend(cum_indexing_for_unconcatenating.copy())\n","      \n","      maxunpool_layers_ = [None] * (len(dense_conv3_weights) - idx)\n","      maxunpool_layers_.extend(dense_conv3_maxunpool.copy())\n","      maxunpool_layers_.extend(cum_maxunpool_layers.copy())\n","\n","      maxunpool_indices_ = [None] * (len(dense_conv3_weights) - idx)\n","      maxunpool_indices_.extend(dense_conv3_maxunpool_indices.copy())\n","      maxunpool_indices_.extend(cum_maxunpool_indices.copy())\n","      # for weight in weights_:\n","      #   print(f\"weight.shape: '{weight.shape}'\")\n","      # print(f\"indexing_for_unconcatenating_: {indexing_for_unconcatenating_}\")\n","      # print(f\"reconstruction_input.shape: '{reconstruction_input.shape}'\")\n","      # print(f\"activation_layers_ = {activation_layers_}\")\n","      deconvnet(\n","        reconstruction_input,\n","        weights_,\n","        fig_output_path_,\n","        [label],\n","        deconv_shape_determinations_,\n","        deconv_shape_upsample_factors_,\n","        activation_layer=activation_layers_,\n","        maxunpool=maxunpool_layers_,\n","        maxpool_idx=maxunpool_indices_,\n","        indexing_for_unconcatenating=indexing_for_unconcatenating_,\n","      )\n","\n","    dense_conv4_reconstruction_entries = self.dense_conv4.reconstruction_entries(index)\n","    dense_conv4_weights = dense_conv4_reconstruction_entries.get(\"weights\")\n","    dense_conv4_deconv_shape_determinations = dense_conv4_reconstruction_entries.get(\"deconv_shape_determinations\")\n","    dense_conv4_deconv_shape_upsample_factors = dense_conv4_reconstruction_entries.get(\"deconv_shape_upsample_factors\")\n","    dense_conv4_reconstruction_inputs = dense_conv4_reconstruction_entries.get(\"reconstruction_inputs\")\n","    dense_conv4_activation_layers = dense_conv4_reconstruction_entries.get(\"activation_layers\")\n","    dense_conv4_indexing_for_unconcatenating = dense_conv4_reconstruction_entries.get(\"indexing_for_unconcatenating\")\n","  \n","    dense_conv4_maxunpool = [nn.MaxUnpool3d(kernel_size=self.dense_conv3.trans_layer.kernel_size)]\n","    maxunpool_layers_for_dense_conv3 = [None] * (len(dense_conv3_weights) - 1)\n","    dense_conv4_maxunpool.extend(maxunpool_layers_for_dense_conv3)\n","\n","    dense_conv4_maxunpool_indices = [self.dense_conv3.trans_layer_out_indices[index].unsqueeze(0)]\n","    maxunpool_indices = [None] * (len(dense_conv3_weights) - 1)\n","    dense_conv4_maxunpool_indices.extend(maxunpool_indices)\n","\n","    cum_weights_ = dense_conv3_weights.copy()\n","    cum_weights_.extend(cum_weights.copy())\n","    cum_weights = cum_weights_.copy()\n","\n","    cum_deconv_shape_determinations_ = dense_conv3_deconv_shape_determinations.copy()\n","    cum_deconv_shape_determinations_.extend(cum_deconv_shape_determinations.copy())\n","    cum_deconv_shape_determinations = cum_deconv_shape_determinations_.copy()\n","\n","    cum_deconv_shape_upsample_factors_ = dense_conv3_deconv_shape_upsample_factors.copy()\n","    cum_deconv_shape_upsample_factors_.extend(cum_deconv_shape_upsample_factors.copy())\n","    cum_deconv_shape_upsample_factors = cum_deconv_shape_upsample_factors_.copy()\n","\n","    cum_activation_layers_ = dense_conv3_activation_layers.copy()\n","    cum_activation_layers_[-1] = self.dense_conv2.activation_func\n","    cum_activation_layers_.extend(cum_activation_layers.copy())\n","    cum_activation_layers = cum_activation_layers_.copy()\n","\n","    cum_indexing_for_unconcatenating_ = dense_conv3_indexing_for_unconcatenating.copy()\n","    cum_indexing_for_unconcatenating_[-1] = self.dense_conv2.conv2_1_out.shape[1]\n","    cum_indexing_for_unconcatenating_.extend(cum_indexing_for_unconcatenating.copy())\n","    cum_indexing_for_unconcatenating = cum_indexing_for_unconcatenating_.copy()\n","\n","    cum_maxunpool_layers_ = [None] * (len(dense_conv3_weights) - 1)\n","    cum_maxunpool_layers_.extend(dense_conv3_maxunpool.copy())\n","    cum_maxunpool_layers_.extend(cum_maxunpool_layers.copy())\n","    cum_maxunpool_layers = cum_maxunpool_layers_.copy()\n","\n","    cum_maxunpool_indices_ = [None] * (len(dense_conv3_weights) - 1)\n","    cum_maxunpool_indices_.extend(dense_conv3_maxunpool_indices.copy())\n","    cum_maxunpool_indices_.extend(cum_maxunpool_indices.copy())\n","    cum_maxunpool_indices = cum_maxunpool_indices_.copy()\n","\n","    print(\"DenseConv4\")\n","    for idx in tqdm(range(len(dense_conv4_weights)-1, -1, -1)):\n","      # print(f\"idx = {idx}\")\n","      fig_output_path_ = fig_output_path / \"dense-conv4\" / f\"layer-{len(dense_conv4_weights) - idx}\"\n","      fig_output_path_.mkdir(exist_ok=True, parents=True)\n","      weights_ = dense_conv4_weights[idx:].copy()\n","      weights_.extend(cum_weights.copy())\n","\n","      deconv_shape_determinations_ = dense_conv4_deconv_shape_determinations[idx:].copy()\n","      deconv_shape_determinations_.extend(cum_deconv_shape_determinations.copy())\n","      \n","      deconv_shape_upsample_factors_ = dense_conv4_deconv_shape_upsample_factors[idx:].copy()\n","      deconv_shape_upsample_factors_.extend(cum_deconv_shape_upsample_factors.copy())\n","      \n","      # print(f\"reconstruction_input.shape = {dense_conv3_reconstruction_inputs[idx].shape}\")\n","      reconstruction_input = torch.clone(dense_conv4_reconstruction_inputs[idx])\n","      \n","      activation_layers_ = dense_conv4_activation_layers[idx:].copy()\n","      activation_layers_[-1] = self.dense_conv3.activation_func\n","      activation_layers_.extend(cum_activation_layers.copy())\n","      \n","      indexing_for_unconcatenating_ = dense_conv4_indexing_for_unconcatenating[idx:].copy()\n","      indexing_for_unconcatenating_[-1] = self.dense_conv3.conv3_1_out.shape[1]\n","      indexing_for_unconcatenating_.extend(cum_indexing_for_unconcatenating.copy())\n","      \n","      maxunpool_layers_ = [None] * (len(dense_conv4_weights) - idx)\n","      maxunpool_layers_.extend(dense_conv4_maxunpool.copy())\n","      maxunpool_layers_.extend(cum_maxunpool_layers.copy())\n","\n","      maxunpool_indices_ = [None] * (len(dense_conv4_weights) - idx)\n","      maxunpool_indices_.extend(dense_conv4_maxunpool_indices.copy())\n","      maxunpool_indices_.extend(cum_maxunpool_indices.copy())\n","      # for weight in weights_:\n","      #   print(f\"weight.shape: '{weight.shape}'\")\n","      # print(f\"indexing_for_unconcatenating_: {indexing_for_unconcatenating_}\")\n","      # print(f\"reconstruction_input.shape: '{reconstruction_input.shape}'\")\n","      # print(f\"activation_layers_ = {activation_layers_}\")\n","      deconvnet(\n","        reconstruction_input,\n","        weights_,\n","        fig_output_path_,\n","        [label],\n","        deconv_shape_determinations_,\n","        deconv_shape_upsample_factors_,\n","        activation_layer=activation_layers_,\n","        maxunpool=maxunpool_layers_,\n","        maxpool_idx=maxunpool_indices_,\n","        indexing_for_unconcatenating=indexing_for_unconcatenating_,\n","      )\n","  \n","    print(\"Conv5\")\n","    act5 = self.activation5_out[index]\n","    ac5 = F.relu(act5)\n","    cum_weights_ = [self.conv5.weight]\n","    cum_weights_.extend(dense_conv4_weights.copy())\n","    cum_weights_.extend(cum_weights.copy())\n","    cum_weights = cum_weights_.copy()\n","\n","    cum_deconv_shape_determinations_ = [(0, 3, 0)]\n","    cum_deconv_shape_determinations_.extend(dense_conv4_deconv_shape_determinations.copy())\n","    cum_deconv_shape_determinations_.extend(cum_deconv_shape_determinations.copy())\n","    cum_deconv_shape_determinations = cum_deconv_shape_determinations_.copy()\n","\n","    cum_deconv_shape_upsample_factors_ = [0]\n","    cum_deconv_shape_upsample_factors_.extend(dense_conv4_deconv_shape_upsample_factors.copy())\n","    cum_deconv_shape_upsample_factors_.extend(cum_deconv_shape_upsample_factors.copy())\n","    cum_deconv_shape_upsample_factors = cum_deconv_shape_upsample_factors_.copy()\n","\n","    cum_activation_layers_ = [None]\n","    cum_activation_layers_.extend(dense_conv4_activation_layers.copy())\n","    cum_activation_layers_[-1] = self.dense_conv3.activation_func\n","    cum_activation_layers_.extend(cum_activation_layers.copy())\n","    cum_activation_layers = cum_activation_layers_.copy()\n","\n","    cum_indexing_for_unconcatenating_ = [self.dense_conv4.conv1_1_out.shape[1]]\n","    cum_indexing_for_unconcatenating_.extend(dense_conv4_indexing_for_unconcatenating.copy())\n","    cum_indexing_for_unconcatenating_[-1] = self.dense_conv3.conv3_1_out.shape[1]\n","    cum_indexing_for_unconcatenating_.extend(cum_indexing_for_unconcatenating.copy())\n","    cum_indexing_for_unconcatenating = cum_indexing_for_unconcatenating_.copy()\n","\n","    cum_maxunpool_layers_ = [None]\n","    cum_maxunpool_layers_.extend([None] * (len(dense_conv4_weights) - 1))\n","    cum_maxunpool_layers_.extend(dense_conv4_maxunpool.copy())\n","    cum_maxunpool_layers_.extend(cum_maxunpool_layers.copy())\n","    cum_maxunpool_layers = cum_maxunpool_layers_.copy()\n","\n","    cum_maxunpool_indices_ = [None]\n","    cum_maxunpool_indices_.extend([None] * (len(dense_conv4_weights) - 1))\n","    cum_maxunpool_indices_.extend(dense_conv4_maxunpool_indices.copy())\n","    cum_maxunpool_indices_.extend(cum_maxunpool_indices.copy())\n","    cum_maxunpool_indices = cum_maxunpool_indices_.copy()\n","\n","    fig_output_path_ = fig_output_path / \"conv5\"\n","    fig_output_path_.mkdir(exist_ok=True, parents=True)\n","\n","    deconvnet(\n","      act5,\n","      cum_weights,\n","      fig_output_path_,\n","      [label],\n","      cum_deconv_shape_determinations,\n","      cum_deconv_shape_upsample_factors,\n","      activation_layer=cum_activation_layers,\n","      maxunpool=cum_maxunpool_layers,\n","      maxpool_idx=cum_maxunpool_indices,\n","      indexing_for_unconcatenating=cum_indexing_for_unconcatenating,\n","    )\n","\n","  def forward(self, x):\n","    self.conv0_out = self.conv0(x)\n","    self.batch_norm0_out = self.batch_norm0(self.conv0_out)\n","    self.activation0_out = self.activation0(self.batch_norm0_out)\n","    \n","    self.dense_conv1_out = self.dense_conv1(self.activation0_out)\n","    # print(f\"dense_conv1_out.shape = '{self.dense_conv1_out.shape}'\")\n","    self.dense_conv2_out = self.dense_conv2(self.dense_conv1_out)\n","    # print(f\"dense_conv2_out.shape = '{self.dense_conv2_out.shape}'\")\n","    self.dense_conv3_out = self.dense_conv3(self.dense_conv2_out)\n","    # print(f\"dense_conv3_out.shape = '{self.dense_conv3_out.shape}'\")\n","    self.dense_conv4_out = self.dense_conv4(self.dense_conv3_out)\n","    # print(f\"dense_conv4_out.shape = '{self.dense_conv4_out.shape}'\")\n","    self.conv5_out = self.conv5(self.dense_conv4_out)\n","    # print(f\"conv5_out.shape = '{self.conv5_out.shape}'\")\n","    self.batch_norm5_out = self.batch_norm5(self.conv5_out)\n","    self.activation5_out = self.activation5(self.batch_norm5_out)\n","\n","    # fn5_module = self.fully_connected(*self.activation5_out.shape[1:])\n","    # fn5 = fn5_module(self.activation5_out)\n","    activation5_flattened = self.flatten(self.activation5_out)\n","    fn5_out = self.fully_connected5(activation5_flattened)\n","    fn5 = self.activation_func(fn5_out)\n","\n","    # fn4_module = self.fully_connected(*self.dense_conv4_out.shape[1:])\n","    # fn4 = fn4_module(self.dense_conv4_out)\n","    dense_conv4_flattened = self.flatten(self.dense_conv4_out)\n","    fn4_out = self.fully_connected4(dense_conv4_flattened)\n","    fn4 = self.activation_func(fn4_out)\n","\n","    # fn3_module = self.fully_connected(*self.dense_conv3_out.shape[1:])\n","    # fn3 = fn3_module(self.dense_conv3_out)\n","    dense_conv3_flattened = self.flatten(self.dense_conv3_out)\n","    fn3_out = self.fully_connected3(dense_conv3_flattened)\n","    fn3 = self.activation_func(fn3_out)\n","\n","    x_out = torch.cat((fn5, fn4, fn3), dim=1)\n","\n","    # return output of convolution 5, which will be input to classification network\n","    # x_out is discriminative features output used for the pairwise learning for CFFN network\n","    return self.activation5_out, x_out\n","\n","  def loss_back_grad(self, img0, img1, pairs_indicator, back_grad: bool = True):\n","    criterion = self.loss(img0, img1, pairs_indicator)\n","    if back_grad:\n","      criterion.backward()\n","      self.optimizer.step()"],"metadata":{"id":"9wMVtogPRhft","executionInfo":{"status":"ok","timestamp":1679678742144,"user_tz":240,"elapsed":133,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","TupleType = Union[Tuple[Union[int, float], Union[int, float]], List[Union[int, float]], torch.Size]\n","\n","def check_tuples_for_shape_determination(\n","  padding_size: Union[int, TupleType],\n","  filter_size: Union[int, TupleType],\n","  stride_size: Union[int, TupleType],\n",") -> Tuple[TupleType, TupleType, TupleType]:\n","  if isinstance(padding_size, int):\n","    padding_size = [padding_size] * 2\n","  if len(padding_size) != 2:\n","    raise ValueError(f\"padding_size length is not '2'. Got '{len(padding_size)}'\")\n","  if isinstance(filter_size, int):\n","    filter_size = [filter_size] * 2\n","  if len(filter_size) != 2:\n","    raise ValueError(f\"filter_size length is not '2'. Got '{len(filter_size)}'\")\n","  if isinstance(stride_size, int):\n","    if stride_size == 0:\n","      stride_size = 1\n","    stride_size = [stride_size] * 2\n","  elif 0 in stride_size:\n","    stride_size = [1] * 2\n","  if len(stride_size) != 2:\n","    raise ValueError(f\"stride_size length is not '2'. Got '{len(stride_size)}'\")\n","\n","  return padding_size, filter_size, stride_size\n","\n","\n","def deconv2d_out_shape_(\n","  input_size: int,\n","  padding_size: int,\n","  filter_size: int,\n","  stride_size: int,\n",") -> int:\n","  return int((stride_size * (input_size - 1)) + filter_size - (2 * padding_size))\n","\n","\n","def deconv2d_out_shape(\n","  input_shape: Tuple[int, int],\n","  padding_size: Union[int, TupleType],\n","  filter_size: Union[int, TupleType],\n","  stride_size: Union[int, TupleType],\n",") -> Tuple[int, int]:\n","  padding_size, filter_size, stride_size = check_tuples_for_shape_determination(padding_size, filter_size, stride_size)\n","  width = deconv2d_out_shape_(input_shape[0], padding_size[0], filter_size[0], stride_size[0])\n","  height = deconv2d_out_shape_(input_shape[1], padding_size[1], filter_size[1], stride_size[1])\n","\n","  if padding_size[0] > 0:\n","    width += 1\n","  if padding_size[1] > 0:\n","    height += 1\n","  return width, height\n","\n","\n","def deconvnet(\n","  activation_output: torch.Tensor,\n","  weights: Union[List[torch.Tensor], torch.Tensor],\n","  figures_output_path: Path,\n","  labels: List[Union[str, int]],\n","  deconv_shape_determinations: Optional[Tuple[int, int, int]] = None,\n","  deconv_shape_upsample_factors: Optional[Tuple[int]] = None,\n","  maxunpool=None,\n","  maxpool_idx=None,\n","  activation_layer=None,\n","  indexing_for_unconcatenating: Optional[List[int]] = None,\n","  upsample: bool = False,\n","  upsample_size: Optional[Tuple[int, int]] = None,\n","  upsample_mode: str = \"nearest\",\n","  only_upsample: bool = False,\n","  save_outputs: bool = True,\n",") -> torch.Tensor:\n","  if activation_output.ndim == 3:\n","    activation_output = activation_output.unsqueeze(0)\n","  elif activation_output.ndim != 4:\n","    raise ValueError(f\"Expecting activation output when reconstructing to have 3 or 4 channels: (batches, channles, width, height). Got '{activation_output.shape}'\")\n","  \n","  if only_upsample and not upsample:\n","    raise RuntimeError(\"If desiring to only upsample outputs, you must also turn on the 'upsample' flag.\")\n","  if upsample and upsample_size is None:\n","    raise RuntimeError(\"When upsampling, you must specify a desired size to upsample to.\")\n","  \n","  if not isinstance(weights, list):\n","    weights = [weights]\n","  if not isinstance(activation_layer, list):\n","    activation_layer = [activation_layer] * len(weights)\n","  if not isinstance(indexing_for_unconcatenating, list):\n","    indexing_for_unconcatenating = [indexing_for_unconcatenating] * len(weights)\n","  \n","  # determine output shape from deconvolutions\n","  out_shape = activation_output.shape[-2:]\n","  if maxunpool is not None:\n","    if not isinstance(maxunpool, list):\n","      maxunpool = [maxunpool]\n","    if not isinstance(maxpool_idx, list):\n","      maxpool_idx = [maxpool_idx]\n","  if maxunpool is not None:\n","    for idx, weight in enumerate(weights[:-1]):\n","      if maxunpool[idx] is not None:\n","        out_shape = deconv2d_out_shape(out_shape, 0, weight.shape[-2:], 1)[0] * maxunpool[idx].stride[0]\n","        out_shape = [out_shape] * 2\n","  elif deconv_shape_determinations is not None:\n","    for weight, deconv_shape_determination in zip(weights[:-1], deconv_shape_determinations):\n","      out_shape = deconv2d_out_shape(out_shape, *deconv_shape_determination)\n","  \n","  # deconv_out = torch.empty((activation_output.shape[0], weights[0].shape[-1], out_shape[0], out_shape[1]), dtype=activation_output.dtype).to(activation_output.device)\n","  for idx, act_out in enumerate(torch.swapaxes(activation_output, 0, 1)):\n","    # act_out_zeros = torch.zeros(activation_output.shape, dtype=activation_output.dtype).to(activation_output.device)\n","    # act_out_zeros[:, idx, :, :] = act_out\n","    for weight_idx, weight in enumerate(weights):\n","      upsample_for_deconv: bool = True\n","      if weight_idx == len(weights) - 1 or maxunpool is None:\n","        if deconv_shape_upsample_factors is not None:\n","          padding = deconv_shape_upsample_factors[weight_idx] + 1\n","          # padding = \"same\"\n","          scale_factor = deconv_shape_upsample_factors[weight_idx]\n","        else:\n","          upsample_for_deconv = False\n","          padding = \"same\"\n","      elif maxunpool is not None and maxunpool[weight_idx] is not None and deconv_shape_upsample_factors is None:\n","        upsample_for_deconv = False\n","        padding = maxunpool[weight_idx].stride[0] * 2\n","      else:\n","        if deconv_shape_upsample_factors is not None:\n","          padding = deconv_shape_upsample_factors[weight_idx] + 1\n","          # padding = \"same\"\n","          scale_factor = deconv_shape_upsample_factors[weight_idx]\n","        else:\n","          upsample_for_deconv = False\n","          padding = \"same\"\n","      if scale_factor == 0:\n","        scale_factor = 1\n","      \n","      if upsample_for_deconv:\n","        if act_out.ndim == 3:\n","          act_out = act_out.unsqueeze(0)\n","        act_out = F.upsample(act_out, scale_factor=scale_factor)\n","      \n","      if weight_idx == 0:\n","        act_out_zeros = torch.zeros((act_out.shape[0], weight.shape[0], *act_out.shape[-2:]), dtype=activation_output.dtype).to(activation_output.device)\n","        act_out_zeros[:, idx, :, :] = act_out\n","      else:\n","        act_out_zeros = torch.clone(act_out)\n","\n","      # print(f\"act_out_zeros.shape = {act_out_zeros.shape}\")\n","      # print(f\"padding = {padding}\")\n","      # print(f\"deconv_shape_upsample_factors[{weight_idx}] = {deconv_shape_upsample_factors[weight_idx]}\")\n","      # print(f\"weight.shape = {weight.transpose(dim0=2, dim1=3).shape}\")\n","      # print(f\"indexing_for_unconcatenating[{weight_idx}] = {indexing_for_unconcatenating[weight_idx]}\")\n","      deconv_ = F.conv2d(act_out_zeros, weight=torch.swapaxes(weight.transpose(dim0=2, dim1=3), 0, 1), padding=padding).squeeze()\n","      if deconv_.ndim == 3:\n","        deconv_ = deconv_.unsqueeze(0)\n","      # print(f\"deconv_.shape = {deconv_.shape}\")\n","      if weight_idx != len(weights) - 1 and maxunpool is not None and maxunpool[weight_idx] is not None and not upsample_for_deconv:\n","        deconv_ = maxunpool[weight_idx](deconv_, maxpool_idx[weight_idx])\n","        deconv_ = activation_layer[weight_idx](deconv_)\n","      elif upsample_for_deconv:\n","        if activation_layer[weight_idx] is not None:\n","          deconv_ = activation_layer[weight_idx](deconv_)\n","      if deconv_.ndim > 4:\n","        deconv_ = deconv_.squeeze()\n","      if deconv_.ndim == 3:\n","        deconv_ = deconv_.unsqueeze(0)\n","      if indexing_for_unconcatenating[weight_idx] is not None:\n","        deconv_ = deconv_[:, :indexing_for_unconcatenating[weight_idx], :, :]\n","      if deconv_.ndim == 3:\n","        deconv_ = deconv_.unsqueeze(0)\n","      act_out = torch.clone(deconv_)\n","    # deconv_out[:, idx, :, :] = deconv_\n","    \n","    if save_outputs:\n","      if upsample and only_upsample:\n","        # deconv = F.interpolate(deconv_out[:, idx, :, :], size=upsample_size, mode=upsample_mode)\n","        deconv = F.interpolate(deconv_, size=upsample_size, mode=upsample_mode)\n","      else:\n","        # deconv = deconv_out[:, idx, :, :]\n","        deconv = deconv_\n","      if deconv.ndim == 3:\n","        deconv = deconv.unsqueeze(1)\n","      \n","      # print(f\"deconv.shape: {deconv.shape}\")\n","      for batch_idx, (deconv_batch, label) in enumerate(zip(deconv, labels)):\n","        label_fig_path = figures_output_path / f\"label-{label}--batch-idx-{batch_idx}\"\n","        label_fig_path.mkdir(exist_ok=True, parents=True)\n","        kernel_fig_name = f\"kernel-{idx}.png\"\n","        deconv_batch = deconv_batch.squeeze()\n","        batch_min = torch.min(deconv_batch)\n","        batch_max = torch.max(deconv_batch)\n","        # print(f\"deconv_batch min = {batch_min}\")\n","        # print(f\"deconb_batch max = {batch_max}\")\n","        deconv_batch = torch.divide(torch.subtract(deconv_batch, batch_min), torch.max(torch.subtract(deconv_batch, batch_min)))\n","        if not only_upsample:\n","          kernel_fig_path_default_size = label_fig_path / \"default_size\"\n","          kernel_fig_path_default_size.mkdir(exist_ok=True)\n","          # print(f\"deconv_batch max = {torch.max(deconv_batch)}\")\n","          # print(f\"deconv_batch.shape = {deconv_batch.permute(1, 2, 0).shape}\")\n","          plt.imsave(str(kernel_fig_path_default_size / kernel_fig_name), deconv_batch.permute(1, 2, 0).detach().cpu().numpy())\n","        if upsample:\n","          kernel_fig_path_upsampled = label_fig_path / \"upsampled\"\n","          kernel_fig_path_upsampled.mkdir(exist_ok=True)\n","          if not only_upsample:\n","            deconv_batch = F.interpolate(deconv_batch.unsqueeze(0).unsqueeze(0), size=upsample_size, mode=upsample_mode).squeeze()\n","          plt.imsave(str(kernel_fig_path_upsampled / kernel_fig_name), deconv_batch.permute(1, 2, 0).detach().cpu().numpy())"],"metadata":{"id":"EwcuQOoIAu50","executionInfo":{"status":"ok","timestamp":1679678742147,"user_tz":240,"elapsed":128,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["cffn = CFFN(\n","    input_image_shape=(64, 64),\n","    growth_rate=config[\"growth_rate\"],\n","    transition_layer_theta=config[\"transition_layer_theta\"],\n","    learning_rate=config[\"lr\"],\n","    m_th=config[\"m_th\"],\n","    batch_size=config[\"batch_size\"],\n","    device=config[\"device\"],\n","    ).to(config[\"device\"])"],"metadata":{"id":"Pqn7Uv02CtJQ","executionInfo":{"status":"ok","timestamp":1679678773996,"user_tz":240,"elapsed":1673,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["**2. Classification Network**\n","\n","\"The classification sub-network consists of a convolution layer with two channels, and a fully connected layer with two neurons.\""],"metadata":{"id":"YS-iJPjLUe4v"}},{"cell_type":"code","source":["class ClassificationNetwork(nn.Module):\n","  conv_layer_out = None\n","  activation_out = None\n","  global_avg_pool_out = None\n","  flatten_out = None\n","  fully_connected_out = None\n","  softmax_out = None\n","\n","  def __init__(self, learning_rate: float = 1e-3):\n","    super().__init__()\n","    self.conv_layer = nn.Conv2d(in_channels=128, out_channels=2, kernel_size=(3, 3))\n","    self.activation = nn.ReLU()\n","    self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","    self.flatten = nn.Flatten()\n","    self.fully_connected = nn.Linear(2, 2)\n","    self.softmax = nn.Softmax(dim=1)\n","\n","    self.loss = nn.BCELoss()\n","\n","    self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n","\n","  def forward(self, x):\n","    self.conv_layer_out = self.conv_layer(x)\n","    self.activation_out = self.activation(self.conv_layer_out)\n","    self.global_avg_pool_out = self.global_avg_pool(self.activation_out)\n","    self.flatten_out = self.flatten(self.global_avg_pool_out)\n","    self.fully_connected_out = self.fully_connected(self.flatten_out)\n","    self.softmax_out = self.softmax(self.fully_connected_out)\n","\n","    return self.softmax_out"],"metadata":{"id":"rOfO9_JgC3RV","executionInfo":{"status":"ok","timestamp":1679678742149,"user_tz":240,"elapsed":127,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["cn = ClassificationNetwork().to(device)"],"metadata":{"id":"fO06QNwq5pCA","executionInfo":{"status":"ok","timestamp":1679678747464,"user_tz":240,"elapsed":5438,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["base_models_path = Path(\"/content/drive/MyDrive/ECE 792 - Advance Topics in Machine Learning/Code/DeepFakeImageDetection\")"],"metadata":{"id":"aKM1P80SedQO","executionInfo":{"status":"ok","timestamp":1679692521020,"user_tz":240,"elapsed":141,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["base_cffn_model_path = base_models_path / \"CFFN\" / \"model\" / \"models\"\n","cffn_model_path = get_latest_model(base_cffn_model_path)\n","print(f\"CFFN model path: '{cffn_model_path}'\")\n","base_cn_model_path = base_models_path / \"CN\" / \"model\" / \"models\"\n","cn_model_path = get_latest_model(base_cn_model_path)\n","print(f\"CN model path: '{cn_model_path}'\")\n","\n","cffn_checkpoint = torch.load(str(cffn_model_path), map_location=config[\"device\"])\n","cffn.load_state_dict(cffn_checkpoint[\"CFFN_state_dict\"])\n","cffn.eval()\n","\n","cn_checkpoint = torch.load(str(cn_model_path), map_location=config[\"device\"])\n","cn.load_state_dict(cn_checkpoint[\"CN_state_dict\"])\n","cn.eval()"],"metadata":{"id":"pitoJ0iJofGo","executionInfo":{"status":"ok","timestamp":1679678779908,"user_tz":240,"elapsed":1938,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"18fd9038-5094-4d7b-c473-5214594b920f"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["CFFN model path: '/content/drive/MyDrive/ECE 792 - Advance Topics in Machine Learning/Code/DeepFakeImageDetection/CFFN/model/models/CFFN--15.pth'\n","CN model path: '/content/drive/MyDrive/ECE 792 - Advance Topics in Machine Learning/Code/DeepFakeImageDetection/CN/model/models/model-2023-03-24--17-11-42--25.pth'\n"]},{"output_type":"execute_result","data":{"text/plain":["ClassificationNetwork(\n","  (conv_layer): Conv2d(128, 2, kernel_size=(3, 3), stride=(1, 1))\n","  (activation): ReLU()\n","  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (fully_connected): Linear(in_features=2, out_features=2, bias=True)\n","  (softmax): Softmax(dim=1)\n","  (loss): BCELoss()\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","visualize_latent_dim_from_cffn: bool = True\n","batch_idx_to_stop: Optional[int] = 1\n","\n","if visualize_latent_dim_from_cffn:\n","  discriminative_features = {}\n","\n","  for batch_idx, (img, label, gan_selection) in tqdm(enumerate(dataloader), total=len(dataloader)):\n","    vals = np.unique(gan_selection)\n","    gan_select_idx = {}\n","    for val in vals:\n","      idx = np.where(np.array(gan_selection) == val)\n","      gan_select_idx[val] = torch.Tensor(idx[0]).to(torch.int64)\n","    \n","    img = img.to(config[\"device\"])\n","    label = label.to(config[\"device\"])\n","\n","    img_cffn, img_discriminative_features = cffn(img)\n","\n","    for gan_name, idx in gan_select_idx.items():\n","      feats = img_discriminative_features[gan_select_idx[gan_name]]\n","      temp = discriminative_features.get(gan_name)\n","      if temp is None:\n","        temp = torch.clone(feats).to(torch.float32).detach().cpu()\n","      else:\n","        # temp = torch.concat([temp, feats], dim=0)\n","        temp = np.concatenate((temp, feats.to(torch.float32).detach().cpu()))\n","      discriminative_features[gan_name] = temp\n","\n","    if batch_idx_to_stop is not None:\n","      if batch_idx + 1 == batch_idx_to_stop:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GCmpeKty6FDj","executionInfo":{"status":"ok","timestamp":1679678792946,"user_tz":240,"elapsed":8692,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"36a980b6-95ee-4d3f-c9ec-8c7dc70fd09a"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/5029 [00:07<?, ?it/s]\n"]}]},{"cell_type":"code","source":["print(list(gan_select_idx.keys()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ip-w_58L7pUj","executionInfo":{"status":"ok","timestamp":1679678792951,"user_tz":240,"elapsed":138,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"9bd9c90d-a0c0-448b-a8f0-41c6950129fa"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["['CDCGAN', 'DCGAN', 'LSGAN', 'PGGAN', 'WGAN-CP', 'WGAN-GP', 'real']\n"]}]},{"cell_type":"code","source":["fig_output_path = base_models_path / \"CFFN\" / \"model\" / \"reconstruction\"\n","fig_output_path.mkdir(exist_ok=True, parents=True)\n","for gan_name in list(gan_select_idx.keys()):\n","  print(f\"Reconstructing single image for '{gan_name}'\")\n","  gan_fig_output_path = fig_output_path / gan_name\n","  gan_fig_output_path.mkdir(exist_ok=True, parents=True)\n","  cffn.reconstruction(int(gan_select_idx[gan_name][0]), gan_name, gan_fig_output_path, img[gan_select_idx[gan_name][0]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ye46w7YSsmIa","executionInfo":{"status":"ok","timestamp":1679679154651,"user_tz":240,"elapsed":359529,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"a2c0ef1a-ac41-4ab3-d01a-a53489d089fe"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Reconstructing single image for 'CDCGAN'\n","Conv0\n","DenseConv1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:03<00:00,  1.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6/6 [00:07<00:00,  1.31s/it]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 8/8 [00:22<00:00,  2.87s/it]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv4\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:13<00:00,  3.46s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Conv5\n","Reconstructing single image for 'DCGAN'\n","Conv0\n","DenseConv1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:03<00:00,  1.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6/6 [00:08<00:00,  1.42s/it]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 8/8 [00:21<00:00,  2.70s/it]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv4\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:13<00:00,  3.38s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Conv5\n","Reconstructing single image for 'LSGAN'\n","Conv0\n","DenseConv1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:03<00:00,  1.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6/6 [00:07<00:00,  1.31s/it]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 8/8 [00:22<00:00,  2.85s/it]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv4\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:14<00:00,  3.52s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Conv5\n","Reconstructing single image for 'PGGAN'\n","Conv0\n","DenseConv1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:03<00:00,  1.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6/6 [00:08<00:00,  1.45s/it]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 8/8 [00:21<00:00,  2.71s/it]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv4\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:13<00:00,  3.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Conv5\n","Reconstructing single image for 'WGAN-CP'\n","Conv0\n","DenseConv1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:03<00:00,  1.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6/6 [00:07<00:00,  1.31s/it]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 8/8 [00:22<00:00,  2.80s/it]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv4\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:13<00:00,  3.37s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Conv5\n","Reconstructing single image for 'WGAN-GP'\n","Conv0\n","DenseConv1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:03<00:00,  1.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6/6 [00:08<00:00,  1.42s/it]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 8/8 [00:21<00:00,  2.70s/it]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv4\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:13<00:00,  3.38s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Conv5\n","Reconstructing single image for 'real'\n","Conv0\n","DenseConv1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:03<00:00,  1.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6/6 [00:07<00:00,  1.29s/it]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 8/8 [00:22<00:00,  2.76s/it]\n"]},{"output_type":"stream","name":"stdout","text":["DenseConv4\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:13<00:00,  3.47s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Conv5\n"]}]},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","\n","pca_in = None\n","for val in discriminative_features.values():\n","  val = val.numpy()\n","  if pca_in is None:\n","    pca_in = val.copy()\n","  else:\n","    pca_in = np.concatenate((pca_in, val))\n","\n","print(pca_in.shape)\n","\n","pca = PCA(n_components=50)\n","tsne_in = pca.fit_transform(pca_in)\n","\n","print(tsne_in.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"McZi92AJLR3l","executionInfo":{"status":"ok","timestamp":1679679223410,"user_tz":240,"elapsed":418,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"6aae1f89-6dc9-4b5e-94ad-6ac2dc8d902b"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["(88, 384)\n","(88, 50)\n"]}]},{"cell_type":"code","source":["from sklearn.manifold import TSNE\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","%matplotlib inline\n","\n","perplexity = 20\n","learning_rate = 'auto'\n","fig_path = base_models_path / \"CN\" / \"model\" / f\"TSNE-discriminative-features-cffn--perplexity-{perplexity}--lr-{learning_rate}.png\"\n","\n","disc_embed = TSNE(n_components=2, learning_rate=learning_rate, perplexity=perplexity).fit_transform(tsne_in)\n","\n","df = pd.DataFrame()\n","df[\"tsne-2d-one\"] = disc_embed[:, 0]\n","df[\"tsne-2d-two\"] = disc_embed[:, 1]\n","\n","plt.figure(figsize=(16, 10))\n","sns.scatterplot(\n","    # hue=\"y\",\n","    # palette=sns.color_palette(\"hls\", 10),\n","    data=disc_embed,\n","    # legend=df,\n","    alpha=0.3,\n",")\n","# scat_plot.savefig(str(fig_path))\n","plt.savefig(str(fig_path))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":592},"id":"WF9XxazoBDCq","executionInfo":{"status":"ok","timestamp":1679679311907,"user_tz":240,"elapsed":1483,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"9d249dab-1b63-435c-b348-3bb766794f96"},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1152x720 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["from torchvision.utils import make_grid, save_image\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import torch\n","import torchvision.transforms as transforms\n","import os\n","%matplotlib inline\n","\n","# grid reconstructed images\n","\n","seed = 999\n","rng = np.random.default_rng(seed=seed)\n","reconstruction_output_path_phase1_cffn_model = base_models_path / \"CFFN\" / \"model\" / \"reconstruction\"\n","reconstructed_image_paths = [reconstruction_output_path_phase1_cffn_model / path_ for path_ in os.listdir(str(reconstruction_output_path_phase1_cffn_model))]\n","for reconstructed_image_path in reconstructed_image_paths:\n","  conv5_path = reconstructed_image_path / \"conv5\"\n","  grid_output_path = conv5_path / \"grid.png\"\n","  conv5_image_paths = sorted(conv5_path.glob(\"**/*.png\"))\n","  rng.shuffle(conv5_image_paths)\n","\n","  transform = transforms.Compose([transforms.ToTensor(), transforms.CenterCrop(128)])\n","  conv5_images = transform(Image.open(conv5_image_paths[-1]).convert(\"RGB\")).unsqueeze(0)\n","  for idx, conv5_img_path in enumerate(conv5_image_paths):\n","    if idx == len(conv5_image_paths) - 1:\n","      break\n","    conv5_image = transform(Image.open(conv5_img_path).convert(\"RGB\")).unsqueeze(0)\n","    conv5_images = torch.concat([conv5_images, conv5_image], dim=0)\n","\n","  grid_of_imgs = make_grid(conv5_images, padding=2, normalize=True, nrows=16)\n","  # plt.imshow(grid_of_imgs.permute(1, 2, 0).detach().cpu().numpy())\n","  save_image(grid_of_imgs, grid_output_path)"],"metadata":{"id":"O0HJzDvLc9rh","executionInfo":{"status":"ok","timestamp":1679694900801,"user_tz":240,"elapsed":44933,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":67,"outputs":[]}]}