{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dnMNcrwOJ5Eh","executionInfo":{"status":"ok","timestamp":1679444155102,"user_tz":240,"elapsed":27726,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"a76ade71-7131-45b6-88b5-d222b599ff09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","Folder exists\n"]}],"source":["from pathlib import Path\n","USE_COLAB: bool = True\n","dataset_base_path = Path(\"/content/drive/My Drive/ECE 792 - Advance Topics in Machine Learning/Datasets\")\n","if USE_COLAB:\n","  from google.colab import drive\n","  \n","  # Mount the drive to access google shared docs\n","  drive.mount('/content/drive/', force_remount=True)\n","\n","  if dataset_base_path.exists():\n","    print(\"Folder exists\")\n","  else:\n","    print(\"DOESN'T EXIST. Add desired folder as a shortcut in your 'My Drive'\")"]},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dataset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","from typing import Tuple, Optional, List\n","\n","import argparse\n","import os\n","from tqdm import tqdm\n","import time\n","import copy\n","import math\n","from zipfile import ZipFile\n","\n","from PIL import Image\n","from typing import Dict, List, Union\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# PyTorch's versions:\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)\n","print(\"NumPy Version: \",np.__version__)\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F3BAhDaXMYfd","executionInfo":{"status":"ok","timestamp":1679444191435,"user_tz":240,"elapsed":3671,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"48a58df2-7332-4196-f250-4ccaab14f8f4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch Version:  1.13.1+cu116\n","Torchvision Version:  0.14.1+cu116\n","NumPy Version:  1.22.4\n","Wed Mar 22 00:16:31 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   51C    P0    26W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["class CelebrityData(torch.utils.data.Dataset):\n","\n","  def __init__(\n","    self,\n","    base_path: Path,\n","    transform = None,\n","    seed = None,\n","    gans_to_skip: Optional[List[str]] = None,\n","    n_fake_imgs_to_extract: int = 40000,\n","    *,\n","    unzip_real_imgs: bool = True,\n","    ):\n","    '''\n","    Folder structure of base_path should be\n","    base_path -> RealFaces/FakeFaces\n","    RealFaces -> .zip\n","    FakeFaces -> GANType -> .zip (e.g., FakeFaces -> PGGAN -> .zip)\n","    '''\n","    super().__init__()\n","    self.rng = np.random.default_rng(seed)\n","    self.unzip_real_imgs = unzip_real_imgs\n","\n","    self.fake_images_path = Path(base_path) / \"FakeFaces\"\n","    self.fake_image_gan_names = os.listdir(str(self.fake_images_path))\n","    if gans_to_skip is not None:\n","      print(f\"Not unzipping '{gans_to_skip}'\")\n","      self.fake_image_gan_names = list(filter(lambda x: x not in gans_to_skip, self.fake_image_gan_names))\n","    self.fake_images: Dict[str, List[Union[str, Path]]] = {}\n","    for fake_image_gan_name in self.fake_image_gan_names:\n","      print(f\"Extracting imagery for '{fake_image_gan_name}'\")\n","      fake_image_gan_path = self.fake_images_path / fake_image_gan_name\n","      zip_file = sorted(fake_image_gan_path.glob(\"*.zip\"))\n","      if len(zip_file) == 1:\n","        zip_file = zip_file[0]\n","        with ZipFile(str(zip_file), 'r') as zipObj:\n","          zipObj.extractall()\n","        self.fake_images.update({fake_image_gan_name: zipObj.namelist()[:n_fake_imgs_to_extract]})\n","      else:\n","        fake_image_paths = sorted(fake_image_gan_path.glob(\"*.jpg\"))\n","        self.fake_images.update({fake_image_gan_name: fake_image_paths[:n_fake_imgs_to_extract]})\n","\n","    self.n_fake_gans = len(list(self.fake_images.keys()))\n","\n","    if unzip_real_imgs:\n","      self.real_images_path = Path(base_path) / \"RealFaces\"\n","      print(\"Extracting RealFaces imagery\")\n","      real_images_zip_files = sorted(self.real_images_path.glob(\"*.zip\"))\n","      if len(real_images_zip_files) != 1:\n","        raise RuntimeError(f\"Got more than or less than 1 zip file in '{self.real_images_path}'. Got '{len(real_images_zip_files)}'\")\n","      self.real_images_zip_file = real_images_zip_files[0]\n","      # Create a ZipFile Object and load sample.zip in it\n","      with ZipFile(str(self.real_images_zip_file), 'r') as zipObj:\n","        # Extract all the contents of zip file in current directory\n","        zipObj.extractall()\n","      if self.len_of_fake_images >= len(zipObj.namelist()):\n","        self.real_images = zipObj.namelist()[1:]\n","      else:\n","        self.real_images = zipObj.namelist()[1:self.len_of_fake_images+1]\n","    else:\n","      self.real_imgs = []\n","\n","    self.transform = transform\n","\n","    # according to Deep Fake Image Detection Based on Pairwise Learning, we need to make combinations for all\n","    # real images with all fake images\n","    fake_img_list = []\n","    for fake_imgs in self.fake_images.values():\n","      fake_img_list.extend(fake_imgs)\n","    self.fake_img_list = fake_img_list\n","\n","  def fake_image_rand_selection(self, index) -> str:\n","    rand_selection = self.rng.uniform(low=-0.499, high=len(self.fake_images) - 0.501)\n","    gan_selection = self.fake_image_gan_names[int(np.round(rand_selection))]\n","    \n","    return self.fake_images.get(gan_selection)[index // len(self.fake_images)]\n","\n","  @property\n","  def len_of_fake_images(self) -> int:\n","    total_len = 0\n","    for val in self.fake_images.values():\n","      total_len += len(val)\n","\n","    return total_len\n","\n","  def len_of_real_and_fake(self):\n","    return len(self.real_images) + self.len_of_fake_images\n","\n","  def "],"metadata":{"id":"feIzG8dArDEw","executionInfo":{"status":"ok","timestamp":1679444192933,"user_tz":240,"elapsed":6,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from dataclasses import dataclass\n","from typing import List\n","\n","@dataclass\n","class Loss:\n","  loss_vals_per_batch: List[float]\n","  loss_vals_per_epoch: List[float]\n","  batch_cnt: int = 0\n","  previous_batch_cnt: int = 0\n","  epoch_cnt: int = 0\n","\n","  @classmethod\n","  def init(cls) -> \"Loss\":\n","    return cls(\n","        loss_vals_per_batch=[],\n","        loss_vals_per_epoch=[],\n","        batch_cnt=0,\n","        previous_batch_cnt=0,\n","        epoch_cnt=0,\n","    )\n","\n","  def __add__(self, other: Union[float, int]) -> \"Loss\":\n","    self.loss_vals_per_batch.append(other)\n","    self.batch_cnt += 1\n","    return self\n","\n","  def __iadd__(self, other: Union[float, int]) -> \"Loss\":\n","    return self.__add__(other)\n","\n","  @property\n","  def current_loss(self) -> float:\n","    return np.sum(self.loss_vals_per_batch) / self.batch_cnt\n","\n","  @property\n","  def previous_loss(self) -> float:\n","    if len(self.loss_vals_per_batch) > 1:\n","      return np.sum(self.loss_vals_per_batch[:-2]) / (self.batch_cnt - 1)\n","    else:\n","      return 0\n","\n","  def update_for_epoch(self):\n","    self.epoch_cnt += 1\n","    self.loss_vals_per_epoch.append(\n","        sum(self.loss_vals_per_batch[self.previous_batch_cnt:self.batch_cnt])\n","        / (self.batch_cnt - self.previous_batch_cnt)\n","    )\n","    self.previous_batch_cnt = self.batch_cnt"],"metadata":{"id":"Rgg-lMWkxUbU","executionInfo":{"status":"ok","timestamp":1679444196208,"user_tz":240,"elapsed":439,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from typing import Optional\n","\n","def plot_accuracy_or_loss(\n","  train_vals: List[float],\n","  output_path: Union[str, Path],\n","  validation_vals: Optional[List[float]] = None,\n","  test_vals: Optional[List[float]] = None,\n","  title: Optional[str] = None,\n","  ylabel: Optional[str] = None,\n","  xlabel: Optional[str] = None,\n","  plot_labels: Optional[Union[str, List[str]]] = None,\n","):\n","  if plot_labels is None:\n","    plot_labels = [\"train\"]\n","    if validation_vals is not None:\n","      plot_labels.append(\"validation\")\n","    if test_vals is not None:\n","      plot_labels.append(\"test\")\n","  elif not isinstance(plot_labels, list):\n","    plot_labels = [plot_labels] * 3\n","\n","  x_epochs = np.arange(1, len(train_vals) + 1)\n","  plt.plot(x_epochs, train_vals, label=plot_labels[0])\n","  if validation_vals is not None:\n","    x_epochs = np.arange(len(train_vals) - len(validation_vals) + 1, len(train_vals) + 1)\n","    plt.plot(x_epochs, validation_vals, label=plot_labels[1])\n","  if test_vals is not None:\n","    x_epochs = np.arange(len(train_vals) - len(test_vals) + 1, len(train_vals) + 1)\n","    plt.plot(x_epochs, test_vals, label=plot_labels[2])\n","  if title is not None:\n","    plt.title(title)\n","  if ylabel is not None:\n","    plt.ylabel(ylabel)\n","  if xlabel is not None:\n","    plt.xlabel(xlabel)\n","  plt.legend()\n","  plt.savefig(output_path)\n","  plt.close()\n","\n","\n","def save_loss_plot(\n","  train_loss: List[float],\n","  output_path: Union[str, Path],\n","  test_loss: Optional[List[float]] = None,\n","  val_loss: Optional[List[float]] = None,\n","  title: str = \"Loss\",\n","  ylabel: str = \"Loss\",\n","  xlabel: str = \"Epochs\",\n","  plot_labels: Optional[Union[str, List[str]]] = None,\n","):\n","  plot_accuracy_or_loss(\n","    train_vals=train_loss,\n","    output_path=output_path,\n","    validation_vals=val_loss,\n","    test_vals=test_loss,\n","    title=title,\n","    ylabel=ylabel,\n","    xlabel=xlabel,\n","    plot_labels=plot_labels,\n","  )\n","\n","def save_accuracy_plot(\n","  train_acc: List[float],\n","  output_path: Union[str, Path],\n","  test_acc: Optional[List[float]] = None,\n","  val_acc: Optional[List[float]] = None,\n","  plot_labels: Optional[Union[str, List[str]]] = None,\n","):\n","  plot_accuracy_or_loss(\n","    train_vals=train_acc,\n","    output_path=output_path,\n","    validation_vals=val_acc,\n","    test_vals=test_acc,\n","    title=\"Accuracy\",\n","    ylabel=\"Accuracy\",\n","    xlabel=\"Epochs\",\n","    plot_labels=plot_labels,\n","  )"],"metadata":{"id":"oAqcebVRywER","executionInfo":{"status":"ok","timestamp":1679444199337,"user_tz":240,"elapsed":743,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import re\n","def get_latest_model(base_path, suffix: str = \".pth\") -> Path:\n","  epoch_num = []\n","  all_files = sorted(Path(base_path).glob(suffix))\n","  for file_ in all_files:\n","    idx_num = re.search(\"--\", str(file_)).span()\n","    idx_pt = re.search(suffix, str(file_)).span()\n","    model_num = str(file_)[idx_num[-1]:idx_pt[0]]\n","    try:\n","      epoch_num.append(int(model_num))\n","    except ValueError:\n","      idx_num = re.search(\"--\", str(model_num)).span()\n","      epoch_num.append(int(model_num[idx_num[-1]:]))\n","\n","  idx = epoch_num.index(np.max(epoch_num))\n","  return all_files[idx]"],"metadata":{"id":"sM86rgUEC9tf","executionInfo":{"status":"ok","timestamp":1679444201901,"user_tz":240,"elapsed":479,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import math\n","def number_of_combinations(n_objs: int, r_at_a_time: int) -> int:\n","  num = math.factorial(n_objs)\n","  den = math.factorial(n_objs - r_at_a_time) * math.factorial(r_at_a_time)\n","  return int(num / den)\n","\n","def get_n_objs_for_a_number_of_combinations_with_2_at_a_time(combs: int) -> int:\n","  return int((1 + math.sqrt(1 + (4*combs * 2))) / 2)"],"metadata":{"id":"t7kOC23hzLu6","executionInfo":{"status":"ok","timestamp":1679444203651,"user_tz":240,"elapsed":4,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Two-step learning policy as employed by 'Deep Fake Image Detection Based on Pairwise Learning'. Therefore, we first train the CFFN network with the contrastive loss. After the CFFN network is learned to minimize the contrastive loss, we then train the classification network using the outputs from the CFFN network, as this better feature representation output will allow the classification network to better classify the images as fake or real. The classification network is trained using the binary cross-entropy loss of predicting whether the image is real or fake [p.6. section 2.4]"],"metadata":{"id":"QLtX8bv5Qw_6"}},{"cell_type":"markdown","source":["**1. Common Fake Feature Network**\n","\n","Network structure includes a pairwise learning approach. \"A fake face image detector based on the novel CFFN, consisting of an improved DenseNet backbone network and Siamese network architecture...The cross-layer features are investigated by the proposed CFFN, which can be used to improve the performance.\"\n","\n","The fake and real images are paired together and the pairwise information is used to construct the contrastive loss to learn the discriminative common fake feature (CFF) by the CFFN. The paper states that 2 million pairwise samples are used for training.\n","\n","\"One way to learn both the CFFs and classifier is the join learning strategy incorporating the contrastive loss and cross-entropy loss into the total energy function. In another way, the CFFN is first trained by the proposed contrastive loss and follows by training the classifier based on cross-entropy loss. When the first strategy is applied, it is difficult to observe the impact of both contrastive and cross-entropy loss functions on the performance of the fake image detection tasks. Therefore, we adopt the second strategy to ensure the best performance of the proposed method.\""],"metadata":{"id":"RkqQitJIUMJf"}},{"cell_type":"code","source":["import itertools\n","from typing import Tuple\n","class CelebrityDataCFFN(CelebrityData):\n","\n","  def __init__(self, base_path: Path, transform = None, seed = None, n_combinations: int = 4e6, gans_to_skip: Optional[List[str]] = None):\n","    super().__init__(base_path=base_path, transform=transform, seed=seed, gans_to_skip=gans_to_skip)\n","\n","    # for training the CFFN we want fake-fake pairs & real-real pairs\n","    # self.n_fake_combinations = self.n_fake_gans * number_of_combinations(int(self.len_of_fake_images / self.n_fake_gans), 2)\n","    self.n_imgs_for_combinations = get_n_objs_for_a_number_of_combinations_with_2_at_a_time(n_combinations)\n","    # only making combinations between images made by the same GAN\n","    # could try experimenting with combinations of images between different GANs\n","    self.fake_image_combos: Dict[str, list] = {}\n","    for gan_name, img_list in self.fake_images.items():\n","      self.fake_image_combos.update({gan_name: list(itertools.combinations(img_list[:self.n_imgs_for_combinations], 2))})\n","    \n","    self.real_image_combos = list(itertools.combinations(self.real_images[:self.n_imgs_for_combinations], 2))\n","\n","  def __getitem__(self, index):\n","    img0_path, img1_path, pair_indicator = self.choose_real_or_fake_pair(index)\n","    img0 = Image.open(img0_path).convert('RGB')\n","    if self.transform is not None:\n","      img0 = self.transform(img0)\n","\n","    img1 = Image.open(img1_path).convert('RGB')\n","    if self.transform is not None:\n","      img1 = self.transform(img1)\n","\n","    return img0, img1, pair_indicator\n","\n","  # def choose_real_or_fake_pair(self, index) -> Tuple[str, str, int]:\n","  #   if self.rng.standard_normal() > 0:\n","  #     img_pair = next(itertools.islice(self.real_image_combos, index, None))\n","  #     pair_indicator = 1\n","  #   else:\n","  #     img_pair = self.fake_image_rand_selection(index)\n","  #     pair_indicator = 0\n","\n","  #   return img_pair[0], img_pair[1], pair_indicator\n","\n","  def choose_real_or_fake_pair(self, index) -> Tuple[str, str, int]:\n","    if self.rng.standard_normal() > 0:\n","      img_pair = self.real_image_combos[index]\n","      pair_indicator = 1\n","    else:\n","      img_pair = self.fake_image_rand_selection(index)\n","      pair_indicator = 0\n","\n","    return img_pair[0], img_pair[1], pair_indicator\n","\n","  # def fake_image_rand_selection(self, index) -> Tuple[str, str]:\n","  #   rand_selection = self.rng.uniform(low=-0.499, high=len(self.fake_images) - 0.501)\n","  #   gan_selection = self.fake_image_gan_names[int(np.round(rand_selection))]\n","  #   iter_combo = self.fake_image_combos.get(gan_selection)\n","  #   return next(itertools.islice(iter_combo, index // len(self.fake_image_gan_names), None))\n","\n","  def fake_image_rand_selection(self, index) -> Tuple[str, str]:\n","    rand_selection = self.rng.uniform(low=-0.499, high=self.n_fake_gans - 0.501)\n","    gan_selection = self.fake_image_gan_names[int(np.round(rand_selection))]\n","\n","    return self.fake_image_combos[gan_selection][index]\n","\n","  def __len__(self):\n","    return len(self.real_image_combos)"],"metadata":{"id":"BdmG6jjDMhXK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We will be working with GPU:\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('Device : ' , device)\n","\n","# Number of GPUs available. \n","num_GPU = torch.cuda.device_count()\n","print('Number of GPU : ', num_GPU)\n","\n","model_output_path = Path(\"/content/drive/MyDrive/ECE 792 - Advance Topics in Machine Learning/Code/DeepFakeImageDetection/CFFN/model\")\n","if not model_output_path.exists():\n","  model_output_path.mkdir(exist_ok=True, parents=True)\n","\n","config_cffn = { 'batch_size'             : 88,\n","                'image_size'             : 64,\n","                'n_channel'              : 3,\n","                'n_epochs'               : 15,\n","                'lr'                     : 1e-3,\n","                'growth_rate'            : 24,\n","                'transition_layer_theta' : 0.5,\n","                'device'                 : device,\n","                'm_th'                   : 0.5,\n","                'n_combinations'         : 2e6,\n","                'seed'                   : 999,\n","                'model_output_path'      : model_output_path,\n","                'chkp_freq'              : 1,  # number of epochs to save model out\n","                'n_workers'              : 4,\n","                'gans_to_skip'           : [\"CDCGAN\"],\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QA-ybvdX_aVl","executionInfo":{"status":"ok","timestamp":1679444209785,"user_tz":240,"elapsed":1553,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"9c9b6cc7-3f04-403c-c679-40fb62923758"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Device :  cuda\n","Number of GPU :  1\n"]}]},{"cell_type":"code","source":["celebrity_data_cffn = CelebrityDataCFFN(\n","  base_path=dataset_base_path,\n","  transform=transforms.Compose(\n","    [\n","      transforms.Resize(int(config_cffn[\"image_size\"] * 1.1)),\n","      transforms.CenterCrop(config_cffn[\"image_size\"]),\n","      transforms.ToTensor(),\n","      transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n","    ]\n","  ),\n","  seed=config_cffn[\"seed\"],\n","  n_combinations=config_cffn[\"n_combinations\"],\n",")\n","\n","dataloader_cffn = torch.utils.data.DataLoader(\n","  dataset=celebrity_data_cffn,\n","  shuffle=True,\n","  batch_size=config_cffn[\"batch_size\"],\n","  num_workers=config_cffn[\"n_workers\"],\n","  drop_last=True,  # drop last batch that may not be the same size as the expected batch for the network\n","  pin_memory=True,\n",")"],"metadata":{"id":"8hF9OU_QqKni","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"error","timestamp":1679167135502,"user_tz":240,"elapsed":304,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"fe084f56-05ba-4ce5-b9cf-461d8247bf19"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-96fd1dbd350e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m celebrity_data_cffn = CelebrityDataCFFN(\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_base_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   transform=transforms.Compose(\n\u001b[1;32m      4\u001b[0m     [\n\u001b[1;32m      5\u001b[0m       \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_cffn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_size\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'CelebrityDataCFFN' is not defined"]}]},{"cell_type":"code","source":["from typing import Callable, Tuple\n","\n","class DenseBlock2(nn.Module):\n","  conv0_0_out = None\n","  conv0_1_out = None\n","  batch_norm0_out = None\n","  concat0_out = None\n","  activation0_out = None\n","  conv1_0_out = None\n","  conv1_1_out = None\n","  batch_norm1_out = None\n","  concat1_out = None\n","  activation1_out = None\n","  trans_layer_out = None\n","  def __init__(\n","    self,\n","    in_channels: int,\n","    out_channels: int,\n","    growth_rate: int,\n","    transition_layer_theta: float,\n","    device: torch.device = None,\n","  ):\n","    super().__init__()\n","    if device is None:\n","      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.conv0_0 = nn.Conv2d(\n","      in_channels=in_channels,\n","      out_channels=in_channels * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv0_1 = nn.Conv2d(\n","      in_channels=in_channels * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm0 = nn.BatchNorm2d(in_channels + growth_rate, device=device)\n","\n","    self.conv1_0 = nn.Conv2d(\n","      in_channels=in_channels + growth_rate,\n","      out_channels=(in_channels + growth_rate) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv1_1 = nn.Conv2d(\n","      in_channels=(in_channels + growth_rate) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm1 = nn.BatchNorm2d(in_channels + (2 * growth_rate), device=device)\n","\n","    trans_kernel_size = int(1/transition_layer_theta)\n","    self.trans_layer = nn.MaxPool3d(kernel_size=(trans_kernel_size, 1, 1))\n","    self.activation_func = nn.ReLU()\n","\n","  def forward(self, x):\n","    self.conv0_0_out = self.conv0_0(x)\n","    self.conv0_1_out = self.conv0_1(self.conv0_0_out)\n","    self.concat0_out = torch.concat((self.conv0_1_out, x), dim=1)\n","    self.batch_norm0_out = self.batch_norm0(self.concat0_out)\n","    self.activation0_out = self.activation_func(self.batch_norm0_out)\n","\n","    self.conv1_0_out = self.conv1_0(self.activation0_out)\n","    self.conv1_1_out = self.conv1_1(self.conv1_0_out)\n","    self.concat1_out = torch.concat((self.conv1_1_out, self.activation0_out), dim=1)\n","    self.batch_norm1_out = self.batch_norm1(self.concat1_out)\n","    self.activation1_out = self.activation_func(self.batch_norm1_out)\n","\n","    self.trans_layer_out = self.trans_layer(self.activation1_out)\n","\n","    return self.trans_layer_out\n","\n","class DenseBlock3(nn.Module):\n","  conv0_0_out = None\n","  conv0_1_out = None\n","  batch_norm0_out = None\n","  concat0_out = None\n","  activation0_out = None\n","  conv1_0_out = None\n","  conv1_1_out = None\n","  batch_norm1_out = None\n","  concat1_out = None\n","  activation1_out = None\n","  conv2_0_out = None\n","  conv2_1_out = None\n","  batch_norm2_out = None\n","  concat2_out = None\n","  activation2_out = None\n","  trans_layer_out = None\n","  def __init__(\n","    self,\n","    in_channels: int,\n","    out_channels: int,\n","    growth_rate: int,\n","    transition_layer_theta: float,\n","    device: torch.device = None,\n","  ):\n","    super().__init__()\n","    if device is None:\n","      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.conv0_0 = nn.Conv2d(\n","      in_channels=in_channels,\n","      out_channels=in_channels * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv0_1 = nn.Conv2d(\n","      in_channels=in_channels * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm0 = nn.BatchNorm2d(in_channels + growth_rate, device=device)\n","\n","    self.conv1_0 = nn.Conv2d(\n","      in_channels=in_channels + growth_rate,\n","      out_channels=(in_channels + growth_rate) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv1_1 = nn.Conv2d(\n","      in_channels=(in_channels + growth_rate) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm1 = nn.BatchNorm2d(in_channels + (2 * growth_rate), device=device)\n","\n","    self.conv2_0 = nn.Conv2d(\n","      in_channels=in_channels + (2 * growth_rate),\n","      out_channels=(in_channels + (2 * growth_rate)) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv2_1 = nn.Conv2d(\n","      in_channels=(in_channels + (2 * growth_rate)) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm2 = nn.BatchNorm2d((in_channels + (3 * growth_rate)), device=device)\n","\n","    trans_kernel_size = int(1/transition_layer_theta)\n","    self.trans_layer = nn.MaxPool3d(kernel_size=(trans_kernel_size, 1, 1))\n","    self.activation_func = nn.ReLU()\n","\n","  def forward(self, x):\n","    self.conv0_0_out = self.conv0_0(x)\n","    self.conv0_1_out = self.conv0_1(self.conv0_0_out)\n","    self.concat0_out = torch.concat((self.conv0_1_out, x), dim=1)\n","    self.batch_norm0_out = self.batch_norm0(self.concat0_out)\n","    self.activation0_out = self.activation_func(self.batch_norm0_out)\n","\n","    self.conv1_0_out = self.conv1_0(self.activation0_out)\n","    self.conv1_1_out = self.conv1_1(self.conv1_0_out)\n","    self.concat1_out = torch.concat((self.conv1_1_out, self.activation0_out), dim=1)\n","    self.batch_norm1_out = self.batch_norm1(self.concat1_out)\n","    self.activation1_out = self.activation_func(self.batch_norm1_out)\n","\n","    self.conv2_0_out = self.conv2_0(self.activation1_out)\n","    self.conv2_1_out = self.conv2_1(self.conv2_0_out)\n","    self.concat2_out = torch.concat((self.conv2_1_out, self.activation1_out), dim=1)\n","    self.batch_norm2_out = self.batch_norm2(self.concat2_out)\n","    self.activation2_out = self.activation_func(self.batch_norm2_out)\n","\n","    self.trans_layer_out = self.trans_layer(self.activation2_out)\n","\n","    return self.trans_layer_out\n","\n","class DenseBlock4(nn.Module):\n","  conv0_0_out = None\n","  conv0_1_out = None\n","  batch_norm0_out = None\n","  concat0_out = None\n","  activation0_out = None\n","  conv1_0_out = None\n","  conv1_1_out = None\n","  batch_norm1_out = None\n","  concat1_out = None\n","  activation1_out = None\n","  conv2_0_out = None\n","  conv2_1_out = None\n","  batch_norm2_out = None\n","  concat2_out = None\n","  activation2_out = None\n","  conv3_0_out = None\n","  conv3_1_out = None\n","  batch_norm3_out = None\n","  concat3_out = None\n","  activation3_out = None\n","  trans_layer_out = None\n","  def __init__(\n","    self,\n","    in_channels: int,\n","    out_channels: int,\n","    growth_rate: int,\n","    transition_layer_theta: float,\n","    device: torch.device = None,\n","  ):\n","    super().__init__()\n","    if device is None:\n","      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.conv0_0 = nn.Conv2d(\n","      in_channels=in_channels,\n","      out_channels=in_channels * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv0_1 = nn.Conv2d(\n","      in_channels=in_channels * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm0 = nn.BatchNorm2d(in_channels + growth_rate, device=device)\n","\n","    self.conv1_0 = nn.Conv2d(\n","      in_channels=in_channels + growth_rate,\n","      out_channels=(in_channels + growth_rate) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv1_1 = nn.Conv2d(\n","      in_channels=(in_channels + growth_rate) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm1 = nn.BatchNorm2d(in_channels + (2 * growth_rate), device=device)\n","\n","    self.conv2_0 = nn.Conv2d(\n","      in_channels=in_channels + (2 * growth_rate),\n","      out_channels=(in_channels + (2 * growth_rate)) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv2_1 = nn.Conv2d(\n","      in_channels=(in_channels + (2 * growth_rate)) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm2 = nn.BatchNorm2d((in_channels + (3 * growth_rate)), device=device)\n","\n","    self.conv3_0 = nn.Conv2d(\n","      in_channels=in_channels + (3 * growth_rate),\n","      out_channels=(in_channels + (3 * growth_rate)) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv3_1 = nn.Conv2d(\n","      in_channels=(in_channels + (3 * growth_rate)) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm3 = nn.BatchNorm2d((in_channels + (4 * growth_rate)), device=device)\n","\n","    trans_kernel_size = int(1/transition_layer_theta)\n","    self.trans_layer = nn.MaxPool3d(kernel_size=(trans_kernel_size, 1, 1))\n","    self.activation_func = nn.ReLU()\n","\n","  def forward(self, x):\n","    self.conv0_0_out = self.conv0_0(x)\n","    self.conv0_1_out = self.conv0_1(self.conv0_0_out)\n","    self.concat0_out = torch.concat((self.conv0_1_out, x), dim=1)\n","    self.batch_norm0_out = self.batch_norm0(self.concat0_out)\n","    self.activation0_out = self.activation_func(self.batch_norm0_out)\n","\n","    self.conv1_0_out = self.conv1_0(self.activation0_out)\n","    self.conv1_1_out = self.conv1_1(self.conv1_0_out)\n","    self.concat1_out = torch.concat((self.conv1_1_out, self.activation0_out), dim=1)\n","    self.batch_norm1_out = self.batch_norm1(self.concat1_out)\n","    self.activation1_out = self.activation_func(self.batch_norm1_out)\n","\n","    self.conv2_0_out = self.conv2_0(self.activation1_out)\n","    self.conv2_1_out = self.conv2_1(self.conv2_0_out)\n","    self.concat2_out = torch.concat((self.conv2_1_out, self.activation1_out), dim=1)\n","    self.batch_norm2_out = self.batch_norm2(self.concat2_out)\n","    self.activation2_out = self.activation_func(self.batch_norm2_out)\n","\n","    self.conv3_0_out = self.conv3_0(self.activation2_out)\n","    self.conv3_1_out = self.conv3_1(self.conv3_0_out)\n","    self.concat3_out = torch.concat((self.conv3_1_out, self.activation2_out), dim=1)\n","    self.batch_norm3_out = self.batch_norm3(self.concat3_out)\n","    self.activation3_out = self.activation_func(self.batch_norm3_out)\n","\n","    self.trans_layer_out = self.trans_layer(self.activation3_out)\n","\n","    return self.trans_layer_out\n","\n","# class DenseBlock(nn.Module):\n","#   def __init__(\n","#     self,\n","#     n_conv: int,\n","#     in_channels: int,\n","#     out_channels: int,\n","#     growth_rate: int,\n","#     transition_layer_theta: float,\n","#     device: torch.device = None,\n","#   ):\n","#     super().__init__()\n","#     self.modules = []\n","#     self.batch_norms = []\n","#     if device is None:\n","#       device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#     for idx in range(n_conv):\n","#       in_channels_with_growth = in_channels + (idx * growth_rate)\n","#       out_channels_with_growth = in_channels + ((idx + 1) * growth_rate)\n","#       self.modules.append(\n","#         [\n","#           nn.Conv2d(\n","#             in_channels=in_channels_with_growth,\n","#             out_channels=in_channels_with_growth * 2,\n","#             kernel_size=(1, 1),\n","#             padding=0,\n","#             stride=(1, 1),\n","#             device=device,\n","#           ),\n","#           nn.Conv2d(\n","#             in_channels=in_channels_with_growth * 2,\n","#             out_channels=growth_rate,\n","#             kernel_size=(3, 3),\n","#             padding=1,\n","#             stride=(1, 1),\n","#             device=device,\n","#           ),\n","#         ]\n","#       )\n","#       self.batch_norms.append(nn.BatchNorm2d(out_channels_with_growth, device=device))\n","#     trans_kernel_size = int(1 / transition_layer_theta)\n","#     self.trans_layer = nn.MaxPool3d(kernel_size=(trans_kernel_size, 1, 1))\n","#     self.activation_func = nn.ReLU()\n","\n","#   def forward(self, x):\n","#     layer_outputs = [x]\n","#     for d_block, batch_norm in zip(self.modules, self.batch_norms):\n","#       for module in d_block:\n","#         x = module(x)\n","#       x = torch.concat((x, layer_outputs[-1]), dim=1)\n","#       x = batch_norm(x)\n","#       x = self.activation_func(x)\n","#       layer_outputs.append(x)\n","\n","#     x = self.trans_layer(x)\n","#     return x\n","\n","\n","class CFFNEnergyFunction(nn.Module):\n","  loss = None\n","\n","  def __init__(self, batch_size: int = 88, m_th: float = 0.5, device=device):\n","    super().__init__()\n","    self.m_th = torch.empty(batch_size, device=device).fill_(m_th)\n","    self.zero_tensor = torch.empty(batch_size, device=device).fill_(0)\n","    self.energy_function = nn.MSELoss(reduction=\"none\")\n","\n","  def forward(self, img0, img1, pairs_indicator):\n","    E_w = torch.mean(self.energy_function(img0, img1), dim=1)\n","    real_pairs = (0.5 * torch.mul(pairs_indicator, torch.pow(E_w, 2)))\n","    fake_pairs = torch.mul(\n","        (1 - pairs_indicator),\n","        torch.max(self.zero_tensor, self.energy_function(self.m_th, E_w))\n","        )\n","    self.loss = torch.mean(torch.add(real_pairs, fake_pairs))\n","\n","    return self.loss\n","\n","  def item(self):\n","    return self.loss.item()\n","\n","\n","class CFFN(nn.Module):\n","  dense_conv1_out = None\n","  dense_conv2_out = None\n","  dense_conv3_out = None\n","  dense_conv4_out = None\n","  conv5_out = None\n","  batch_norm5_out = None\n","  activation5_out = None\n","\n","  def __init__(\n","    self,\n","    input_image_shape: Tuple[int, int],\n","    growth_rate: int = 24,\n","    transition_layer_theta: float = 0.5,\n","    learning_rate: float = 1e-3,\n","    m_th: float = 0.5,  # threshold for contrastive loss\n","    batch_size: int = 88,\n","    device: torch.device = None,\n","  ):\n","    super().__init__()\n","    self.conv0 = nn.Conv2d(in_channels=3, out_channels=48, kernel_size=(7, 7), stride=(4, 4))\n","    self.batch_norm0 = nn.BatchNorm2d(48)\n","    self.activation0 = nn.ReLU()\n","    if device is None:\n","      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    # self.dense_conv1 = DenseBlock(\n","    #   n_conv=2,\n","    #   in_channels=48,\n","    #   out_channels=48,\n","    #   growth_rate=growth_rate,\n","    #   transition_layer_theta=transition_layer_theta,\n","    #   device=device,\n","    # ).to(device)\n","    self.dense_conv1 = DenseBlock2(\n","      in_channels=48,\n","      out_channels=48,\n","      growth_rate=growth_rate,\n","      transition_layer_theta=transition_layer_theta,\n","      device=device,\n","    )\n","    self.dense_conv2 = DenseBlock3(\n","      in_channels=48,\n","      out_channels=60,\n","      growth_rate=24,\n","      transition_layer_theta=transition_layer_theta,\n","      device=device,\n","    ).to(device)\n","    self.dense_conv3 = DenseBlock4(\n","      in_channels=60,\n","      out_channels=78,\n","      growth_rate=24,\n","      transition_layer_theta=transition_layer_theta,\n","      device=device,\n","    ).to(device)\n","    self.dense_conv4 = DenseBlock2(\n","      in_channels=78,\n","      out_channels=126,\n","      growth_rate=24,\n","      transition_layer_theta=1,\n","      device=device,\n","    ).to(device)\n","    self.conv5 = nn.Conv2d(in_channels=126, out_channels=128, kernel_size=(3, 3))\n","    self.batch_norm5 = nn.BatchNorm2d(128)\n","    self.activation5 = nn.ReLU()\n","    # self.fully_connected: Callable = lambda in_conv_n_channels, conv_shape0, conv_shape1: nn.Sequential(\n","    #   nn.Flatten(),\n","    #   nn.Linear(in_conv_n_channels * conv_shape0 * conv_shape1, 128, device=device),\n","    #   nn.ReLU(),\n","    # )\n","    self.flatten = nn.Flatten()\n","    self.fully_connected3 = nn.Linear(78 * 15 * 15, 128, device=device)\n","    self.fully_connected4 = nn.Linear(126 * 15 * 15, 128, device=device)\n","    self.fully_connected5 = nn.Linear(128 * 13 * 13, 128, device=device)\n","    self.activation_func = nn.ReLU()\n","    self.loss = CFFNEnergyFunction(batch_size=batch_size, m_th=m_th, device=device).to(device)\n","    self.optimizer = torch.optim.Adam(\n","        self.parameters(), lr=learning_rate,\n","    )\n","\n","  def forward(self, x):\n","    x = self.conv0(x)\n","    x = self.batch_norm0(x)\n","    x = self.activation0(x)\n","    self.dense_conv1_out = self.dense_conv1(x)\n","    # print(f\"dense_conv1_out.shape = '{self.dense_conv1_out.shape}'\")\n","    self.dense_conv2_out = self.dense_conv2(self.dense_conv1_out)\n","    # print(f\"dense_conv2_out.shape = '{self.dense_conv2_out.shape}'\")\n","    self.dense_conv3_out = self.dense_conv3(self.dense_conv2_out)\n","    # print(f\"dense_conv3_out.shape = '{self.dense_conv3_out.shape}'\")\n","    self.dense_conv4_out = self.dense_conv4(self.dense_conv3_out)\n","    # print(f\"dense_conv4_out.shape = '{self.dense_conv4_out.shape}'\")\n","    self.conv5_out = self.conv5(self.dense_conv4_out)\n","    # print(f\"conv5_out.shape = '{self.conv5_out.shape}'\")\n","    self.batch_norm5_out = self.batch_norm5(self.conv5_out)\n","    self.activation5_out = self.activation5(self.batch_norm5_out)\n","\n","    # fn5_module = self.fully_connected(*self.activation5_out.shape[1:])\n","    # fn5 = fn5_module(self.activation5_out)\n","    activation5_flattened = self.flatten(self.activation5_out)\n","    fn5_out = self.fully_connected5(activation5_flattened)\n","    fn5 = self.activation_func(fn5_out)\n","\n","    # fn4_module = self.fully_connected(*self.dense_conv4_out.shape[1:])\n","    # fn4 = fn4_module(self.dense_conv4_out)\n","    dense_conv4_flattened = self.flatten(self.dense_conv4_out)\n","    fn4_out = self.fully_connected4(dense_conv4_flattened)\n","    fn4 = self.activation_func(fn4_out)\n","\n","    # fn3_module = self.fully_connected(*self.dense_conv3_out.shape[1:])\n","    # fn3 = fn3_module(self.dense_conv3_out)\n","    dense_conv3_flattened = self.flatten(self.dense_conv3_out)\n","    fn3_out = self.fully_connected3(dense_conv3_flattened)\n","    fn3 = self.activation_func(fn3_out)\n","\n","    x_out = torch.cat((fn5, fn4, fn3), dim=1)\n","\n","    # return output of convolution 5, which will be input to classification network\n","    # x_out is discriminative features output used for the pairwise learning for CFFN network\n","    return self.activation5_out, x_out\n","\n","  def loss_back_grad(self, img0, img1, pairs_indicator, back_grad: bool = True):\n","    criterion = self.loss(img0, img1, pairs_indicator)\n","    if back_grad:\n","      criterion.backward()\n","      self.optimizer.step()"],"metadata":{"id":"9wMVtogPRhft","executionInfo":{"status":"ok","timestamp":1679444214111,"user_tz":240,"elapsed":537,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["cffn = CFFN(\n","    input_image_shape=(64, 64),\n","    growth_rate=config_cffn[\"growth_rate\"],\n","    transition_layer_theta=config_cffn[\"transition_layer_theta\"],\n","    learning_rate=config_cffn[\"lr\"],\n","    m_th=config_cffn[\"m_th\"],\n","    batch_size=config_cffn[\"batch_size\"],\n","    device=config_cffn[\"device\"],\n","    ).to(config_cffn[\"device\"])\n","cffn_models_output_path = config_cffn[\"model_output_path\"] / \"models\"\n","cffn_models_output_path.mkdir(exist_ok=True, parents=True)\n","loss_output_path = config_cffn[\"model_output_path\"] / \"loss\"\n","loss_output_path.mkdir(exist_ok=True, parents=True)"],"metadata":{"id":"Pqn7Uv02CtJQ","executionInfo":{"status":"ok","timestamp":1679444227627,"user_tz":240,"elapsed":6776,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","## CFFN TRAINING\n","train: bool = False\n","test: bool = False\n","if train:\n","  epoch_tqdm = tqdm(total=config_cffn[\"n_epochs\"], position=0)\n","  train_loss = Loss.init()\n","  print(\"Starting Training Loop...\")\n","  for epoch in range(config_cffn[\"n_epochs\"]):\n","    for batch_idx, (img0, img1, pair_indicator) in enumerate(dataloader_cffn):\n","      cffn.optimizer.zero_grad()\n","      img0 = img0.to(config_cffn[\"device\"])\n","      img1 = img1.to(config_cffn[\"device\"])\n","      _, img0_discriminative_features = cffn(img0)\n","      _, img1_discriminative_features = cffn(img1)\n","      pair_indicator = torch.tensor(pair_indicator, device=config_cffn[\"device\"])\n","      cffn.loss_back_grad(\n","          img0_discriminative_features,\n","          img1_discriminative_features,\n","          pair_indicator\n","          )\n","      train_loss += cffn.loss.item()\n","      \n","      if batch_idx % 50 == 0:\n","        epoch_tqdm.write(\" [%d/%d]\\tLoss: %.8f\" % (batch_idx, len(dataloader_cffn), train_loss.current_loss))\n","\n","    epoch_tqdm.update(1)\n","    train_loss.update_for_epoch()\n","    if epoch % config_cffn[\"chkp_freq\"] == 0:\n","      torch.save(\n","          {\n","            \"CFFN_state_dict\": cffn.state_dict(),\n","            \"CFFN_optimizer\": cffn.optimizer.state_dict(),      \n","          },\n","          str(cffn_models_output_path / f\"CFFN--{epoch}.pth\")\n","      )\n","\n","      loss_epoch_out_path = loss_output_path / f\"epoch-loss--{epoch}.png\"\n","      save_loss_plot(train_loss.loss_vals_per_epoch, loss_epoch_out_path)\n","      loss_batch_out_path = loss_output_path / f\"batch-loss--{epoch}.png\"\n","      save_loss_plot(train_loss.loss_vals_per_batch, loss_batch_out_path, xlabel=\"Batches\")\n","elif test:\n","  model_file = get_latest_model(cffn_models_output_path)\n","  print(model_file)\n","  checkpoint = torch.load(str(model_file))\n","  cffn.load_state_dict(checkpoint[\"CFFN_state_dict\"])\n","  cffn.to(device)\n","  cffn.eval()\n","  test_loss = Loss.init()\n","  batch_tqdm = tqdm(total=len(dataloader_cffn), position=0)\n","  for batch_idx, (img0, img1, pair_indicator) in enumerate(dataloader_cffn):\n","    img0 = img0.to(config_cffn[\"device\"])\n","    img1 = img1.to(config_cffn[\"device\"])\n","    _, img0_discriminative_features = cffn(img0)\n","    _, img1_discriminative_features = cffn(img1)\n","    pair_indicator = torch.tensor(pair_indicator, device=config_cffn[\"device\"])\n","    cffn.loss_back_grad(\n","        img0_discriminative_features,\n","        img1_discriminative_features,\n","        pair_indicator,\n","        back_grad=False,\n","        )\n","    test_loss += cffn.loss.item()\n","    \n","    if batch_idx % 50 == 0:\n","      batch_tqdm.write(\" [%d/%d]\\tLoss: %.8f\" % (batch_idx, len(dataloader_cffn), test_loss.current_loss))\n","\n","    batch_tqdm.update(1)\n","else:\n","  print(\"PASSING AS 'train' & 'test' ARE BOTH FALSE.\")"],"metadata":{"id":"_fIjnkQ8K8um","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679167154827,"user_tz":240,"elapsed":8,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"3cf1d1a3-c386-43ca-b0e5-9261729729b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PASSING AS 'train' & 'test' ARE BOTH FALSE.\n"]}]},{"cell_type":"markdown","source":["**2. Classification Network**\n","\n","\"The classification sub-network consists of a convolution layer with two channels, and a fully connected layer with two neurons.\""],"metadata":{"id":"YS-iJPjLUe4v"}},{"cell_type":"code","source":["import pandas as pd\n","import seaborn as sns\n","\n","@dataclass\n","class Accuracy:\n","  acc_vals_per_batch: List[float]\n","  acc_vals_per_epoch: List[float]\n","  precision_per_epoch: List[float]\n","  recall_per_epoch: List[float]\n","  f1_score_per_epoch: List[float]\n","  correct_hits: np.ndarray\n","  correct_hits_per_epoch: List[np.ndarray]\n","  incorrect_hits: np.ndarray\n","  incorrect_hits_per_epoch: List[np.ndarray]\n","  output_decisions: int\n","  batch_cnt: int = 0\n","  previous_batch_cnt: int = 0\n","  epoch_cnt: int = 0\n","  onehotencoding: bool = True\n","\n","  @classmethod\n","  def from_output_decisions(cls, output_size: int, onehotencoding: bool = True) -> \"Accuracy\":\n","    if onehotencoding:\n","      inc_hits_shape = (output_size, output_size)\n","    else:\n","      inc_hits_shape = (output_size,)\n","    return cls(\n","        acc_vals_per_batch=[],\n","        acc_vals_per_epoch=[],\n","        precision_per_epoch=[],\n","        recall_per_epoch=[],\n","        f1_score_per_epoch=[],\n","        batch_cnt=0,\n","        previous_batch_cnt=0,\n","        epoch_cnt=0,\n","        correct_hits=np.zeros((output_size,)),\n","        correct_hits_per_epoch=[],\n","        incorrect_hits=np.zeros(inc_hits_shape),\n","        incorrect_hits_per_epoch=[],\n","        output_decisions=output_size,\n","        onehotencoding=onehotencoding,\n","    )\n","\n","  def compare_batch(self, targets: torch.Tensor, outputs: torch.Tensor) -> List[Tuple[int, int]]:\n","    # determine accuracy between a batch of targets and outputs to update accuracy\n","    hit: int = 0\n","    indices = None\n","    if self.onehotencoding:\n","      indices = []\n","      for batch_idx, (target, output) in enumerate(zip(targets, outputs)):\n","          max_idx = int(torch.argmax(output))\n","          if bool(\n","              target[max_idx]\n","          ):  # see if the one hot encoding scheme of our output neuron layer determined the highest probability to be the same as the true target label\n","              hit += 1\n","              self.correct_hits[max_idx] += 1\n","              indices.append((batch_idx, max_idx))\n","          else:\n","              self.incorrect_hits[int(torch.argmax(target)), max_idx] += 1\n","    else:\n","      outputs = torch.argmax(outputs, dim=1)\n","      \n","      zero_targets_idx = torch.where(targets == 0)\n","      zero_equality = torch.eq(outputs[zero_targets_idx], targets[zero_targets_idx])\n","      self.correct_hits[0] += int(sum(zero_equality))\n","      hit = int(sum(zero_equality))\n","      self.incorrect_hits[0] += int(sum(~zero_equality))\n","\n","      ones_targets_idx = torch.where(targets == 1)\n","      ones_equality = torch.eq(outputs[ones_targets_idx], targets[ones_targets_idx])\n","      self.correct_hits[1] += int(sum(ones_equality))\n","      hit += int(sum(ones_equality))\n","      self.incorrect_hits[1] += int(sum(~ones_equality))\n","\n","    self.acc_vals_per_batch.append(hit / len(targets))\n","    self.batch_cnt += 1\n","\n","    return indices\n","  \n","  @property\n","  def current_accuracy(self) -> float:\n","    return sum(self.acc_vals_per_batch) / self.batch_cnt\n","\n","  def update_for_epoch(self):\n","    # update accuracy for epoch\n","    self.epoch_cnt += 1\n","    self.acc_vals_per_epoch.append(\n","        sum(self.acc_vals_per_batch[self.previous_batch_cnt : self.batch_cnt])\n","        / (self.batch_cnt - self.previous_batch_cnt)\n","    )\n","    if len(self.correct_hits_per_epoch) == 0:\n","      self.correct_hits_per_epoch.append(self.correct_hits.copy())\n","      self.incorrect_hits_per_epoch.append(self.incorrect_hits.copy())\n","    else:\n","      correct_hits_previous_epoch = self.correct_hits_per_epoch[-1].copy()\n","      self.correct_hits_per_epoch.append(self.correct_hits - correct_hits_previous_epoch)\n","      incorrect_hits_previous_epoch = self.incorrect_hits_per_epoch[-1].copy()\n","      self.incorrect_hits_per_epoch.append(self.incorrect_hits - incorrect_hits_previous_epoch)\n","    self.previous_batch_cnt = self.batch_cnt\n","\n","  @property\n","  def confusion_matrix(self) -> np.ndarray:\n","    # get confusion matrix to better visualize incorrect hits vs correct hits\n","    if self.onehotencoding:\n","      return self.incorrect_hits.copy() + np.diag(self.correct_hits)\n","    else:  # assuming binary classification\n","      mat = np.diag(self.correct_hits)\n","      mat[0, 1] = self.cum_false_positive\n","      mat[1, 0] = self.cum_false_negative\n","      return mat\n","\n","  def save_confusion_matrix(self, output_path: Union[str, Path], categories: str):\n","    # save confusion matrix\n","    df = pd.DataFrame(self.confusion_matrix, index=[i for i in categories], columns=[i for i in categories])\n","    plt.figure(figsize=(10, 7))\n","    sns.heatmap(df, annot=True)\n","    plt.savefig(output_path)\n","    plt.close()\n","\n","  def roc_curve(self, output_path: Union[str, Path]):\n","    tp = []\n","    fp = []\n","    for epoch in range(self.epoch_cnt):\n","      tp.append(self.true_positive(epoch))\n","      fp.append(self.false_positive(epoch))\n","    tp /= max(tp)\n","    fp /= max(fp)\n","    plt.plot(fp, tp)\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.0])\n","    plt.xlabel(\"False Positive Rate\")\n","    plt.ylabel(\"True Positive Rate\")\n","    plt.title(\"ROC curve\")\n","    plt.savefig(output_path)\n","    plt.close()\n","\n","  def cum_stats_to_csv(self, output_path: Union[str, Path]):\n","    df = pd.DataFrame(\n","        columns=[\n","            \"true_positive\",\n","            \"false_positive\",\n","            \"true_negative\",\n","            \"false_negative\",\n","            \"accuracy\",\n","            \"recall\",\n","            \"precision\",\n","            \"f1_score\",\n","        ]\n","    )\n","    for epoch in range(self.epoch_cnt):\n","        tp = self.true_positive(epoch)\n","        fp = self.false_positive(epoch)\n","        tn = self.true_negative(epoch)\n","        fn = self.false_negative(epoch)\n","        accuracy = self.acc_vals_per_epoch[epoch]\n","        recall = self.recall(epoch)\n","        precision = self.precision(epoch)\n","        f1_score = self.f1_score(epoch)\n","        df.loc[epoch] = [tp, fp, tn, fn, accuracy, recall, precision, f1_score]\n","    cum_tp = self.cum_true_positive\n","    cum_fp = self.cum_false_positive\n","    cum_tn = self.cum_true_negative\n","    cum_fn = self.cum_false_negative\n","    cum_accuracy = np.mean(self.acc_vals_per_epoch)\n","    cum_recall = self.cum_recall\n","    cum_precision = self.cum_precision\n","    cum_f1_score = self.cum_f1_score\n","    df.loc[\"cumulative\"] = [cum_tp, cum_fp, cum_tn, cum_fn, cum_accuracy, cum_recall, cum_precision, cum_f1_score]\n","    df.to_csv(output_path)\n","\n","  def true_positive(self, epoch: int) -> int:\n","    if len(self.correct_hits) != 2:\n","        raise RuntimeError(f\"True Positive only defined for Binary Classification\")\n","    return self.correct_hits_per_epoch[epoch][1]  # 1 = true label, 0 = false label\n","\n","  @property\n","  def cum_true_positive(self) -> int:\n","    tp = 0\n","    for epoch in range(self.epoch_cnt):\n","        tp += self.true_positive(epoch)\n","    return tp\n","\n","  def true_negative(self, epoch: int) -> int:\n","    if len(self.correct_hits) != 2:\n","        raise RuntimeError(f\"True Negative only defined for Binary Classification\")\n","    return self.correct_hits_per_epoch[epoch][0]\n","\n","  @property\n","  def cum_true_negative(self) -> int:\n","    tn = 0\n","    for epoch in range(self.epoch_cnt):\n","        tn += self.true_negative(epoch)\n","    return tn\n","\n","  def false_positive(self, epoch: int) -> int:\n","    if len(self.correct_hits) != 2:\n","        raise RuntimeError(f\"False Positive only defined for Binary Classification\")\n","    return self.incorrect_hits_per_epoch[epoch][0]\n","\n","  @property\n","  def cum_false_positive(self) -> int:\n","    fp = 0\n","    for epoch in range(self.epoch_cnt):\n","        fp += self.false_positive(epoch)\n","    return fp\n","\n","  def false_negative(self, epoch: int) -> int:\n","    if len(self.correct_hits) != 2:\n","        raise RuntimeError(f\"False Negative only defined for Binary Classification\")\n","    return self.incorrect_hits_per_epoch[epoch][1]\n","\n","  @property\n","  def cum_false_negative(self) -> int:\n","    fn = 0\n","    for epoch in range(self.epoch_cnt):\n","        fn += self.false_negative(epoch)\n","    return fn\n","\n","  def precision(self, epoch: int) -> float:\n","    return self.true_positive(epoch) / (self.true_positive(epoch) + self.false_positive(epoch))\n","\n","  @property\n","  def cum_precision(self) -> float:\n","    return self.cum_true_positive / (self.cum_true_positive + self.cum_false_positive)\n","\n","  def recall(self, epoch: int) -> float:\n","    return self.true_positive(epoch) / (self.true_positive(epoch) + self.false_negative(epoch))\n","\n","  @property\n","  def cum_recall(self) -> float:\n","    return self.cum_true_positive / (self.cum_true_positive + self.cum_false_negative)\n","\n","  def f1_score(self, epoch: int) -> float:\n","    return 2 * ((self.precision(epoch) * self.recall(epoch)) / (self.precision(epoch) + self.recall(epoch)))\n","\n","  @property\n","  def cum_f1_score(self) -> float:\n","    return 2 * ((self.cum_precision * self.cum_recall) / (self.cum_precision + self.cum_recall))"],"metadata":{"id":"lusyWGl7Tllw","executionInfo":{"status":"ok","timestamp":1679444488866,"user_tz":240,"elapsed":7,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","def training_plot_paths(output_path: Path, epoch: int) -> Tuple[Union[str, Path], ...]:\n","  output_path = Path(output_path)\n","  if not output_path.exists():\n","    output_path.mkdir(exist_ok=True, parents=True)\n","  time_now = datetime.now()\n","  model_pt_name = time_now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n","  Path(output_path / \"loss\").mkdir(exist_ok=True, parents=True)\n","  Path(output_path / \"accuracy\").mkdir(exist_ok=True, parents=True)\n","  Path(output_path / \"confusion\").mkdir(exist_ok=True, parents=True)\n","  Path(output_path / \"models\").mkdir(exist_ok=True, parents=True)\n","  Path(output_path / \"stats\").mkdir(exist_ok=True, parents=True)\n","\n","  confusion_matrix_path = output_path / \"confusion\" / f\"confusion-matrix-{model_pt_name}--{epoch}.png\"\n","  accuracy_plot_path = output_path / \"accuracy\" / f\"accuracy-{model_pt_name}--{epoch}.png\"\n","  roc_curve_plot_path = output_path / \"stats\" / f\"roc-curve--{model_pt_name}--{epoch}.png\"\n","  cum_stats_csv_path = output_path / \"stats\" / f\"cumulative-stats--{model_pt_name}--{epoch}.csv\"\n","  loss_plot_path = output_path / \"loss\" / f\"loss-{model_pt_name}--{epoch}.png\"\n","  model_output_path = output_path / \"models\" / f\"model-{model_pt_name}--{epoch}.pth\"\n","\n","  return (\n","    model_output_path,\n","    confusion_matrix_path,\n","    accuracy_plot_path,\n","    roc_curve_plot_path,\n","    cum_stats_csv_path,\n","    loss_plot_path,\n","  )"],"metadata":{"id":"ObwAcBaypXpD","executionInfo":{"status":"ok","timestamp":1679444495437,"user_tz":240,"elapsed":461,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","\n","def circular_index(idx: int, upper_bound: int) -> int:\n","  if idx < upper_bound:\n","    return idx\n","  return idx - (upper_bound * (idx // upper_bound))\n","\n","class CelebrityDataClassificationNetwork(CelebrityData):\n","  def __init__(\n","      self,\n","      base_path: Path,\n","      transform = None,\n","      seed = None,\n","      gans_to_skip: Optional[List[str]] = None,\n","      unzip_real_imgs: bool = True,\n","  ):\n","    super().__init__(\n","        base_path=base_path,\n","        transform=transform,\n","        seed=seed,\n","        gans_to_skip=gans_to_skip,\n","        unzip_real_imgs=unzip_real_imgs,\n","    )\n","    self.to_tensor = transforms.ToTensor()\n","    self.enc = OneHotEncoder()\n","    self.enc.fit([[0], [1]])\n","\n","  def __getitem__(self, index):\n","    img_path, label, gan_selection = self.choose_real_or_fake_image(index)\n","\n","    img = Image.open(img_path).convert('RGB')\n","    if self.transform is not None:\n","      img = self.transform(img)\n","    else:\n","      img = self.to_tensor(img)\n","\n","    return img, label, gan_selection\n","\n","  def choose_real_or_fake_image(self, index) -> Tuple[str, np.ndarray, str]:\n","    if self.rng.standard_normal() > 0:\n","      img = self.real_images[circular_index(index // 2, len(self.real_images))]  # divide by 2 b/c we define __len__ as all real & fake images\n","      gan_selection = \"real\"\n","      label = [1]\n","    else:\n","      img, gan_selection = self.fake_image_rand_selection(index)\n","      label = [0]\n","\n","    label = np.squeeze(self.enc.transform(np.column_stack(label).reshape(-1, 1)).toarray())\n","    return img, label.astype(np.float32), gan_selection\n","\n","  def fake_image_rand_selection(self, index) -> str:\n","    rand_selection = self.rng.uniform(low=-0.499, high=self.n_fake_gans - 0.501)\n","    gan_selection = self.fake_image_gan_names[int(np.round(rand_selection))]\n","\n","    return self.fake_images[gan_selection][index // (self.n_fake_gans * 2)], gan_selection  # mult den by 2 b/c of definition of __len__ being all real & fake images\n","\n","  def names_of_gans(self) -> List[str]:\n","    return list(self.fake_images.keys())\n","\n","  def __len__(self):\n","    return len(self.real_images) + self.len_of_fake_images"],"metadata":{"id":"NGvuBcd_rj6p","executionInfo":{"status":"ok","timestamp":1679444498656,"user_tz":240,"elapsed":807,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["model_output_path = Path(\"/content/drive/MyDrive/ECE 792 - Advance Topics in Machine Learning/Code/DeepFakeImageDetection/CN/model\")\n","if not model_output_path.exists():\n","  model_output_path.mkdir(exist_ok=True, parents=True)\n","test_output_path = model_output_path / \"test\"\n","if not test_output_path.exists():\n","  test_output_path.mkdir(exist_ok=True, parents=True)\n","\n","config_cn = { 'batch_size'             : 88,\n","              'image_size'             : 64,\n","              'n_channel'              : 3,\n","              'n_epochs'               : 25,\n","              'n_test_epochs'          : 1,\n","              'lr'                     : 1e-3,\n","              'device'                 : device,\n","              'seed'                   : 999,\n","              'model_output_path'      : model_output_path,\n","              'test_output_path'       : test_output_path,\n","              'chkp_freq'              : 1,  # number of epochs to save model out\n","              'n_workers'              : 4,\n","              'gans_to_skip'           : None,\n","              'test_chkp_freq'         : 1,\n","}"],"metadata":{"id":"pitoJ0iJofGo","executionInfo":{"status":"ok","timestamp":1679444502519,"user_tz":240,"elapsed":1651,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["celebrity_data_cn = CelebrityDataClassificationNetwork(\n","  base_path=dataset_base_path,\n","  transform=transforms.Compose(\n","    [\n","      transforms.Resize(int(config_cn[\"image_size\"] * 1.1)),\n","      transforms.CenterCrop(config_cn[\"image_size\"]),\n","      transforms.ToTensor(),\n","      transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n","    ]\n","  ),\n","  seed=config_cn[\"seed\"],\n","  gans_to_skip=config_cn[\"gans_to_skip\"],\n",")\n","\n","dataloader_cn = torch.utils.data.DataLoader(\n","  dataset=celebrity_data_cn,\n","  shuffle=True,\n","  batch_size=config_cn[\"batch_size\"],\n","  num_workers=config_cn[\"n_workers\"],\n","  drop_last=True,  # drop last batch that may not be the same size as the expected batch for the network\n","  pin_memory=True,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gMR96ydioYke","executionInfo":{"status":"ok","timestamp":1679444621315,"user_tz":240,"elapsed":117614,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"6fbabc15-dee7-4594-e8f7-e9e45183128f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting imagery for 'WGAN-CP'\n","Extracting imagery for 'PGGAN'\n","Extracting imagery for 'DCGAN'\n","Extracting imagery for 'LSGAN'\n","Extracting imagery for 'CDCGAN'\n","Extracting imagery for 'WGAN-GP'\n","Extracting RealFaces imagery\n"]}]},{"cell_type":"code","source":["class ClassificationNetwork(nn.Module):\n","  conv_layer_out = None\n","  activation_out = None\n","  global_avg_pool_out = None\n","  flatten_out = None\n","  fully_connected_out = None\n","  softmax_out = None\n","\n","  def __init__(self, learning_rate: float = 1e-3):\n","    super().__init__()\n","    self.conv_layer = nn.Conv2d(in_channels=128, out_channels=2, kernel_size=(3, 3))\n","    self.activation = nn.ReLU()\n","    self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","    self.flatten = nn.Flatten()\n","    self.fully_connected = nn.Linear(2, 2)\n","    self.softmax = nn.Softmax(dim=1)\n","\n","    self.loss = nn.BCELoss()\n","\n","    self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n","\n","  def forward(self, x):\n","    self.conv_layer_out = self.conv_layer(x)\n","    self.activation_out = self.activation(self.conv_layer_out)\n","    self.global_avg_pool_out = self.global_avg_pool(self.activation_out)\n","    self.flatten_out = self.flatten(self.global_avg_pool_out)\n","    self.fully_connected_out = self.fully_connected(self.flatten_out)\n","    self.softmax_out = self.softmax(self.fully_connected_out)\n","\n","    return self.softmax_out"],"metadata":{"id":"rOfO9_JgC3RV","executionInfo":{"status":"ok","timestamp":1679444625350,"user_tz":240,"elapsed":401,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","## Load pretrained CFFN model and change to eval mode, since the model is already trained\n","cffn_model_file = get_latest_model(cffn_models_output_path)\n","print(f\"CFFN model file : '{cffn_model_file}'\")\n","checkpoint = torch.load(str(cffn_model_file), map_location=config_cn[\"device\"])\n","cffn.load_state_dict(checkpoint[\"CFFN_state_dict\"])\n","cffn.to(device)\n","cffn.eval()\n","\n","cn = ClassificationNetwork(\n","  learning_rate=config_cn[\"lr\"]\n",").to(device)\n","\n","train: bool = False\n","load_pretrained_cn: bool = True\n","if train:\n","  epoch_tqdm = tqdm(total=config_cn[\"n_epochs\"], position=0)\n","  train_loss = {\n","      \"total\": Loss.init(),\n","  }\n","  train_acc = {\n","      \"total\": Accuracy.from_output_decisions(2, onehotencoding=False),\n","  }\n","  for gan_name in celebrity_data_cn.names_of_gans():\n","    train_loss[gan_name] = Loss.init()\n","    train_acc[gan_name] = Accuracy.from_output_decisions(2, onehotencoding=False)\n","\n","  bce_loss = nn.BCELoss()\n","  print(\"Starting Training Loop for Classification Network...\")\n","  for epoch in range(config_cn[\"n_epochs\"]):\n","    for batch_idx, (img, label, gan_selection) in enumerate(dataloader_cn):\n","      # get indices of images that are from the different GANs or real\n","      vals = np.unique(gan_selection)\n","      gan_select_idx = {}\n","      for val in vals:\n","        idx = np.where(np.array(gan_selection) == val)\n","        gan_select_idx[val] = torch.Tensor(idx[0]).to(torch.int64)\n","\n","      # zero classification network gradients\n","      cn.optimizer.zero_grad()\n","      # cast img & label to device we are working on\n","      img = img.to(config_cn[\"device\"])\n","      label = label.to(config_cn[\"device\"])\n","      # get discriminative features from last convolution output of CFFN\n","      img_cffn, img_discriminative_features = cffn(img)\n","      # classify output from CFFN using classification network\n","      out_cn = cn(img_cffn)\n","      # calculate total loss\n","      criterion = bce_loss(out_cn, label)\n","      # calculate gradients\n","      criterion.backward()\n","      # backpropagate gradients\n","      cn.optimizer.step()\n","      # update loss & accuracy for each \n","      train_loss[\"total\"] += criterion.item()\n","      label = torch.argmax(label, dim=1)  # need to unencode one hot encoded labels for accuracy measurements in binary classification task\n","      train_acc[\"total\"].compare_batch(targets=label, outputs=out_cn)\n","\n","      labels_real = label[gan_select_idx[\"real\"]]\n","      out_cn_real = out_cn[gan_select_idx[\"real\"]]\n","      gan_select_idx.pop(\"real\")\n","      for gan_name, idx in gan_select_idx.items():\n","        labels_ = label[idx]\n","        labels_ = torch.concat([labels_, labels_real], dim=0)\n","        out_cn_ = out_cn[idx]\n","        out_cn_ = torch.concat([out_cn_, out_cn_real], dim=0)\n","        # criterion = bce_loss(out_cn_, labels_)\n","        # train_loss[gan_name] += criterion.item()\n","        train_acc[gan_name].compare_batch(targets=labels_, outputs=out_cn_)\n","\n","      if (batch_idx + 1) % 50 == 0:\n","        epoch_write_str = \" [%d/%d]\\tAcc_Total: %.5f\\tLoss_Total: %.8f\"\n","        epoch_write_vars = [batch_idx+1, len(dataloader_cn), train_acc[\"total\"].current_accuracy, train_loss[\"total\"].current_loss]\n","        # for key in celebrity_data_cn.names_of_gans():\n","        #   epoch_write_str += f\"\\tLoss_{key}: %.8f\"\n","        #   epoch_write_vars.append(train_loss[key].current_loss)\n","        \n","        epoch_write_vars = tuple(epoch_write_vars)\n","        # epoch_tqdm.write(epoch_write_str % epoch_write_vars)\n","        print(epoch_write_str % epoch_write_vars)\n","      \n","    epoch_tqdm.update(1)\n","    for key in list(train_loss.keys()):\n","      # train_loss[key].update_for_epoch()\n","      train_acc[key].update_for_epoch()\n","    if (epoch + 1) % config_cn[\"chkp_freq\"] == 0:\n","      (\n","        model_output_path,\n","        confusion_matrix_path,\n","        accuracy_plot_path,\n","        roc_curve_plot_path,\n","        cum_stats_csv_path,\n","        loss_plot_path,\n","       ) = training_plot_paths(config_cn[\"model_output_path\"], epoch+1)\n","      torch.save(\n","          {\n","            \"CN_state_dict\": cn.state_dict(),\n","            \"CN_optimizer\": cn.optimizer.state_dict(),\n","          },\n","          str(model_output_path),\n","      )\n","      for (key, train_loss_), train_acc_ in zip(train_loss.items(), train_acc.values()):\n","        loss_plot_path_ = loss_plot_path.parent / key / loss_plot_path.name\n","        loss_plot_path_.parent.mkdir(exist_ok=True, parents=True)\n","        accuracy_plot_path_ = accuracy_plot_path.parent / key / accuracy_plot_path.name\n","        accuracy_plot_path_.parent.mkdir(exist_ok=True, parents=True)\n","        confusion_matrix_path_ = confusion_matrix_path.parent / key / confusion_matrix_path.name\n","        confusion_matrix_path_.parent.mkdir(exist_ok=True, parents=True)\n","        roc_curve_plot_path_ = roc_curve_plot_path.parent / key / roc_curve_plot_path.name\n","        roc_curve_plot_path_.parent.mkdir(exist_ok=True, parents=True)\n","        cum_stats_csv_path_ = cum_stats_csv_path.parent / key / cum_stats_csv_path.name\n","        cum_stats_csv_path_.parent.mkdir(exist_ok=True, parents=True)\n","\n","        if key in [\"total\"]:\n","          save_loss_plot(train_loss_.loss_vals_per_epoch, loss_plot_path_)\n","        save_accuracy_plot(train_acc_.acc_vals_per_epoch, accuracy_plot_path_)\n","        train_acc_.save_confusion_matrix(confusion_matrix_path_, \"01\")\n","        train_acc_.roc_curve(roc_curve_plot_path_)\n","        train_acc_.cum_stats_to_csv(cum_stats_csv_path_)\n","elif load_pretrained_cn:\n","  cn_model_base_path = config_cn[\"model_output_path\"] / \"models\"\n","  cn_model_path = get_latest_model(cn_model_base_path)\n","  print(f\"cn_model_path: '{cn_model_path}'\")\n","  \n","  checkpoint = torch.load(str(cn_model_path), map_location=config_cn[\"device\"])\n","  cn.load_state_dict(checkpoint[\"CN_state_dict\"])\n","\n","  epoch_tqdm = tqdm(total=config_cn[\"n_test_epochs\"], position=0, initial=1)\n","  test_loss = {\n","      \"total\": Loss.init(),\n","  }\n","  test_acc = {\n","      \"total\": Accuracy.from_output_decisions(2, onehotencoding=False),\n","  }\n","  for gan_name in celebrity_data_cn.names_of_gans():\n","    test_loss[gan_name] = Loss.init()\n","    test_acc[gan_name] = Accuracy.from_output_decisions(2, onehotencoding=False)\n","  \n","  cn.eval()\n","  bce_loss = nn.BCELoss()\n","  print(\"Testing Classification Network + CFFN...\")\n","  for epoch in range(config_cn[\"n_test_epochs\"]):\n","    for batch_idx, (img, label, gan_selection) in enumerate(dataloader_cn):\n","      vals = np.unique(gan_selection)\n","      gan_select_idx = {}\n","      for val in vals:\n","        idx = np.where(np.array(gan_selection) == val)\n","        gan_select_idx[val] = torch.Tensor(idx[0]).to(torch.int64)\n","      \n","      img = img.to(config_cn[\"device\"])\n","      label = label.to(config_cn[\"device\"])\n","\n","      img_cffn, img_discriminative_features = cffn(img)\n","      out_cn = cn(img_cffn)\n","      criterion = bce_loss(out_cn, label)\n","      test_loss[\"total\"] += criterion.item()\n","      label = torch.argmax(label, dim=1)\n","      test_acc[\"total\"].compare_batch(targets=label, outputs=out_cn)\n","\n","      labels_real = label[gan_select_idx[\"real\"]]\n","      out_cn_real = out_cn[gan_select_idx[\"real\"]]\n","      gan_select_idx.pop(\"real\")\n","      for gan_name, idx in gan_select_idx.items():\n","        labels_ = label[idx]\n","        labels_ = torch.concat([labels_, labels_real], dim=0)\n","        out_cn_ = out_cn[idx]\n","        out_cn_ = torch.concat([out_cn_, out_cn_real], dim=0)\n","        # criterion = bce_loss(out_cn_, labels_)\n","        # test_loss[gan_name] += criterion.item()\n","        test_acc[gan_name].compare_batch(targets=labels_, outputs=out_cn_)\n","      \n","      if (batch_idx + 1) % 50 == 0:\n","        epoch_write_str = \" [%d/%d]\\tAcc_Total: %.5f\\tLoss_Total: %.8f\"\n","        epoch_write_vars = [batch_idx + 1, len(dataloader_cn), test_acc[\"total\"].current_accuracy, test_loss[\"total\"].current_loss]\n","        for key in celebrity_data_cn.names_of_gans():\n","          epoch_write_str += f\"\\tAcc_{key}: %.5f\"\n","          epoch_write_vars.append(test_acc[key].current_accuracy)\n","\n","        epoch_write_vars = tuple(epoch_write_vars)\n","        print(epoch_write_str % epoch_write_vars)\n","    \n","    epoch_tqdm.update(1)\n","    for key in list(test_acc.keys()):\n","      # test_loss[key].update_for_epoch()\n","      test_acc[key].update_for_epoch()\n","    if (epoch + 1) % config_cn[\"test_chkp_freq\"] == 0:\n","      (\n","        _,\n","        confusion_matrix_path,\n","        accuracy_plot_path,\n","        roc_curve_plot_path,\n","        cum_stats_csv_path,\n","        loss_plot_path,\n","      ) = training_plot_paths(config_cn[\"test_output_path\"], epoch+1)\n","\n","      for (key, test_loss_), test_acc_ in zip(test_loss.items(), test_acc.values()):\n","        loss_plot_path_ = loss_plot_path.parent / key / loss_plot_path.name\n","        loss_plot_path_.parent.mkdir(exist_ok=True, parents=True)\n","        accuracy_plot_path_ = accuracy_plot_path.parent / key / accuracy_plot_path.name\n","        accuracy_plot_path_.parent.mkdir(exist_ok=True, parents=True)\n","        confusion_matrix_path_ = confusion_matrix_path.parent / key / confusion_matrix_path.name\n","        confusion_matrix_path_.parent.mkdir(exist_ok=True, parents=True)\n","        roc_curve_plot_path_ = roc_curve_plot_path.parent / key / roc_curve_plot_path.name\n","        roc_curve_plot_path_.parent.mkdir(exist_ok=True, parents=True)\n","        cum_stats_csv_path_ = cum_stats_csv_path.parent / key / cum_stats_csv_path.name\n","        cum_stats_csv_path_.parent.mkdir(exist_ok=True, parents=True)\n","\n","        if key in [\"real\"]:\n","          save_loss_plot(test_loss_.loss_vals_per_epoch, loss_plot_path_)\n","        save_accuracy_plot(test_acc_.acc_vals_per_epoch, accuracy_plot_path_)\n","        test_acc_.save_confusion_matrix(confusion_matrix_path_, \"01\")\n","        test_acc_.roc_curve(roc_curve_plot_path_)\n","        test_acc_.cum_stats_to_csv(cum_stats_csv_path_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4YkxLn6_dHo","executionInfo":{"status":"ok","timestamp":1679445390168,"user_tz":240,"elapsed":762808,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"52972dae-6ba4-4d51-f763-f9f4f28d6cc6"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["CFFN model file : '/content/drive/MyDrive/ECE 792 - Advance Topics in Machine Learning/Code/DeepFakeImageDetection/CFFN/model/models/CFFN--14.pth'\n","cn_model_path: '/content/drive/MyDrive/ECE 792 - Advance Topics in Machine Learning/Code/DeepFakeImageDetection/CN/model/models/model-2023-03-18--20-06-24--25.pth'\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 1/1 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Testing Classification Network + CFFN...\n"," [50/5029]\tAcc_Total: 0.91818\tLoss_Total: 0.65482322\tAcc_WGAN-CP: 0.99406\tAcc_PGGAN: 0.99524\tAcc_DCGAN: 0.99399\tAcc_LSGAN: 0.99560\tAcc_CDCGAN: 0.86596\tAcc_WGAN-GP: 0.99037\n"," [100/5029]\tAcc_Total: 0.91295\tLoss_Total: 0.73146579\tAcc_WGAN-CP: 0.99370\tAcc_PGGAN: 0.99446\tAcc_DCGAN: 0.99379\tAcc_LSGAN: 0.99457\tAcc_CDCGAN: 0.85798\tAcc_WGAN-GP: 0.99025\n"," [150/5029]\tAcc_Total: 0.91152\tLoss_Total: 0.72858827\tAcc_WGAN-CP: 0.99435\tAcc_PGGAN: 0.99466\tAcc_DCGAN: 0.99376\tAcc_LSGAN: 0.99452\tAcc_CDCGAN: 0.85572\tAcc_WGAN-GP: 0.99067\n"," [200/5029]\tAcc_Total: 0.91159\tLoss_Total: 0.74166871\tAcc_WGAN-CP: 0.99411\tAcc_PGGAN: 0.99429\tAcc_DCGAN: 0.99322\tAcc_LSGAN: 0.99391\tAcc_CDCGAN: 0.85627\tAcc_WGAN-GP: 0.98987\n"," [250/5029]\tAcc_Total: 0.91309\tLoss_Total: 0.72368475\tAcc_WGAN-CP: 0.99420\tAcc_PGGAN: 0.99434\tAcc_DCGAN: 0.99363\tAcc_LSGAN: 0.99393\tAcc_CDCGAN: 0.86037\tAcc_WGAN-GP: 0.99002\n"," [300/5029]\tAcc_Total: 0.90989\tLoss_Total: 0.73900251\tAcc_WGAN-CP: 0.99400\tAcc_PGGAN: 0.99403\tAcc_DCGAN: 0.99337\tAcc_LSGAN: 0.99371\tAcc_CDCGAN: 0.85462\tAcc_WGAN-GP: 0.98980\n"," [350/5029]\tAcc_Total: 0.91123\tLoss_Total: 0.73627469\tAcc_WGAN-CP: 0.99392\tAcc_PGGAN: 0.99395\tAcc_DCGAN: 0.99316\tAcc_LSGAN: 0.99358\tAcc_CDCGAN: 0.85529\tAcc_WGAN-GP: 0.98960\n"," [400/5029]\tAcc_Total: 0.91330\tLoss_Total: 0.72317593\tAcc_WGAN-CP: 0.99422\tAcc_PGGAN: 0.99405\tAcc_DCGAN: 0.99346\tAcc_LSGAN: 0.99375\tAcc_CDCGAN: 0.85870\tAcc_WGAN-GP: 0.98982\n"," [450/5029]\tAcc_Total: 0.91351\tLoss_Total: 0.70622977\tAcc_WGAN-CP: 0.99442\tAcc_PGGAN: 0.99413\tAcc_DCGAN: 0.99365\tAcc_LSGAN: 0.99378\tAcc_CDCGAN: 0.85928\tAcc_WGAN-GP: 0.98982\n"," [500/5029]\tAcc_Total: 0.91309\tLoss_Total: 0.71969848\tAcc_WGAN-CP: 0.99421\tAcc_PGGAN: 0.99393\tAcc_DCGAN: 0.99349\tAcc_LSGAN: 0.99328\tAcc_CDCGAN: 0.85837\tAcc_WGAN-GP: 0.98955\n"," [550/5029]\tAcc_Total: 0.91283\tLoss_Total: 0.72065296\tAcc_WGAN-CP: 0.99419\tAcc_PGGAN: 0.99398\tAcc_DCGAN: 0.99348\tAcc_LSGAN: 0.99328\tAcc_CDCGAN: 0.85814\tAcc_WGAN-GP: 0.98959\n"," [600/5029]\tAcc_Total: 0.91286\tLoss_Total: 0.72612908\tAcc_WGAN-CP: 0.99429\tAcc_PGGAN: 0.99408\tAcc_DCGAN: 0.99354\tAcc_LSGAN: 0.99339\tAcc_CDCGAN: 0.85796\tAcc_WGAN-GP: 0.98973\n"," [650/5029]\tAcc_Total: 0.91129\tLoss_Total: 0.73816257\tAcc_WGAN-CP: 0.99425\tAcc_PGGAN: 0.99408\tAcc_DCGAN: 0.99349\tAcc_LSGAN: 0.99330\tAcc_CDCGAN: 0.85575\tAcc_WGAN-GP: 0.98958\n"," [700/5029]\tAcc_Total: 0.91221\tLoss_Total: 0.73037581\tAcc_WGAN-CP: 0.99429\tAcc_PGGAN: 0.99424\tAcc_DCGAN: 0.99359\tAcc_LSGAN: 0.99352\tAcc_CDCGAN: 0.85703\tAcc_WGAN-GP: 0.98955\n"," [750/5029]\tAcc_Total: 0.91183\tLoss_Total: 0.73082867\tAcc_WGAN-CP: 0.99427\tAcc_PGGAN: 0.99414\tAcc_DCGAN: 0.99353\tAcc_LSGAN: 0.99352\tAcc_CDCGAN: 0.85638\tAcc_WGAN-GP: 0.98941\n"," [800/5029]\tAcc_Total: 0.91196\tLoss_Total: 0.72771081\tAcc_WGAN-CP: 0.99435\tAcc_PGGAN: 0.99414\tAcc_DCGAN: 0.99356\tAcc_LSGAN: 0.99353\tAcc_CDCGAN: 0.85667\tAcc_WGAN-GP: 0.98917\n"," [850/5029]\tAcc_Total: 0.91156\tLoss_Total: 0.73282171\tAcc_WGAN-CP: 0.99444\tAcc_PGGAN: 0.99415\tAcc_DCGAN: 0.99358\tAcc_LSGAN: 0.99365\tAcc_CDCGAN: 0.85623\tAcc_WGAN-GP: 0.98913\n"," [900/5029]\tAcc_Total: 0.91104\tLoss_Total: 0.74198381\tAcc_WGAN-CP: 0.99429\tAcc_PGGAN: 0.99402\tAcc_DCGAN: 0.99344\tAcc_LSGAN: 0.99356\tAcc_CDCGAN: 0.85528\tAcc_WGAN-GP: 0.98899\n"," [950/5029]\tAcc_Total: 0.91089\tLoss_Total: 0.73840276\tAcc_WGAN-CP: 0.99425\tAcc_PGGAN: 0.99405\tAcc_DCGAN: 0.99341\tAcc_LSGAN: 0.99354\tAcc_CDCGAN: 0.85488\tAcc_WGAN-GP: 0.98910\n"," [1000/5029]\tAcc_Total: 0.91091\tLoss_Total: 0.73559979\tAcc_WGAN-CP: 0.99421\tAcc_PGGAN: 0.99395\tAcc_DCGAN: 0.99327\tAcc_LSGAN: 0.99348\tAcc_CDCGAN: 0.85503\tAcc_WGAN-GP: 0.98892\n"," [1050/5029]\tAcc_Total: 0.91092\tLoss_Total: 0.73200403\tAcc_WGAN-CP: 0.99410\tAcc_PGGAN: 0.99388\tAcc_DCGAN: 0.99327\tAcc_LSGAN: 0.99344\tAcc_CDCGAN: 0.85542\tAcc_WGAN-GP: 0.98893\n"," [1100/5029]\tAcc_Total: 0.91145\tLoss_Total: 0.72905529\tAcc_WGAN-CP: 0.99410\tAcc_PGGAN: 0.99382\tAcc_DCGAN: 0.99320\tAcc_LSGAN: 0.99341\tAcc_CDCGAN: 0.85621\tAcc_WGAN-GP: 0.98896\n"," [1150/5029]\tAcc_Total: 0.91131\tLoss_Total: 0.72479746\tAcc_WGAN-CP: 0.99417\tAcc_PGGAN: 0.99388\tAcc_DCGAN: 0.99328\tAcc_LSGAN: 0.99349\tAcc_CDCGAN: 0.85602\tAcc_WGAN-GP: 0.98905\n"," [1200/5029]\tAcc_Total: 0.91172\tLoss_Total: 0.71957466\tAcc_WGAN-CP: 0.99404\tAcc_PGGAN: 0.99386\tAcc_DCGAN: 0.99318\tAcc_LSGAN: 0.99349\tAcc_CDCGAN: 0.85684\tAcc_WGAN-GP: 0.98902\n"," [1250/5029]\tAcc_Total: 0.91220\tLoss_Total: 0.71591810\tAcc_WGAN-CP: 0.99407\tAcc_PGGAN: 0.99389\tAcc_DCGAN: 0.99322\tAcc_LSGAN: 0.99355\tAcc_CDCGAN: 0.85769\tAcc_WGAN-GP: 0.98913\n"," [1300/5029]\tAcc_Total: 0.91219\tLoss_Total: 0.71841224\tAcc_WGAN-CP: 0.99408\tAcc_PGGAN: 0.99389\tAcc_DCGAN: 0.99324\tAcc_LSGAN: 0.99353\tAcc_CDCGAN: 0.85751\tAcc_WGAN-GP: 0.98916\n"," [1350/5029]\tAcc_Total: 0.91188\tLoss_Total: 0.72302813\tAcc_WGAN-CP: 0.99420\tAcc_PGGAN: 0.99402\tAcc_DCGAN: 0.99337\tAcc_LSGAN: 0.99361\tAcc_CDCGAN: 0.85678\tAcc_WGAN-GP: 0.98918\n"," [1400/5029]\tAcc_Total: 0.91202\tLoss_Total: 0.71982623\tAcc_WGAN-CP: 0.99417\tAcc_PGGAN: 0.99399\tAcc_DCGAN: 0.99336\tAcc_LSGAN: 0.99357\tAcc_CDCGAN: 0.85680\tAcc_WGAN-GP: 0.98921\n"," [1450/5029]\tAcc_Total: 0.91243\tLoss_Total: 0.71531076\tAcc_WGAN-CP: 0.99421\tAcc_PGGAN: 0.99406\tAcc_DCGAN: 0.99343\tAcc_LSGAN: 0.99362\tAcc_CDCGAN: 0.85740\tAcc_WGAN-GP: 0.98926\n"," [1500/5029]\tAcc_Total: 0.91215\tLoss_Total: 0.71693468\tAcc_WGAN-CP: 0.99419\tAcc_PGGAN: 0.99408\tAcc_DCGAN: 0.99343\tAcc_LSGAN: 0.99360\tAcc_CDCGAN: 0.85724\tAcc_WGAN-GP: 0.98924\n"," [1550/5029]\tAcc_Total: 0.91229\tLoss_Total: 0.71574367\tAcc_WGAN-CP: 0.99420\tAcc_PGGAN: 0.99408\tAcc_DCGAN: 0.99344\tAcc_LSGAN: 0.99358\tAcc_CDCGAN: 0.85747\tAcc_WGAN-GP: 0.98928\n"," [1600/5029]\tAcc_Total: 0.91211\tLoss_Total: 0.71308821\tAcc_WGAN-CP: 0.99415\tAcc_PGGAN: 0.99406\tAcc_DCGAN: 0.99344\tAcc_LSGAN: 0.99354\tAcc_CDCGAN: 0.85725\tAcc_WGAN-GP: 0.98920\n"," [1650/5029]\tAcc_Total: 0.91198\tLoss_Total: 0.71564905\tAcc_WGAN-CP: 0.99415\tAcc_PGGAN: 0.99402\tAcc_DCGAN: 0.99341\tAcc_LSGAN: 0.99356\tAcc_CDCGAN: 0.85695\tAcc_WGAN-GP: 0.98926\n"," [1700/5029]\tAcc_Total: 0.91184\tLoss_Total: 0.71877726\tAcc_WGAN-CP: 0.99414\tAcc_PGGAN: 0.99400\tAcc_DCGAN: 0.99340\tAcc_LSGAN: 0.99354\tAcc_CDCGAN: 0.85679\tAcc_WGAN-GP: 0.98931\n"," [1750/5029]\tAcc_Total: 0.91216\tLoss_Total: 0.71659282\tAcc_WGAN-CP: 0.99415\tAcc_PGGAN: 0.99399\tAcc_DCGAN: 0.99340\tAcc_LSGAN: 0.99354\tAcc_CDCGAN: 0.85738\tAcc_WGAN-GP: 0.98935\n"," [1800/5029]\tAcc_Total: 0.91212\tLoss_Total: 0.71813856\tAcc_WGAN-CP: 0.99413\tAcc_PGGAN: 0.99400\tAcc_DCGAN: 0.99339\tAcc_LSGAN: 0.99349\tAcc_CDCGAN: 0.85762\tAcc_WGAN-GP: 0.98935\n"," [1850/5029]\tAcc_Total: 0.91225\tLoss_Total: 0.71613711\tAcc_WGAN-CP: 0.99413\tAcc_PGGAN: 0.99398\tAcc_DCGAN: 0.99341\tAcc_LSGAN: 0.99348\tAcc_CDCGAN: 0.85789\tAcc_WGAN-GP: 0.98935\n"," [1900/5029]\tAcc_Total: 0.91207\tLoss_Total: 0.71779308\tAcc_WGAN-CP: 0.99413\tAcc_PGGAN: 0.99398\tAcc_DCGAN: 0.99339\tAcc_LSGAN: 0.99347\tAcc_CDCGAN: 0.85757\tAcc_WGAN-GP: 0.98934\n"," [1950/5029]\tAcc_Total: 0.91203\tLoss_Total: 0.71584136\tAcc_WGAN-CP: 0.99417\tAcc_PGGAN: 0.99402\tAcc_DCGAN: 0.99341\tAcc_LSGAN: 0.99350\tAcc_CDCGAN: 0.85742\tAcc_WGAN-GP: 0.98939\n"," [2000/5029]\tAcc_Total: 0.91178\tLoss_Total: 0.71748607\tAcc_WGAN-CP: 0.99411\tAcc_PGGAN: 0.99396\tAcc_DCGAN: 0.99334\tAcc_LSGAN: 0.99346\tAcc_CDCGAN: 0.85705\tAcc_WGAN-GP: 0.98929\n"," [2050/5029]\tAcc_Total: 0.91180\tLoss_Total: 0.71842948\tAcc_WGAN-CP: 0.99403\tAcc_PGGAN: 0.99390\tAcc_DCGAN: 0.99328\tAcc_LSGAN: 0.99339\tAcc_CDCGAN: 0.85709\tAcc_WGAN-GP: 0.98918\n"," [2100/5029]\tAcc_Total: 0.91151\tLoss_Total: 0.72132775\tAcc_WGAN-CP: 0.99399\tAcc_PGGAN: 0.99386\tAcc_DCGAN: 0.99324\tAcc_LSGAN: 0.99335\tAcc_CDCGAN: 0.85677\tAcc_WGAN-GP: 0.98915\n"," [2150/5029]\tAcc_Total: 0.91172\tLoss_Total: 0.72033463\tAcc_WGAN-CP: 0.99398\tAcc_PGGAN: 0.99385\tAcc_DCGAN: 0.99324\tAcc_LSGAN: 0.99335\tAcc_CDCGAN: 0.85714\tAcc_WGAN-GP: 0.98915\n"," [2200/5029]\tAcc_Total: 0.91186\tLoss_Total: 0.72153907\tAcc_WGAN-CP: 0.99400\tAcc_PGGAN: 0.99387\tAcc_DCGAN: 0.99329\tAcc_LSGAN: 0.99337\tAcc_CDCGAN: 0.85740\tAcc_WGAN-GP: 0.98914\n"," [2250/5029]\tAcc_Total: 0.91197\tLoss_Total: 0.71921309\tAcc_WGAN-CP: 0.99399\tAcc_PGGAN: 0.99384\tAcc_DCGAN: 0.99326\tAcc_LSGAN: 0.99335\tAcc_CDCGAN: 0.85768\tAcc_WGAN-GP: 0.98911\n"," [2300/5029]\tAcc_Total: 0.91207\tLoss_Total: 0.71651044\tAcc_WGAN-CP: 0.99398\tAcc_PGGAN: 0.99383\tAcc_DCGAN: 0.99328\tAcc_LSGAN: 0.99338\tAcc_CDCGAN: 0.85781\tAcc_WGAN-GP: 0.98909\n"," [2350/5029]\tAcc_Total: 0.91223\tLoss_Total: 0.71344483\tAcc_WGAN-CP: 0.99404\tAcc_PGGAN: 0.99388\tAcc_DCGAN: 0.99335\tAcc_LSGAN: 0.99345\tAcc_CDCGAN: 0.85824\tAcc_WGAN-GP: 0.98908\n"," [2400/5029]\tAcc_Total: 0.91217\tLoss_Total: 0.71119579\tAcc_WGAN-CP: 0.99402\tAcc_PGGAN: 0.99387\tAcc_DCGAN: 0.99334\tAcc_LSGAN: 0.99341\tAcc_CDCGAN: 0.85807\tAcc_WGAN-GP: 0.98898\n"," [2450/5029]\tAcc_Total: 0.91226\tLoss_Total: 0.71048401\tAcc_WGAN-CP: 0.99405\tAcc_PGGAN: 0.99393\tAcc_DCGAN: 0.99338\tAcc_LSGAN: 0.99345\tAcc_CDCGAN: 0.85828\tAcc_WGAN-GP: 0.98902\n"," [2500/5029]\tAcc_Total: 0.91246\tLoss_Total: 0.71163189\tAcc_WGAN-CP: 0.99401\tAcc_PGGAN: 0.99388\tAcc_DCGAN: 0.99334\tAcc_LSGAN: 0.99341\tAcc_CDCGAN: 0.85860\tAcc_WGAN-GP: 0.98895\n"," [2550/5029]\tAcc_Total: 0.91257\tLoss_Total: 0.71071390\tAcc_WGAN-CP: 0.99397\tAcc_PGGAN: 0.99386\tAcc_DCGAN: 0.99331\tAcc_LSGAN: 0.99337\tAcc_CDCGAN: 0.85873\tAcc_WGAN-GP: 0.98896\n"," [2600/5029]\tAcc_Total: 0.91256\tLoss_Total: 0.71158830\tAcc_WGAN-CP: 0.99398\tAcc_PGGAN: 0.99385\tAcc_DCGAN: 0.99332\tAcc_LSGAN: 0.99338\tAcc_CDCGAN: 0.85873\tAcc_WGAN-GP: 0.98891\n"," [2650/5029]\tAcc_Total: 0.91253\tLoss_Total: 0.71304330\tAcc_WGAN-CP: 0.99398\tAcc_PGGAN: 0.99384\tAcc_DCGAN: 0.99333\tAcc_LSGAN: 0.99339\tAcc_CDCGAN: 0.85869\tAcc_WGAN-GP: 0.98887\n"," [2700/5029]\tAcc_Total: 0.91239\tLoss_Total: 0.71442640\tAcc_WGAN-CP: 0.99399\tAcc_PGGAN: 0.99385\tAcc_DCGAN: 0.99332\tAcc_LSGAN: 0.99340\tAcc_CDCGAN: 0.85851\tAcc_WGAN-GP: 0.98885\n"," [2750/5029]\tAcc_Total: 0.91211\tLoss_Total: 0.71678838\tAcc_WGAN-CP: 0.99390\tAcc_PGGAN: 0.99376\tAcc_DCGAN: 0.99323\tAcc_LSGAN: 0.99329\tAcc_CDCGAN: 0.85817\tAcc_WGAN-GP: 0.98875\n"," [2800/5029]\tAcc_Total: 0.91203\tLoss_Total: 0.71728999\tAcc_WGAN-CP: 0.99387\tAcc_PGGAN: 0.99375\tAcc_DCGAN: 0.99320\tAcc_LSGAN: 0.99326\tAcc_CDCGAN: 0.85801\tAcc_WGAN-GP: 0.98876\n"," [2850/5029]\tAcc_Total: 0.91215\tLoss_Total: 0.71402629\tAcc_WGAN-CP: 0.99389\tAcc_PGGAN: 0.99377\tAcc_DCGAN: 0.99321\tAcc_LSGAN: 0.99327\tAcc_CDCGAN: 0.85818\tAcc_WGAN-GP: 0.98877\n"," [2900/5029]\tAcc_Total: 0.91183\tLoss_Total: 0.72133831\tAcc_WGAN-CP: 0.99391\tAcc_PGGAN: 0.99380\tAcc_DCGAN: 0.99320\tAcc_LSGAN: 0.99329\tAcc_CDCGAN: 0.85774\tAcc_WGAN-GP: 0.98878\n"," [2950/5029]\tAcc_Total: 0.91181\tLoss_Total: 0.72102110\tAcc_WGAN-CP: 0.99392\tAcc_PGGAN: 0.99382\tAcc_DCGAN: 0.99321\tAcc_LSGAN: 0.99331\tAcc_CDCGAN: 0.85773\tAcc_WGAN-GP: 0.98879\n"," [3000/5029]\tAcc_Total: 0.91183\tLoss_Total: 0.72179285\tAcc_WGAN-CP: 0.99390\tAcc_PGGAN: 0.99379\tAcc_DCGAN: 0.99321\tAcc_LSGAN: 0.99329\tAcc_CDCGAN: 0.85777\tAcc_WGAN-GP: 0.98878\n"," [3050/5029]\tAcc_Total: 0.91166\tLoss_Total: 0.72286632\tAcc_WGAN-CP: 0.99393\tAcc_PGGAN: 0.99382\tAcc_DCGAN: 0.99322\tAcc_LSGAN: 0.99332\tAcc_CDCGAN: 0.85747\tAcc_WGAN-GP: 0.98883\n"," [3100/5029]\tAcc_Total: 0.91177\tLoss_Total: 0.72123273\tAcc_WGAN-CP: 0.99391\tAcc_PGGAN: 0.99381\tAcc_DCGAN: 0.99321\tAcc_LSGAN: 0.99330\tAcc_CDCGAN: 0.85768\tAcc_WGAN-GP: 0.98879\n"," [3150/5029]\tAcc_Total: 0.91193\tLoss_Total: 0.72095526\tAcc_WGAN-CP: 0.99392\tAcc_PGGAN: 0.99381\tAcc_DCGAN: 0.99321\tAcc_LSGAN: 0.99332\tAcc_CDCGAN: 0.85809\tAcc_WGAN-GP: 0.98878\n"," [3200/5029]\tAcc_Total: 0.91193\tLoss_Total: 0.71875279\tAcc_WGAN-CP: 0.99392\tAcc_PGGAN: 0.99380\tAcc_DCGAN: 0.99318\tAcc_LSGAN: 0.99330\tAcc_CDCGAN: 0.85809\tAcc_WGAN-GP: 0.98881\n"," [3250/5029]\tAcc_Total: 0.91186\tLoss_Total: 0.72004328\tAcc_WGAN-CP: 0.99391\tAcc_PGGAN: 0.99379\tAcc_DCGAN: 0.99316\tAcc_LSGAN: 0.99329\tAcc_CDCGAN: 0.85794\tAcc_WGAN-GP: 0.98877\n"," [3300/5029]\tAcc_Total: 0.91180\tLoss_Total: 0.72176093\tAcc_WGAN-CP: 0.99392\tAcc_PGGAN: 0.99379\tAcc_DCGAN: 0.99317\tAcc_LSGAN: 0.99329\tAcc_CDCGAN: 0.85791\tAcc_WGAN-GP: 0.98878\n"," [3350/5029]\tAcc_Total: 0.91164\tLoss_Total: 0.72334995\tAcc_WGAN-CP: 0.99390\tAcc_PGGAN: 0.99377\tAcc_DCGAN: 0.99317\tAcc_LSGAN: 0.99326\tAcc_CDCGAN: 0.85776\tAcc_WGAN-GP: 0.98873\n"," [3400/5029]\tAcc_Total: 0.91184\tLoss_Total: 0.72012631\tAcc_WGAN-CP: 0.99387\tAcc_PGGAN: 0.99373\tAcc_DCGAN: 0.99314\tAcc_LSGAN: 0.99322\tAcc_CDCGAN: 0.85807\tAcc_WGAN-GP: 0.98869\n"," [3450/5029]\tAcc_Total: 0.91177\tLoss_Total: 0.72186940\tAcc_WGAN-CP: 0.99384\tAcc_PGGAN: 0.99369\tAcc_DCGAN: 0.99312\tAcc_LSGAN: 0.99318\tAcc_CDCGAN: 0.85796\tAcc_WGAN-GP: 0.98866\n"," [3500/5029]\tAcc_Total: 0.91172\tLoss_Total: 0.72155768\tAcc_WGAN-CP: 0.99383\tAcc_PGGAN: 0.99368\tAcc_DCGAN: 0.99312\tAcc_LSGAN: 0.99317\tAcc_CDCGAN: 0.85773\tAcc_WGAN-GP: 0.98870\n"," [3550/5029]\tAcc_Total: 0.91178\tLoss_Total: 0.72129048\tAcc_WGAN-CP: 0.99383\tAcc_PGGAN: 0.99367\tAcc_DCGAN: 0.99311\tAcc_LSGAN: 0.99317\tAcc_CDCGAN: 0.85781\tAcc_WGAN-GP: 0.98870\n"," [3600/5029]\tAcc_Total: 0.91168\tLoss_Total: 0.72063099\tAcc_WGAN-CP: 0.99382\tAcc_PGGAN: 0.99367\tAcc_DCGAN: 0.99309\tAcc_LSGAN: 0.99316\tAcc_CDCGAN: 0.85758\tAcc_WGAN-GP: 0.98871\n"," [3650/5029]\tAcc_Total: 0.91158\tLoss_Total: 0.72198035\tAcc_WGAN-CP: 0.99379\tAcc_PGGAN: 0.99365\tAcc_DCGAN: 0.99309\tAcc_LSGAN: 0.99314\tAcc_CDCGAN: 0.85742\tAcc_WGAN-GP: 0.98868\n"," [3700/5029]\tAcc_Total: 0.91157\tLoss_Total: 0.72325278\tAcc_WGAN-CP: 0.99381\tAcc_PGGAN: 0.99366\tAcc_DCGAN: 0.99310\tAcc_LSGAN: 0.99316\tAcc_CDCGAN: 0.85748\tAcc_WGAN-GP: 0.98867\n"," [3750/5029]\tAcc_Total: 0.91161\tLoss_Total: 0.72407756\tAcc_WGAN-CP: 0.99383\tAcc_PGGAN: 0.99368\tAcc_DCGAN: 0.99312\tAcc_LSGAN: 0.99318\tAcc_CDCGAN: 0.85758\tAcc_WGAN-GP: 0.98869\n"," [3800/5029]\tAcc_Total: 0.91153\tLoss_Total: 0.72484588\tAcc_WGAN-CP: 0.99382\tAcc_PGGAN: 0.99367\tAcc_DCGAN: 0.99311\tAcc_LSGAN: 0.99317\tAcc_CDCGAN: 0.85750\tAcc_WGAN-GP: 0.98867\n"," [3850/5029]\tAcc_Total: 0.91153\tLoss_Total: 0.72354930\tAcc_WGAN-CP: 0.99383\tAcc_PGGAN: 0.99369\tAcc_DCGAN: 0.99312\tAcc_LSGAN: 0.99317\tAcc_CDCGAN: 0.85743\tAcc_WGAN-GP: 0.98867\n"," [3900/5029]\tAcc_Total: 0.91167\tLoss_Total: 0.72119717\tAcc_WGAN-CP: 0.99381\tAcc_PGGAN: 0.99366\tAcc_DCGAN: 0.99311\tAcc_LSGAN: 0.99316\tAcc_CDCGAN: 0.85766\tAcc_WGAN-GP: 0.98863\n"," [3950/5029]\tAcc_Total: 0.91180\tLoss_Total: 0.71989334\tAcc_WGAN-CP: 0.99378\tAcc_PGGAN: 0.99364\tAcc_DCGAN: 0.99308\tAcc_LSGAN: 0.99313\tAcc_CDCGAN: 0.85779\tAcc_WGAN-GP: 0.98861\n"," [4000/5029]\tAcc_Total: 0.91165\tLoss_Total: 0.72076575\tAcc_WGAN-CP: 0.99379\tAcc_PGGAN: 0.99365\tAcc_DCGAN: 0.99306\tAcc_LSGAN: 0.99311\tAcc_CDCGAN: 0.85756\tAcc_WGAN-GP: 0.98860\n"," [4050/5029]\tAcc_Total: 0.91166\tLoss_Total: 0.72075390\tAcc_WGAN-CP: 0.99380\tAcc_PGGAN: 0.99366\tAcc_DCGAN: 0.99307\tAcc_LSGAN: 0.99312\tAcc_CDCGAN: 0.85758\tAcc_WGAN-GP: 0.98859\n"," [4100/5029]\tAcc_Total: 0.91165\tLoss_Total: 0.72016925\tAcc_WGAN-CP: 0.99382\tAcc_PGGAN: 0.99368\tAcc_DCGAN: 0.99310\tAcc_LSGAN: 0.99314\tAcc_CDCGAN: 0.85761\tAcc_WGAN-GP: 0.98861\n"," [4150/5029]\tAcc_Total: 0.91172\tLoss_Total: 0.71969491\tAcc_WGAN-CP: 0.99380\tAcc_PGGAN: 0.99366\tAcc_DCGAN: 0.99305\tAcc_LSGAN: 0.99312\tAcc_CDCGAN: 0.85770\tAcc_WGAN-GP: 0.98860\n"," [4200/5029]\tAcc_Total: 0.91164\tLoss_Total: 0.72117792\tAcc_WGAN-CP: 0.99378\tAcc_PGGAN: 0.99365\tAcc_DCGAN: 0.99305\tAcc_LSGAN: 0.99310\tAcc_CDCGAN: 0.85763\tAcc_WGAN-GP: 0.98860\n"," [4250/5029]\tAcc_Total: 0.91173\tLoss_Total: 0.71966613\tAcc_WGAN-CP: 0.99379\tAcc_PGGAN: 0.99366\tAcc_DCGAN: 0.99306\tAcc_LSGAN: 0.99312\tAcc_CDCGAN: 0.85775\tAcc_WGAN-GP: 0.98863\n"," [4300/5029]\tAcc_Total: 0.91183\tLoss_Total: 0.71835371\tAcc_WGAN-CP: 0.99379\tAcc_PGGAN: 0.99365\tAcc_DCGAN: 0.99306\tAcc_LSGAN: 0.99312\tAcc_CDCGAN: 0.85783\tAcc_WGAN-GP: 0.98863\n"," [4350/5029]\tAcc_Total: 0.91182\tLoss_Total: 0.71712946\tAcc_WGAN-CP: 0.99380\tAcc_PGGAN: 0.99366\tAcc_DCGAN: 0.99307\tAcc_LSGAN: 0.99312\tAcc_CDCGAN: 0.85781\tAcc_WGAN-GP: 0.98865\n"," [4400/5029]\tAcc_Total: 0.91173\tLoss_Total: 0.71737323\tAcc_WGAN-CP: 0.99378\tAcc_PGGAN: 0.99365\tAcc_DCGAN: 0.99306\tAcc_LSGAN: 0.99310\tAcc_CDCGAN: 0.85756\tAcc_WGAN-GP: 0.98864\n"," [4450/5029]\tAcc_Total: 0.91173\tLoss_Total: 0.71860190\tAcc_WGAN-CP: 0.99379\tAcc_PGGAN: 0.99366\tAcc_DCGAN: 0.99306\tAcc_LSGAN: 0.99309\tAcc_CDCGAN: 0.85763\tAcc_WGAN-GP: 0.98865\n"," [4500/5029]\tAcc_Total: 0.91171\tLoss_Total: 0.71847611\tAcc_WGAN-CP: 0.99379\tAcc_PGGAN: 0.99366\tAcc_DCGAN: 0.99305\tAcc_LSGAN: 0.99308\tAcc_CDCGAN: 0.85758\tAcc_WGAN-GP: 0.98865\n"," [4550/5029]\tAcc_Total: 0.91175\tLoss_Total: 0.71869787\tAcc_WGAN-CP: 0.99379\tAcc_PGGAN: 0.99367\tAcc_DCGAN: 0.99306\tAcc_LSGAN: 0.99308\tAcc_CDCGAN: 0.85769\tAcc_WGAN-GP: 0.98868\n"," [4600/5029]\tAcc_Total: 0.91177\tLoss_Total: 0.71886300\tAcc_WGAN-CP: 0.99380\tAcc_PGGAN: 0.99368\tAcc_DCGAN: 0.99307\tAcc_LSGAN: 0.99308\tAcc_CDCGAN: 0.85768\tAcc_WGAN-GP: 0.98867\n"," [4650/5029]\tAcc_Total: 0.91174\tLoss_Total: 0.71820129\tAcc_WGAN-CP: 0.99378\tAcc_PGGAN: 0.99365\tAcc_DCGAN: 0.99305\tAcc_LSGAN: 0.99307\tAcc_CDCGAN: 0.85761\tAcc_WGAN-GP: 0.98865\n"," [4700/5029]\tAcc_Total: 0.91159\tLoss_Total: 0.72027729\tAcc_WGAN-CP: 0.99375\tAcc_PGGAN: 0.99364\tAcc_DCGAN: 0.99303\tAcc_LSGAN: 0.99306\tAcc_CDCGAN: 0.85743\tAcc_WGAN-GP: 0.98864\n"," [4750/5029]\tAcc_Total: 0.91162\tLoss_Total: 0.72023136\tAcc_WGAN-CP: 0.99377\tAcc_PGGAN: 0.99367\tAcc_DCGAN: 0.99306\tAcc_LSGAN: 0.99307\tAcc_CDCGAN: 0.85750\tAcc_WGAN-GP: 0.98866\n"," [4800/5029]\tAcc_Total: 0.91156\tLoss_Total: 0.72158641\tAcc_WGAN-CP: 0.99379\tAcc_PGGAN: 0.99367\tAcc_DCGAN: 0.99308\tAcc_LSGAN: 0.99309\tAcc_CDCGAN: 0.85740\tAcc_WGAN-GP: 0.98868\n"," [4850/5029]\tAcc_Total: 0.91149\tLoss_Total: 0.72142993\tAcc_WGAN-CP: 0.99381\tAcc_PGGAN: 0.99370\tAcc_DCGAN: 0.99310\tAcc_LSGAN: 0.99313\tAcc_CDCGAN: 0.85734\tAcc_WGAN-GP: 0.98869\n"," [4900/5029]\tAcc_Total: 0.91133\tLoss_Total: 0.72188219\tAcc_WGAN-CP: 0.99380\tAcc_PGGAN: 0.99369\tAcc_DCGAN: 0.99310\tAcc_LSGAN: 0.99312\tAcc_CDCGAN: 0.85706\tAcc_WGAN-GP: 0.98866\n"," [4950/5029]\tAcc_Total: 0.91135\tLoss_Total: 0.72141046\tAcc_WGAN-CP: 0.99380\tAcc_PGGAN: 0.99369\tAcc_DCGAN: 0.99311\tAcc_LSGAN: 0.99311\tAcc_CDCGAN: 0.85705\tAcc_WGAN-GP: 0.98866\n"," [5000/5029]\tAcc_Total: 0.91143\tLoss_Total: 0.71987647\tAcc_WGAN-CP: 0.99381\tAcc_PGGAN: 0.99371\tAcc_DCGAN: 0.99311\tAcc_LSGAN: 0.99314\tAcc_CDCGAN: 0.85718\tAcc_WGAN-GP: 0.98868\n"]},{"output_type":"stream","name":"stderr","text":["\r2it [12:30, 750.67s/it]              "]}]}]}