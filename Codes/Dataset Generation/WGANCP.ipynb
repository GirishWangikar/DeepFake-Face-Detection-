{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["https://www.kaggle.com/code/joonasyoon/wgan-cp-with-celeba-and-lsun-dataset/log"],"metadata":{"id":"7i0YoZDpEz7e"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tWtj6FXoEolz","executionInfo":{"status":"ok","timestamp":1677370122339,"user_tz":300,"elapsed":22993,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"677600d3-6862-4072-f3bd-d619c75dd0be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","Folder exists\n"]}],"source":["from pathlib import Path\n","USE_COLAB: bool = True\n","dataset_base_path = Path(\"/content/drive/My Drive/ECE 792 - Advance Topics in Machine Learning/Datasets/FakeFaces/WGAN-CP\")\n","if USE_COLAB:\n","  from google.colab import drive\n","  \n","  # Mount the drive to access google shared docs\n","  drive.mount('/content/drive/', force_remount=True)\n","\n","  if dataset_base_path.exists():\n","    print(\"Folder exists\")\n","  else:\n","    print(\"DOESN'T EXIST. Add desired folder as a shortcut in your 'My Drive'\")"]},{"cell_type":"code","source":["dataset_base_path.mkdir(exist_ok=True, parents=True)\n","print(dataset_base_path.exists())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pMHryhKABe4d","executionInfo":{"status":"ok","timestamp":1677368982083,"user_tz":300,"elapsed":258,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"2aff61c1-36e8-4051-b99e-a2dfb89d3dd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["from torch import nn\n","import torch\n","# Number of workers for dataloader\n","workers = 4\n","\n","# Batch size during training\n","batch_size = 128\n","\n","# Spatial size of training images. All images will be resized to this\n","#   size using a transformer.\n","image_size = 64\n","\n","# Number of channels in the training images. For color images this is 3\n","nc = 3\n","\n","# Size of z latent vector (i.e. size of generator input)\n","nz = 100\n","\n","# Size of feature maps in generator\n","ngf = 64\n","\n","# Size of feature maps in discriminator\n","ndf = 64\n","\n","# Number of training epochs\n","num_epochs = 20\n","\n","# Nubmer of loop to train ciritic\n","n_critics = 5\n","\n","# Learning rate for optimizers\n","lr = 0.0001\n","\n","# Weight Clipping Limit\n","weight_cliping_limit = 0.01\n","\n","# Beta1 hyperparam for Adam optimizers\n","beta1 = 0.5\n","\n","# Number of GPUs available. Use 0 for CPU mode.\n","ngpu = 1\n","\n","# Animation keyframes for result\n","fig_keyframes = 20\n","\n","# Device\n","device = torch.device('cuda' if (torch.cuda.is_available() and ngpu > 0) else 'cpu')\n","\n","# Create batch of latent vectors that we will use to visualize\n","#  the progression of the generator\n","fixed_noise = torch.randn(64, nz, 1, 1, device=device)"],"metadata":{"id":"XFmkQCuTK_g4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# custom weights initialization called on netG and netD\n","def weights_init(m):\n","  if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n","    nn.init.normal_(m.weight.data, 0.0, 0.02)\n","  elif isinstance(m, nn.BatchNorm2d):\n","    nn.init.normal_(m.weight.data, 1.0, 0.02)\n","    nn.init.constant_(m.bias.data, 0)"],"metadata":{"id":"GSc-NreTLPMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generator Code\n","class Generator(nn.Module):\n","  def __init__(self):\n","    super(Generator, self).__init__()\n","    self.main = nn.Sequential(\n","      # input is Z, going into a convolution\n","      nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n","      nn.BatchNorm2d(ngf * 8),\n","      nn.ReLU(True),\n","      # state size. (ngf*8) x 4 x 4\n","      nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n","      nn.BatchNorm2d(ngf * 4),\n","      nn.ReLU(True),\n","      # state size. (ngf*4) x 8 x 8\n","      nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n","      nn.BatchNorm2d(ngf * 2),\n","      nn.ReLU(True),\n","      # state size. (ngf*2) x 16 x 16\n","      nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n","      nn.BatchNorm2d(ngf),\n","      nn.ReLU(True),\n","      # state size. (ngf) x 32 x 32\n","      nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n","      nn.Tanh()\n","      # state size. (nc) x 64 x 64\n","    )\n","\n","  def forward(self, input):\n","    return self.main(input)"],"metadata":{"id":"g6BDFtk5E7Hn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the generator\n","netG = Generator().to(device)\n","\n","# Apply the weights_init function to randomly initialize all weights\n","#  to mean=0, stdev=0.02.\n","netG.apply(weights_init)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-gYTOwR0LMQw","executionInfo":{"status":"ok","timestamp":1677352923897,"user_tz":300,"elapsed":118,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"07080238-e764-49ae-a2a8-f00a31d604bb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Generator(\n","  (main): Sequential(\n","    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (8): ReLU(inplace=True)\n","    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (11): ReLU(inplace=True)\n","    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (13): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["model_path = Path(\"/content/drive/My Drive/ECE 792 - Advance Topics in Machine Learning/Code/DatasetGeneration/WGGANCP/models/CelebA_netG.pth\")\n","gen_model = torch.load(str(model_path))\n","netG.load_state_dict(gen_model)\n","netG.to(torch.device(\"cuda\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HVs4j1gRLpqs","executionInfo":{"status":"ok","timestamp":1677352926825,"user_tz":300,"elapsed":886,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"8e13bba7-b84b-4760-f5c5-a87cbe9500b0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Generator(\n","  (main): Sequential(\n","    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (8): ReLU(inplace=True)\n","    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (11): ReLU(inplace=True)\n","    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (13): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["torch.manual_seed(999)\n","from torchvision.utils import save_image\n","from tqdm import tqdm\n","output_imgs_path = dataset_base_path\n","\n","num_imgs_to_generate = 40000\n","batch_size = 1\n","iterations = int(num_imgs_to_generate / batch_size)\n","img_cnt = 0\n","for _ in tqdm(range(iterations)):\n","  with torch.no_grad():\n","    netG.eval()\n","    fakes = netG(torch.randn(batch_size, 100, 1, 1, dtype=torch.float, device=torch.device(\"cuda\")))\n","    for fake in fakes:\n","      output_path = output_imgs_path / f\"{img_cnt}.jpg\"\n","      save_image(fake, output_path)\n","      img_cnt += 1\n","\n","img_files = sorted(output_imgs_path.glob(\"*.jpg\"))\n","print(f\"Number of images generated: '{len(img_files)}'\")\n","output_zip_path = output_imgs_path.parent / \"WGAN-CP.zip\"\n","from zipfile import ZipFile\n","from tqdm import tqdm\n","\n","if output_zip_path.exists():\n","  print(f\"Deleting '{output_zip_path}'\")\n","  os.remove(str(output_zip_path))\n","\n","print(f\"Writing images to zip file '{output_zip_path}'\")\n","with ZipFile(str(output_zip_path), mode='w') as archive:\n","  for file_ in tqdm(img_files):\n","    archive.write(file_)\n","\n","with ZipFile(str(output_zip_path), mode='r') as zipObj:\n","  zipObj.extractall()\n","\n","list_of_images_in_archive = zipObj.namelist()\n","print(f\"Number of images in archive: '{len(list_of_images_in_archive)}'\")\n","if len(list_of_images_in_archive) != len(img_files):\n","  raise RuntimeError(f\"NOT ALL IMAGES WERE SUCCESSFULLY ARCHIVED\")\n","\n","drive.flush_and_unmount()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ypFMCGzUL24l","executionInfo":{"status":"ok","timestamp":1677359026642,"user_tz":300,"elapsed":6085438,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"6f1fb88e-1cac-4879-bb8e-c8fc33d72675"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 40000/40000 [03:54<00:00, 170.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Number of images generated: '40000'\n","Writing images to zip file '/content/drive/My Drive/ECE 792 - Advance Topics in Machine Learning/Datasets/FakeFaces/DCGAN.zip'\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40000/40000 [01:15<00:00, 528.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Number of images in archive: '40000'\n"]}]},{"cell_type":"code","source":["output_imgs_path = dataset_base_path\n","import os\n","\n","img_files = sorted(output_imgs_path.glob(\"*.jpg\"))\n","print(f\"Number of images generated: '{len(img_files)}'\")\n","output_zip_path = output_imgs_path.parent / \"WGAN-CP.zip\"\n","from zipfile import ZipFile\n","from tqdm import tqdm\n","\n","if output_zip_path.exists():\n","  print(f\"Deleting '{output_zip_path}'\")\n","  os.remove(str(output_zip_path))\n","\n","print(f\"Writing images to zip file '{output_zip_path}'\")\n","with ZipFile(str(output_zip_path), mode='w') as archive:\n","  for file_ in tqdm(img_files):\n","    archive.write(file_)\n","\n","with ZipFile(str(output_zip_path), mode='r') as zipObj:\n","  zipObj.extractall()\n","\n","list_of_images_in_archive = zipObj.namelist()\n","print(f\"Number of images in archive: '{len(list_of_images_in_archive)}'\")\n","if len(list_of_images_in_archive) != len(img_files):\n","  raise RuntimeError(f\"NOT ALL IMAGES WERE SUCCESSFULLY ARCHIVED\")"],"metadata":{"id":"6u_Wm0R9_e2a"},"execution_count":null,"outputs":[]}]}